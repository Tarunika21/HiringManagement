,URL,data
0,https://www.analyticsvidhya.com/blog/2022/06/custom-named-entity-recognition-using-spacy-v3/,article published part data science blogathon named entity real world object assigned name example person organization location detail check previous article fine tune bert ner ner summarized followslearn use spacy named entity recognition tasknatural language processing nlp lot use case spacy large contribution adoption nlp industry free open source python library designed handle large volume text data draw insight understanding text also suited production use used build information extraction ie natural language understanding nlu system pre process text deep learning spacy support tokenization part speech po tagging dependency parsing many others followssource spacy everything need know spacy usage documentationspacy pre trained model ton use case named entity recognition pre trained model recognize various type named entity text model statistical extremely dependent trained example work every kind entity might require model tuning depending use case hand let see example action first need import spacy load english model en_core_web_sm naming convention model language notation eg en english denoting language trained followed three component en_core_web_sm english language multi task convolutional neural network cnn trained ontonotes assigns context specific token vector po tag dependency parse named entity detail model check spacy model page pas text spacy object nlpto get detected entity need iterate ents property docthe entity detected spacy model followswe even visualize output using displacy followswe need set jupyter parameter true running jupyter notebook google colab let understand entire pipeline detailwhen use nlp spacy model object text spacy first tokenizes text produce doc object processed next stage pipeline pipeline used default model consists tagger stage parser stage entity recognizer ner stage pipeline component processed doc object passed next component stage source language processing pipeline spacy usage documentationto train spacy ner pipeline need follow step spacy model trained iterative process model prediction compared label ground truth order compute gradient loss gradient loss used update model weight backpropagation algorithm gradient indicate much weight value adjusted model prediction become similar closer provided label time source training pipeline model spacy usage documentationfor data preparation used ner annotation toolin interface need define entity type paste raw data need annotate select word required entity click mark completed done sample copy annotated data sample google colab spacy pre installed want run locally need install spacy package using following command notebookto convert data spacy format need create docbin object store data iterate data add example entity label docbin object save object spacy fileto create config file need go spacy training quickstart guide config page select ner component hardware based system availability also select optimize efficiency faster inference smaller model lower memory consumption higher accuracy potentially larger slower model impact choice architecture pretrained weight hyperparameters completed download config file clicking download button bottom right side source training pipeline model spacy usage documentationafter saved config file base_config cfg fill remaining field fill remaining field use following command use running notebook colab command create config cfg file use training ner pipeline config file contains several parameter needed training learning rate optimizer max_steps many others try change according needed configuration knowledge train custom pipeline run following commandas created validation set use training file validation however larger dataset create separate validation set validate model performance google colab en_core_web_sm already installed running locally install using following command notebookif command throw error due en_core_web_lg vector open config cfg file change vector parameter en_core_web_sm want use large model download using following commandonce pipeline trained store best model output directory load test example load model need use spacy load path let test text visualize result using displacy provide custom color dictionary entity want assign optionsisn cool trained custom named entity recognition pipeline using spacy v much ease try annotated dataset invent something new notebook found github hope liked article spacy named entity recognition share feedback suggestion comment connect linkedin medium shown article owned analytics vidhya used author discretion related
1,https://www.analyticsvidhya.com/blog/2022/06/20-sql-coding-interview-questions/,article published part data science blogathon sql stand structured query language programming language interact query manage rdbms relational database management system sql skill highly preferred required used many organization large variety software application fresher experienced candidate planning upcoming sql interview need make sure prepared well sort conceptual theoretical coding sql question article consists real time scenario based sql coding interview question help test sql skill boost confidence note sql query used compatible oracle database version like g c c etc consider student table shown question q write query extract username character symbol email_id column answer extract position email id first using instr function pas position subtracting argument length substr function output q write query extract domain name like com au etc email_id column answer extract position dot character email id first using instr function pas position argument starting position substr function output q write query extract email service provider name like google yahoo outlook etc email_id column answer extract position email id first using instr function pas adding argument starting position substr function extract position dot character subtract earlier extracted position pas subtracting argument length substr function output q output following query b c answer bceil function return smallest integer number greater equal given number pas ceil return smallest integer value e floor function return largest integer number le equal given number pas floor return largest integer value e output q write query extract consonant present name answer first extract consonant input name extracted concatenate consonant character from_string argument remove consonant specifying corresponding character to_string argument pas narendra name query return vowel e output q write query extract vowel present name answer first extract consonant input name extracted concatenate consonant character from_string argument remove consonant specifying corresponding character to_string argument pas narendra name query return vowel e output refer emp table shown question q write query extract employee detail joined year answer use to_char extract year part hiredate column select employee hired using clause output q write query find hiked salary employee adding commission answer since commission column contains null value directly adding salary return null wherever commission null use nvl function determine hiked salary based whether commission null null comm expr null return sal comm expr comm null return sal expr output q write query find employee drawing salary manager answer self join emp table compare employee salary manager salary output q write query find subordinate reportees joined organization manager answer self join emp table compare employee hiredate manager hiredate output q write query find employee subordinate reportees e employee manager answer using simple subquery first find list distinct manager empnos select empno belong manager empnos output q write query find nd senior employee e joined organization second per hire date answer using correlated subquery get nd senior employee comparing inner query output clause output q write query find th maximum salary answer using correlated subquery get th maximum salary comparing inner query output clause output q write query find deviation average salary employee getting average salary note round average salary salary difference two digit answer first select employee getting average salary calculate deviation average salary employee output refer dept table along emp table question q write query find employee getting maximum salary department answer using simple subquery first get list maximum salary department using group operation select employee getting salary per list output q write query find department wise minimum salary maximum salary total salary average salary answer first inner join employee department table group deptno find minimum maximum total average salary department output q consider present table structure desired table structure customer table shown choose correct statement result desired table alter table customer rename custname name b alter table customer rename column custname name c alter table customer add email varchar alter table customer modify email varchar e alter table customer drop familysize f alter table customer drop column familysize answer b erename custname column name using modify email column datatype varchar varchar using drop familysize column using q consider following table schema data transaction table choose valid update statement update transaction set primestatus yes transactionid b update transaction set primestatus valid custname john c update transaction set transactionid null custname john update transaction set shoppingdate null transactionid answer dcheck constraint allows set predefined value n allowed primestatus column null constraint allow null value transactionid set null insert null value shoppingdate column output q output following sql statement avesumeb avsomc avsumd awesumeanswer cfor input string translate function replaces character specified from_string argument corresponding character to_string argument corresponding character to_string argument extra character present from_string argument removed input string output q consider emp insurance table shown get following output select emp left join insurance emp insurancetype insurance insurancetype b select emp join insurance emp insurancetype insurance insurancetype c select emp right join insurance emp insurancetype insurance insurancetype select emp full join insurance emp insurancetype insurance insurancetype answer c dright join return record right table along matched record left table since emp left table insurance type present insurance right table full outer join also return output note analyzing output focus record value rather sequence output medium shown article owned analytics vidhya used author discretion
2,https://www.analyticsvidhya.com/blog/2022/06/how-to-become-a-blockchain-developer/,article published part data science blogathon although blockchain still infancy opportunity developer contribute exciting also many many business including supply chain automotive finance adopted blockchain without problem cryptocurrency namely bitcoin became popular first witnessed use case blockchain satoshi nakamoto established first bitcoin blockchain blockchain technology ability revolutionize virtual world handle data conduct commerce blockchain originally created platform support bitcoin demonstrating level flexibility security attracted curiosity many sector industry government pushing begin putting use mind stand reason blockchain developer good place start want professional lot possibility advancement vibrant new technology getting started however go becoming blockchain developer blockchain developer one go designing blockchain application article go everything need know becoming blockchain developer including ability need blockchain developer job come novel solution complex problem command control high integrity solution specialized product hardware company technical service line blockchain developer performs complex analysis design development testing computer software debugging software development operating system integration computer system selection task performed person finally work variety system must comfortable variety platform programming language naturally blockchain developer face difficulty example meeting need blockchain development project developer must work legacy infrastructure restriction also issue comprehending technological viability deploying decentralized cryptosystems procedure part regular infrastructure different type blockchain developersblockchain developer corethese people charge architecture design optimization guideline approve blockchain system designed developed optimised type blockchain developer consensus protocol example specifies way member blockchain resource agree share use make decision area well set blockchain functionality feature make sure work properly charge network security design implementation ensure network running create blockchain network connector service planning designing implementing want improve product feature function blockchain developerthese blockchain developer create maintain design according plan core developer develop application decentralised dapps implement smart contract way core developer intended ensure dapps function properly examining managing blockchain network integration service application engineer smart contractsthese blockchain developer evaluate create smart contract engage consumer buyer understand business flow security ensure smart contract free fault research smart contract test business process end end extremely popular cutting edge technology opportunity growth according pwc poll last year percent business interested hiring blockchain expert far larger number people use blockchain approximately company adopted technology many exploring near future pay excellent united state blockchain developer earn per year regular basis blockchain developer pay among highest industry according survey blockchain developer greater experience talent get paid digital security identity improved blockchain promise conventional non blockchain platform enterprise looking secure operation platform blockchain platform process digital id also assist blockchain developer company save money process operation responsibility issue becomes one prepare someone necessary ability take blockchain development challenge two different scenario work blockchain hopeful starting zero programming background well experience related field delving two distinct sort people want blockchain developer good idea familiarise mindset best suited blockchain developer unique problem blockchain development need unique mindset hear word hacker pronounced aloud usually positive light business worth salt want work hacker except ethical hacker another story hacker attitude hand lead creation skilled blockchain engineer due fact faced issue obstacle hacker prefer think outside box rather engage conventional thinking skilled blockchain developer also collaborate work well part team similar vein ideal blockchain developer understands seek help issue continue find solution result best candidate blockchain development work well people understands limitation come new solution challenge thorough understanding blockchain technologythis one go without saying grasp decentralized network work good blockchain developer blockchain technology exactly blockchain distributed database allows transparent secure tamper proof transaction operates leveraging peer peer network approve transaction eliminating need intermediary result great company trying cut expense improve productivity result thinking employing blockchain engineer double check set skill least one high level coding language masteryc golang c javascript solidity python ruby java among popular coding language blockchain development bitcoin well known cryptocurrency initially developed c language general purpose coding language blockchain engineer use range purpose exception solidity exclusive ethereum great blockchain developer excellent coder one language regardless situation solid cryptography security principle knowledgeblockchain technology safe since built encryption result solid understanding cryptography security principle required effective blockchain developer cryptography technique encrypting data highly difficult crack algorithm prohibits third party interfering information transferred party involved including transmitter recipient data situation cryptography put simply mean securing data without relying third party blockchain technology develops business need developer understand cryptography security principle result business ensure blockchain developer contemplating set skill thorough understanding distributed system peer peer networking required distributed system computer network allow user communicate sending receiving message put another way system decentralized meaning centralized control management blockchain network node peer central level control malfunction constraint hierarchy need comprehend principle blockchain developer knowledge smart contractssmart contract self executing contract party contract term written code electronic contract incorporate contract term condition two party contract implemented run without need third party intervene smart contract one powerful feature blockchain technology well one significant difference blockchain traditional database next year blockchain developer build smart contract high demand result keeping mind looking blockchain developer job crucial understanding data structure algorithmsa blockchain programmer able write algorithm following purpose necessary confirm balance validate handle newly added blockchain transaction consensus protocol put place verify validity digital signature create blockchain enabled application question blockchain developer successful fruitful professional life source concern let go complete path need take become blockchain developer worry mind start basic first foremost computer science information technology academic background bachelor master degree certain discipline might pursued formal education required become blockchain developer assist understanding fundamental laying framework studying becoming blockchain developer choose variety recommended training programme well degree programme obtain greater exposure specific technology furthermore practically every behemoth requires school credential prerequisite better chance landing desirable job gain technical proficiency necessary skill must master certain prerequisite technical skill entering blockchain development domain technological ability need become blockchain developer listed preceding section mostly consists programming ability database management data structure cryptography develop hacker mindset blockchain technology still early phase future unknown people imagine kind transformative application built blockchain coming year wonder blockchain developer hacker mentality building thing never built always meeting challenge always trying new solution order figure tackle obstacle confront daily basis make portfolio appeal portfolio feature best work done profession element portfolio demonstrate committed project benefitted customer past strong portfolio feature might help acquire freelance employment include case study testimonial data driven outcome photo chart work sample mock ups also highlight important talent assist progressing blockchain development profession join online community people share interest join online forum blockchain developer expert gain better knowledge technology kind group supply helpful knowledge also allowing establish strong professional network surprisingly network excellent resource learning employment possibility blockchain developer ready start career blockchain developer linkedin optymize upwork indeedso starting ground learn build blockchain site may enroll short course learn write java javascript python swift ground include coursera udemy skillshare udacity become blockchain developer need learn programming language thousand free online course available newcomer learn program various language furthermore advanced programming class language available various teaching platform take look following course pursue career blockchain developer beginner guide blockchain blockchain fundamentalsit take time become blockchain developer cliché go mastering something take thousand hour study practice however start path become blockchain developer right slow get disheartened take figure thing blockchain technology still infancy making difficult time consuming endeavor medium shown article owned analytics vidhya used author discretion
3,https://www.analyticsvidhya.com/blog/2022/06/top-15-important-data-science-interview-questions/,article published part data science blogathon job interview scary fresher especially attending interview interdisciplinary role like data science machine learning tension doubt get yes end interview whether answer asked properly lead distraction preparation article sharing encountered question various subject data science ml ai previous interview experience although repetitive one may may get asked question often depend kind work done company organization example organization deal time series data getting time series question note matter curated question must toung tip attending data science interview without ado let u get subject confusion matrix describe role word understand type type ii error an real world machine learning confusion matrix useful way summarize machine learning model performance idea behind confusion matrix directly came perception great idea judge classification task accuracy imbalanced data biased skewed hence give wrong classification result confusion matrix thus provides better way check model performance confusion matrix matrix consisting key metric true positive false positive true negative false negative true positive tp correct prediction true eventsfalse positive fp incorrect prediction true event also called type error true negative tn correct prediction false eventsfalse negative fn incorrect prediction false event also called type ii error model well judged metric called sensitivity specificity accuracy calculated confusion matrix sensitivity measure correctly predicted proportion true positive formula tp tp fn specificity measure correctly predicted proportion true negative formula tn tn fp accuracy measure degree veracity classification simply proportion true prediction tp tn formula tp tn tp tn fp fn define precision recall f score formula an precision precision measure proportion true positive total number predicted positive help analyze quality prediction higher precision mean model generalized give almost correct result formula tp tp fp recall also called sensitivity true positive rate tpr probability detection recall ratio true positive total positive help measure true positive rate completeness prediction formula tp tp fn f score weighted harmonic mean precision recall f score take precision recall rate model performance formula x precision x recall precision recall would distinguish feature selection feature extraction an feature selection feature extraction two important technique scale high dimensional data solve curse dimensionality many feature may necessary hinders good modelling feature selection known fact increase input feature variable consume computational resource slow modelling lead overall failure feature selection stage data preprocessing feature essential predict target considered modelling irrelevant redundant feature removed stage robustness ml pipeline developed later depends hence plenty time effort given stage optimized feature set lead easier modelling fast computation robust machine learning pipeline easily scaled ready production feature extraction feature extraction process accelerates proper feature selection great extent feature extraction take task transforming feature existing data extract new feature feature extraction transformation done feature selection feature selection depends right top extracted feature data preprocessing stage feature extracted given data like n gram part speech tag text corner edge image transforming data feature selection done remove irrelevant redundant feature finally data mining exploring trend insight data modelling interpretation evaluation performed know autoregression moving average can not take non stationary data solve time series problem an generally used statistical tool work time series data autoregression technique us regression generally linear regression take input lag variable observation previous time step hence autoregression model output target variable future time step based linear combination lag variable let mathematically explain x b b x b x b b coefficient calculated optimization training datasetx predicted value timestamp x x input variable observation previous timestamps moving average simply calculated mean changing subset number sliding window mathematical term time series data use extremely helpful technique calculating average particular time variant feature certain period example stock market forecasting giant use moving average see stock trend market period help forecast stock headed next month also interchangeably called rolling mean let mean timestamp x find average x predict value timestamp sliding window hence take mean value predict rd value done slide next subset value moving average model instead taking value previous time step take lagged forecast error previous value white noise error predict ultimate forecast part ii can not take non stationary data solve time series problem never generally stationarity mean non fluctuation consistency machine learning model perform well data fitting stationary nature e without high fluctuation consistency input output parameter like mean median variance time series forecasting data highly fluctuating time period come seasonalities trend tend make data sharply inconsistent impossible model data highly fluctuating mean variance forecasting meet crisis extremely crucial check stationarity remove non stationarity data modelling explain hidden markov model called hidden an hidden markov model markov model hidden state hmms specially used solve problem using machine learning hidden target predicted based set observed feature attribute example mental health hidden state predicted set observed feature like sleep hour stress level work pressure physical health trauma abuse hidden markov model following probabilistic parameter using parameter hmm try answer question likelihood much likely observation happen based emission output probability get easily decide whether likely happen close far behind hidden markov model probabilistic inference hidden target drawn based maximum likelihood estimation markov process relies principle given present future always independent past hence name hidden markov model know autoencoders similar pca an autoencoders type artificial neural network architecture belonging wide diverse family anns uniqueness autoencoders feature learning algorithm produce output input unlike neural network output probability distribution class note autoencoders work mechanism data compression conventionally said latent space representation stacking multiple non linear transformation layer latent mean hidden classify dog cat image machine first place able point dog cat learning facial feature machine learning term structural similarity image really get see process work nope hence call latent space representation simple word hidden space representation task autoencoders compress input data latent space representation used extract feature similarity similar feature image lie coordinate close latent space extract feature similarity finally reconstruct using upsampling convolution operation output image compression go word dimensionality reduction compression heavily used deep learning reduce dimensionality image removing irrelevant detail focusing relevant one generally done encoding n dimensional data dimensional decoding produce original one although autoencoders resembles pca yet considered better pca case non linear feature space present pca handle linear transformation using non linear space lead loss relevant information autoencoders really useful reducing dimensionality high dimensional image data come expensive computational cost carrying computer vision task one numerous compression benefit autoencoders removing noise outputting clear data vanishing exploding gradient an model training stochastic gradient descent sgd computes gradient loss function respect weight neural network backpropagation calculation gradient update functional parameter weight minimize lost function chain rule differentiation product derivative depend parameter reside later network sometimes gradient respect weight previous layer network start becoming smaller smaller wrt time finally squash vanish hence vanishing gradient well product bunch number le going give u even smaller number subtracting small quantity weight would never really update weight pretty self explanatory exploding gradient upside vanishing gradient suppose gradient value large number say product bunch number greater going give u even greater number hence huge change weight epoch result large value therefore move away optimal value resulting explosion gradient explain tokenization stemming lemmatization nlp text preprocessing an tokenization basic text preprocessing stage large sentence piece text corpus broken smallest unit word often sentence represented token tokenization important text mining processing compute word frequency required modelling tokenizers generally two type word level tokenizer break text word level syntax tokenize word_tokenize return type return list word sentence level tokenizer break text sentence level syntax tokenize sent_tokenize return type return list sentence stemming rooting another text preprocessing technique word reduced base level root level example word stem root stem avoid different grammatical form word processed text though stemming work fine word may give weird nonsense output like give base strok word stroking strok even word lost correct data stroking process happens lot text fed stemmer lot information lost resulting model failure later lemmatization developed address loophole spotted stemming saw take care derivational affix lose original word return exactly dictionary base word called lemma lemmatization work logical approach keeping account part speech morphology text example feed united state stemmer get unite state whereas lemmatizer understands part speech proper noun return united state without altering original form briefly explain role tfidf algorithm solving nlp use case an tf idf short term frequency inverse document frequency information retrieval algorithm assigns score weight factor document present text corpus used carry nlp use case scoring directly help understanding important word sentence turn help eliminating extraneous irrelevant vocabulary corpus focus important highly scored word sentence tf idf computed using two kind frequency term frequency tf inverse document frequency idf term frequency tf frequency term word present document large text tf value may high word hence normalization tf done dividing frequency word w found document total number word document inverse document frequency idf idf assigns higher score simply weightage term word rarely found document rarer term higher idf weightage score note idf calculation degree uniqueness important document length increase word like therefore since etc increase large extent word really contribute machine learning part hence rare word document specific word occur le important scored higher idf example help search engine give really good result checking unique word entered search frequent term idf calculated taking log ratio total number document number document term thus tfidf nothing product tf idf tfidf tf idf central limit theorem accept reject null hypothesis explain valid point an central limit theorem simply state population data given kind distribution finite mean variance standard deviation sampling distribution sampling mean always normal gaussian distribution null hypothesis name suggests either nullified rejected let suppose think attacked corona virus thus covid positive assume call null hypothesis alternate hypothesis one want replace null hypothesis covid positive would want reject null hypothesis prove covid positive note either reject null hypothesis fail reject null hypothesis null hypothesis hypothesis tested failing reject null mean enough data support change innovation brought alternative reject null mean enough statistical evidence null hypothesis representative truth let u look visual representation given two tailed test graphically tail distribution show reject null hypothesis notice rejection region everything remains middle acceptance region rationale observed statistic far away depending significance level reject null otherwise accept calculate significance level probability rejecting null hypothesis true need calculate p value p value smallest level significance still reject null hypothesis given observed sample statistic important characteristic p value two type notable p value significance level alpha trick hypothesis testing follows compare contrast l regularization v l regularization an regularization common way controlling model complexity reducing overfitting flexible tunable manner l regularization l norm lasso regression work squashing model specific parameter towards simple word l regularization assigns value feature weight contribute much predictive ability machine learning model hence work like feature selection approach simply adding penalty term loss function generalized linear model regression example know squared loss regularization simply add penalty term penalize weight wj forcing irrelevant feature weight formula follows formula rocket science stuff penalty term added residual sum square r formula lambda hyperparameter regularization rate tune amount regularization applied model larger lambda coefficient shrunk toward zero model may underfit lasso l regularization add absolute value coefficient error term thus regularization term contributes optimal feature selection discarding many feature choose predictive modelling l regularization l norm ridge regression work similarly except penalty term instead taking absolute value coefficient take squared value formula l regularization follows differentiates l force coefficient weight towards never make equal regularization rate lambda used control rate penalization regularization element note higher lambda complexity decrease model underfits lower lambda complexity increase overfitting occurs hyperparameter tuning lambda need done carefully ensure robust performance model ensemble method superior individual model an ensemble method class machine learning algorithm combine several machine learning algorithm collectively called weak learner stack form ensemble learner strong learner combining prediction various weak learner one supervised model known type ensemble technique proven much superior individual model solving complex problem machine learning ensemble approach like bagging reduce variance boosting control bias stacking improve prediction great extent let examine ensemble learning yield better performance individual machine learning model bagging also known bootstrap aggregation bagging ensemble method extensively used work reducing variance noisy data help reducing sign overfitting example fitting many decision tree random different sample dataset averaging prediction made sample like divide conquer thus making model stabilized robust help reducing overfitting greatly dealing bias variance tradeoff boosting boosting work boosting strength working weakness boosting applies ensemble learning approach train sequence base model called weak learner learner compensates weakness precursor work idea united stand divided fall thus bagging strictly focus correcting prediction error stacking stacking basically stitch performance multiple classification regression model thus produce great prediction individual model stacking every single model take job learn best combination prediction done co worker stacking process consists base model followed meta model base model fit training data predict output altogether compiled meta model learns best combine prediction base model ensemble model ranked individual ml model due significance elbow method k mean clustering according opinion elbow method always give optimal number cluster an elbow method standardized approach calculating optimal number cluster unsupervised learning process k mean clustering algorithm step calculate optimal number cluster elbow method quite straightforward simple wcss within cluster sum squared error sound bit complex let break squared error point square distance point representation e predicted cluster center wcss score sum squared error point distance metric like euclidean distance manhattan distance used note number cluster increase wcss metric start fall wcss largest k according opinion elbow method bit naive approach often elbow exact steep enough choose optimal k may confusing many value seem found elbow region whereas one expected found plot really can not decide k could trial error still work often elbow region found mean optimal cluster problem analyzing model manager informed regression model suffering multicollinearity would check true without losing information still build better model an first thing first multicollinearity aesthetic way saying independent variable feature dataset highly correlated multicollinearity may serious issue disrupts independent nature independent variable make hard interpret coefficient reduces power model identify independent variable statistically significant self explanatory variable highly correlated change one variable would cause drastic change model result fluctuate significantly making non stationary model collapse need check feature dataset highly correlated multicollinearity checked standard practice plot correlation matrix right exploratory data analysis show pairwise correlation coefficient different independent variable spot multicollinearity bit time consuming task though variation inflation factor vif take le time sophisticated practice vif calculated independent variable using r squared value vif r² measure correlation one independent variable rest independent variable present dataset greater value multicollinearity research concludes vif mean serious multicollinearity le mean multicollinearity categorical continuous data hand use anova test check multicollinearity however continuous spearman rank correlation coefficient chi squared test enough removing independent variable multicollinearity bad idea may good predictor discarding lead loss important information better treat multicollinearity taking method practice performing l l norm ridge lasso regression weight penalizers reduces multicollinearity shrinking coefficient feature multicollinear data categorical variable one hot encoding scan often arise chance multicollinearity effective way remove setting drop_first true pd get_dummies function seen handle multicollinearity process help extensively build better model without losing information scalability mlops goal mlops experience idea scalability ml pipeline an machine learning training best model successful production ai ml pipeline must ensure scalability scalability mlops designing large scale model handle large amount data fed le time cost efficient way without consuming much memory cpu work smoothly million user throughout world good speed accuracy goal mlops straightforward accelerating scaling ml workload large scale ml model crucial managing processing large quantity data selecting optimized efficient machine learning algorithm deploying model production monitoring performance new data constructing necessary idea scaling ml workload follows amazon sagemaker one stop solution fast cost efficient scalable secure development training deploying ml pipeline scale one click training environment highly optimized machine learning algorithm built model tuning deployment without engineering effort life really le hectic cloud ml pipeline absolutely production ready whether batch prediction real time use flask develop machine learning application flask make stay away panic developing application source code help lot focus building best possible ml solution feature engineering data analysis crucial task really love simplicity flask creating data driven scalable web application line code tensorflow serving tensorflow serving flexible high performance serving system machine learning model designed production environment tensorflow serving make easy deploy new algorithm experiment keeping server architecture apis tensorflowhead see documentation detail ensure environment friendliness ml pipeline containerizing docker container make easy make code run smoothly across different environment operating system kind environmental dependency handled docker may save hour frustration help easier teamwork teammate using different virtual environment working ace interview must prepare basic statistical modelling product sense company interviewed kind analytics kind data business problem work practice implementing ml technique algorithm interview spend atleast hour day thoroughly studying kaggle top voted notebook searching topic simple keyword search pick problem statement various kaggle competition try solve remember perfectly fine view others take idea make sure code notebook brush sql python coding skill r rule made work everytime coefficient determination though determined enough revise review every project put resume matter well remember implementing revise code recalling challenge faced solved learnt journey throughout project necessary lot grueling question asked personal project lastly stay true try make answer thing know humbly say really know answer surely work help lot cheer reaching end article hope article help ace data science interview key takeaway article quick revision important data science stuff like central limit theorem l l regularization vanishing exploding gradient tip data science interview best like article data science interview question make sure follow linkedin discussion data science ml head github medium content data science medium shown article owned analytics vidhya used author discretion
4,https://www.analyticsvidhya.com/blog/2022/06/introduction-to-memcached-using-python/,article published part data science blogathon memcached highly performant distributed caching system memory key value data store make type nosql database memcached used tech giant like facebook twitter instagram netflix previous article explained redis another memory key value data store used purpose caching article explain connect use memcached python memcached officially supported window however install setting window subsystem linux configuring alternatively run memcached container using docker covering article first step install docker window machine download docker desktop installation process fairly simple direct docker machine enter following command command prompt fetch redis image docker hub used build run container done third step start container using memcached image downloaded earlier click run button please refer picture reference congrats successfully started redis server machine connect use memcached python using python module called pymemcache installed running following command command prompt everything ready let get hand dirty dive programming part performing crud operation first need connect memcached server let go ahead connect memcached server successfully connected memcached server let start performing simple crud operation set key value pair use either set function add function accepts key value parameter please note key always either string data type byte get value key stored memcached server use get function accepts key name parameter response always byte hence response decoded string u manually fairly easy convert byte string simply decode byte utf format default data stored expire since memcached memory data store storage limited data might get flushed due insufficient memory set expiry key value pair using add function specifying time second expiration time carefully decided based type application building try access key second get response key would deleted database set multiple key value pair using set_multi function accepts multiple key value pair dictionary dataset also accepts expire parameter key value pair expire make space new data memcached server let u try increase value key ram make use incr function perform addition operation numerical value memcached similarly make use decr function perform subtraction operation numerical value memcached value apart numerical value key value pair would converted string byte storing would get string response original data structure might problem see stored list memcached server try fetch data get string response list make sure store data type well apart string make use serialization deserialization technique using module like json pickle using json explain serialization deserialization technique used store extract data data type let u first try store nested object memcached server see response get compared response get performing serialization deserialization using json try access name property would get error response string dictionary let u use json stringify data store memcached extract deserialize using json let try access area property present inside address property check getting error see get correct response caching system way speed application traditional sql database like mysql oracle new nosql database like mongodb store data disk operation performed disk generally slower compared operation performed memory ram since technology like redis memcached memory data store operation lightning fast downside technology ram limited can not used store information make application fast technology like redis memcached used alongside proper full fledged database like mysql mongodb etc let see memcached used alongside another database application let say set key value pair memcached server trying access assume key expired memcached server case try access data traditional database say mysql data persistent expire unlike memcached simple example show caching system designed work alongside full fledged database application article discussed covered following function discussed article necessary understanding performing basic crud operation memcached lot different function readily available caching mechanism present almost application must understand learn basic caching build production ready application memcached designed work caching server competitor redis built solve multiple use case redis single threaded memcached multi threaded make memcached perform better redis data size huge know redis read article article hope enjoyed reading article learned something new thanks reading happy learning medium shown article owned analytics vidhya used author discretion
5,https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/,article published part data science blogathon field natural language processing e nlp lemmatization stemming text normalization technique technique used prepare word text document processing language english hindi consists several word often derived one another inflected language term used language contains derived word instance word historical derived word history hence derived word always common root form inflected word degree inflection varies lower higher depending language sum root form derived inflected word attained using stemming lemmatization package namely nltk stem used perform stemming via different class import porterstemmer nltk stem perform task instance ran run running derived one word e run therefore lemma three word run lemmatization used get valid word actual word returned wordnetlemmatizer library imported nltk stem look lemma word wordnet database note using wordnet lemmatizer wordnet corpus downloaded nltk downloader process reducing infected word stem instance figure stemming replace word history historical histori similarly word finally final stemming process removing last character given word obtain shorter form even form meaning need stemming nlp use case sentiment analysis spam classification restaurant review etc getting base word important know whether word positive negative stemming used get base word section help stemming paragraph using nltk used various use case sentiment analysis etc let get started note highly recommended use google colab run code import librariesimport library required stemming get inputthe paragraph taken input used stemming tokenization step stemming stemming tokenization done break text chunk case paragraph sentence easy computation seen output paragraph divided sentence based stemmingin code given one sentence taken time word tokenization applied e converting sentence word stopwords etc ignored stemming applied word finally stem word joined make sentence note stopwords word add value sentence python code output see stopwords removed sentence one word vision converted vision history histori stemming purpose lemmatization stemming overcomes drawback stemming stemming word may give may give meaningful representation histori lemmatization come picture give meaningful word lemmatization take time compared stemming find meaningful word representation stemming need get base word therefore take le time stemming application sentiment analysis lemmatization application chatbots human answering similar line stemming import library get input lemmatization import library get input tokenization step stemming output lemmatizationthe difference stemming lemmatization come step wordnetlemmatizer used instead porterstemmer rest step get outputoutput output noticed although word vision converted vision word history remained history unlike stemming thus retained meaning one thing note lot knowledge understanding structure language required lemmatization hence new language creation stemmer easier comparison lemmatization algorithm lemmatization stemming foundation derived inflected word hence difference lemma stem lemma actual word whereas stem may actual language word lemmatization us corpus attain lemma making slower stemming get proper lemma might define part speech whereas stemming step wise algorithm followed making faster point show stemming used speed important since lemmatizers scan corpus time consuming task choice lemmatizers stemmer also depends problem working medium shown article owned analytics vidhya used author discretion
6,https://www.analyticsvidhya.com/blog/2022/06/data-driven-culture-a-far-fetched-goal-for-organizations/,creating collaborative data driven culture one important goal many modern organization data driven culture data used make decision every level organization data driven culture replacing gut feeling make decision fact however access data data processing tool remain restricted selected technical user upper management echelon average people skeptical see number willing play around data nate silver founder editor chief fivethirtyeight data driven culture can not exist without democratization data data democratization certainly mean unrestricted access organizational data aim data democratization data help employee make effective business decision available usable format fast enough access require technical data expert make sense survey result data scientist spend time average data scientist spend almost time data preparation good use time though despite spending time data preparation task data scientist find task least enjoyable part job observation coming one technically advanced community data user understandably people want insight data spend much time data preparation thus hindering propagation data driven culture overcome issue traditionally organization tasked centralized data engineering team create enterprise level data warehouse data lake analyst plug central data store get insight however model data delivery currently strain let paint situation imagine one need additional metric complete data analysis however metric requires additional data processed data warehouse get serviced centralized data engineering team employee need wait turn even minor change capable write themselvesthere three main issue centralized data engineering service interestingly solution scalability tailor made delivery model also come fashion industry readymade garment fashion company produce readymade garment various standard fit size ready product however customer still need minor customization always get done fraction cost time compared bespoke solution scratch building similar analogy data engineering team achieve scale centrally managing heavy technical aspect establishing technology process people practice enabling minor customizations done business user self service model let u see component centrally delivered managed service component plan decentralize business logic component previous data pipeline domain specific business logic component decentralized handle technical user closer domain data analyst data scientist organization struggle propagate data driven culture due significant challenge one way promote data driven culture promoting hybrid data delivery model hybrid delivery model promote data democratization providing self service business logic centrally managing heavy lifting data preparation modern data stack driving force behind concept hybrid data delivery model look technical detail achieve leveraging right tool architecture upcoming article era big data starting point end pearl zhu digital master serieswith modern data stack inflection point end user enablement necessity wishlist truly data driven culture please reach linkedin conversation enjoyed reading article forget follow feedback welcome many thanks reference medium shown article owned analytics vidhya used author discretion
7,https://www.analyticsvidhya.com/blog/2022/06/crack-the-data-science-interview-case-study/,article published part data science blogathon image asked business case challenge interview machine learning engineer data scientist comparable position typical become nervous top firm like faang like integrate business case problem screening process day approach followed leading company like uber twitter case study open minded technical specific company interviewing business case problem data science interview basic term case study real world project may work company type case study question asked determined position interviewing interview given around minute absorb problem description walk thought potential solution can not measure can not improve peter drucker getting right metric case study important difficult due different reason like fresher knowledge industry note business case problem single correct answer simple solution mainly case study categorized three type product based case study prediction based case study machine learning business case study pretend interviewing position twitter engagement team twitter news feed know provides content user based interest assistance news feed ranking algorithm task part engagement team assigned task evaluate algorithm success image given problem never begin firing technique instead start clarifying case study make absolutely sure interviewer page question tend confusing indefinite asking question help get answer extra information show curious case study open minded avoid word like correct approach would might multiple solution problem statement begin phrase like going problem like double check understood problem keeping track approach much easier take note absolutely ok pause consider plan proceeding solution demonstrate understood problem concentrate actually problem example study problem focused twitter news feed user engagement result algorithm success evaluated first step identify relevant indicator ass share comment click rate ctr strategy choose address problem next step question like might included also thinking measurement always follow trend may rising others may falling circumstance may pick metric based approach perspective whether want focus business user experience consider ad business standpoint ctr powerful statistic user share content powerful metric user experience comment reaction example text processing comment take step back ask interviewer question proceed completed key phase process sure cover following clarifying brainstorming strategy conclusionmake certain approach case study complete also discus result method regard problem statement response structured manner conclude case study end end solution sensible strategy know target audience panic say irrelevant point instead tell interviewer stuck let know thinking could proceed approach case interviewer might give clue sure leading towards solution might work lead dead end hesitate mention might lead situation instead concentrate would correct continue strategy problem encountered real life project expected identify problem continue appropriate approach could add new approach problem avoiding problem stuck let interviewer know sure approach think loud get track always better bluffing practice ask case study record camera observe correct help gaining confidence practicing peer practice get better making note practice study help refer future resource found useful practice case study http www interviewquery com blog data science case study interviewvoila follow four step successful case study response clarify plan strategy conclusion word one memorize approach case study caper c clarify assume p plan e execute r review make interview participatory possible taking note benefit structuring response mention pitfall let know pro con chosen approach make sure know know company product practice peer mock interview worry first effort mess take feedback seriously hope liked article data science interview case study please share feedback comment section image http mockinterview co index php four sample case study data scientist analytics position image http www searchenginejournal com increase google search visibility twitter medium shown article owned analytics vidhya used author discretion
8,https://www.analyticsvidhya.com/blog/2022/06/combating-data-inconsistencies-with-sql/,article published part data science blogathon photo sebastian herrmann unsplashas know previous post started mailing list promote blog last post series designed way answer question causing folk subscribe blog matching opt source checked already find time take second look subscriber history data question validity today need step sherlock home shoe time put detective cap solve mystery like good analyst validating subscriber_feed_history table built together give history subscribed mailing list noticed inconsistency table especially compared actual truth remember source table mailing_list feed complete list user opted mailing list particular day well turn subscriber_feed reliable thought use subscriber_feed_history table see many user opted particular day match email_sent_history table derived log email sends align odd assume user opted particular day sent email day turn case welcome world data two table working today small caveat send email daily email_sent_history table date one email sent stop reading ahead want take time think problem reading approach let think approach problem step first need determine user subscribed day email sent since email sent daily compare email sent list day determine two thing email sent email look opted day email opted day sent email finally figured final step would alter subscriber_feed_history table accurately account inconsistency get email opted particular day leverage subscriber_feed_history directly need expiry date check duration user opted purpose built subscriber_feed_history_view add expiry date table see user subscriber_feed_history_view look something like figured next step compare email_sent_history subscriber_feed_history_view see going way full join user opted sent email get side coin remember joining user mailing_list_status time email sent sql get u see user null value opt_in_users field sent email opted vice versa null email_sent_users figured way get inconsistency next step think fix subscriber_feed_history let start thinking user sent email opted well logical thing opt right like date two scenario invalid opt user unsubscribe event recorded email sent missed opt user look seems never opted sent email first case may consider opt captured invalid delete sure implement fix work case opt email day sent email fix subscriber_feed_history would entail adding opt record user subscriber_feed_history hand user appear opted sent email must opted right make assumption revisit future let think two scenario invalid opt user_id seems unsubscribed part send missed opt user_id never seems unsubscribed part email real question unsubscribe given data say best estimate would last email sent date example user_id part email let say must unsubscribed two date lack better knowledge assume day last send apply fix subscriber_feed_history add opt event new date follows accomplished today started identifying inconsistency email subscriber history table built list user subscribed mailing list align list user sent promotional email combat start comparing two data source subscriber email sent history identify user subscribed sent email vice versa design approach case fix original table accounting inconsistency voila tweaking subscriber_feed_history aligned real truth key takeaway photo linkedin sale solution left ben white right unsplashlet know approach found helpful share find twitter abhishek talk data medium shown article owned analytics vidhya used author discretion
9,https://www.analyticsvidhya.com/blog/2022/06/a-beginners-guide-to-the-sql-language/,article published part data science blogathon teenager listed item paper buy shop called data case today world get lot data medium store vast amount data impossible write data book suppose use book store data time consuming easy retrieve data even alter data information may lost due bad weather condition misfortune need system store data retrieve update easily explained introduction data may class price item etc data collection fact e word number picture database repository data provides functionality adding modifying querying data dbms database management system software manage database one type dbms rdbms also called relational database management system set software tool control data access organization storage especially bank healthcare use rdbms example rdbms mysql oracle database ibm db db express cache data model resemble database data model used relational model allows data independence data stored table provides logical data independence physical data independence physical storage independence entity relationship data model alternative relational data model entity relationship data model entity object exist independently entity entity attribute data element describe entity example book entity edition year price title etc attribute entity becomes table attribute column figure entity relationship data model mapping entity table basic foundation designing database begin erd entity relationship diagram map erd table entity becomes table attribute column separate entity attribute example building block relational model set relation relation made part relational schema relational instance relational schema specifies following name relation schema name type attribute relation instance table made row column column attribute row tuples degree number attribute cardinality number tuplesthere mechanism establish data integrity two relation relational database called referencing primary key relational table uniquely identifies row table foreign key set column referring primary key another table table containing primary key related least one foreign key dependent table consists one foreign key business data must often adhere certain restriction called constraint constraint help implement business rule following six constraint entity integrity constraint primary key unique value identifies row tuple attribute participating primary key accept null value example book_id primary key must unique must null value table constraint referential integrity defines relationship table ensures remain valid validity data enforced using combination primary key foreign key example semantic integrity constraint related correctness data example column contain garbage value like table domain constraint allows valid value given attribute example book_name must character book_price must numeric table null constraint attribute value allowed null example book_name must null table check constraint rule set rule help check inserted updated data value table based condition example book_price must negative value table sql statement namely ddl dml ddl data definition language statement used create modify delete database object table example create alter truncate drop create statement used create table define column note basic data type use sql char varchar number date clob syntax create table column_name_ datatype optional_parameters column_ name_ datatype column name_n datatype code create table product prodid number primary key prodname varchar null qty number check qty description varchar output ii alter statement used altering table including adding dropping column modifying data type syntax alter tableadd column_name datatype code alter table product add model_no varchar null output iii truncate statement used deleting data table table syntax truncate table code truncate table test iv drop statement used deleting table syntax drop table code drop table test dml data manipulation language statement used modify data table example dml create read update delete insert statement populates table data data manipulation language statement used read modify data syntax insert value code insert product prodid prodname qty_ available description value laptop dell output ii select statement clause data manipulation language used read modify data syntax select fromcode select emp output iii update statement data manipulation language statement used read modify data syntax update set columnname value wherecode update emp set sal sal comm empno output iv delete statement data manipulation language used read modify data syntax delete wherecode select test output code delete test empno output group function grouping group function group function sum max min avg count code select max sal min sal sum sal emp output give name group function code select max sal high min sal low sum sal total emp output grouping process computing aggregate aggregating based one column grouping generated using group clause syntax select group function group code select deptno sum sal emp group deptno output clause order used filter grouped data used filter non grouped data used group clause used group clause order used end statement syntax select column name group order code select job max sal emp deptno group job max sal order output article learnedi hope found article useful several programming language like python r scala sa use sql syntax data analytics us sql analysis even use sql machine learning algorithm hope see next article medium shown article owned analytics vidhya used author discretion
10,https://www.analyticsvidhya.com/blog/2022/06/python-stock-analysis-for-beginners/,article published part data science blogathon heard stock market right stock essentially share specific company stock market risky game appropriate strategy research investor create generational wealth project tiny fraction analyzing stock market data help python since stock analysis includes technical fundamental analysis broad area short python stock analysis three significant stock indian stock market point correct direction developing data analysis visualization skill well assist right path field data set used project downloaded kaggle nifty stock market data head link click download button downloaded extract zip file data set consists number company stock data including adani port bajaj finance wipro infosys many project analyzing three tata stock tata motor tata steel tata consultancy service tc data data set consists date symbol prev close open high low last close vwap turnover trade deliverable volume deliverable utilizing date open volume importing packagesimporting datasetviewing datafrom table view first row tata motor dataset get brief overview data present see result dataset tata steel tc executing tata_steel head tc head function respectively checking size datahere see size data set represents number row represents number column executing tata_steel shape tc shape function see size e number row x column tata steel tc dataset respectively viewing datatypes columnshere notice data type date object tata motor dataset hence need convert date datatype working data section see similar result datatypes tata steel tc datasets executing tata_steel info tc info function respectively checking null valuesthe column trade deliverable volume deliverable null value present drop column working data section column used analysis see similar result datasets tata steel tc executing tata_steel isna sum tc isna sum function respectively checking duplicate valuesthe output code come indicates duplicate value present data set description data dataframe rounding value two decimal placesthe describe function show statistical data count nonnull value mean standard deviation etc data present dataset round function round value two decimal place see statistical data datasets tata steel tc executing tata_steel describe round tc describe round respectively converting date column dtype object dateonce code executed try executing info function datasets notice datatype date column changed object datetime n datasets dropping column trade deliverable volume deliverableonce code executed try running head tail function datasets notice column trade deliverable volume deliverable present adding new column datasetonce code executed try running head tail function datasets notice new column day month year present using day column analysis price comparisionaccording graph price tc skyrocketed significantly higher tata steel tata motor tc pricing trajectory generally upward beginning whereas tata steel tata motor consolidation trend volume comparisionthough price tc risen significantly compared tata steel tata motor notice graph tc least volume signifying python stock analysis traded comparatively le compared tata steel tata motor lesser liquid tata motor hand traded signifying higher liquidity better order execution part analyze roi tata steel tata motor tc buy one share stock th month beginning january tata motor tata steel november tc tata motor roitata steel roitcs roi result conclude tata steel roi significantly larger tata motor tc tc hand made greatest profit plotting roi bar graphplotting profit loss amount bar graphportfolio allocationdisplaying number share owned financial advice work done project educational purpose analysis depicts stock long term performance show potential sip long run feel free connect hope liked article python stock analysis thank time medium shown article owned analytics vidhya used author discretion
11,https://www.analyticsvidhya.com/blog/2022/06/movie-recommendation-with-sql-using-google-cloud-platform/,article published part data science blogathon ever wondered netflix get know choice show movie interest ever think amazon show recommended product based search item magic behind technology called recommendation engine article let build one using gcp google cloud platform http www cloudskillsboost google google bigquery serve le cost effective highly scalable data warehouse system bigquery ml allows create execute machine learning model bigquery using standard sql query bigquery ml allows data scientist ml engineer data engineer quickly build analyze machine learning model directly using sqlwhat kind model bigquery ml support regression modelslinear regression binary logistic regression multiclass logistic regression clusteringk mean clusteringmatrix factorization creating product like recommendation systemstime series model forecastingboosted tree model xgboost classification regression deep neural network dnn classification regressionhttps cloud google com bigquery ml doc introductionwhat big query behind scene leverage bigquery processing power build modelauto tune learning rateauto split data training testl l regularizationdata splitting training test split random sequential customset learning ratego gcp account navigation bigquery accept term condition click done option create load datasetwe use ui create load datasetwe use simple bigquery command order create load dataseti showing nd way prefer second one run command create bigquery dataset named movie choosing location creating datasetrun following command load dataset csv format upload dataset model creation going use open datasets provided bigquery loading rating loading moviesafter running command gcp console see movie data let quick look dataset see folder called movie contains movielens_movies movielens_ratings ignore movielens_movies_raw nowlet check kind data movielens_movies dataset see movieid title genre going use model building part let see movielens_ratings dataset userid movieid rating timestampcheck total data sizeon bigquery query editor write query execute examine movie rating also visualize rating using google data studio click explore data visualize data studio examine movie rating see column name genre formatted string let split save new table additional data argumentation make model good build recommendation system bigqueryml need pas model_type need identify column important collaborative filtering create model view trained model run query bigquery editorlet find best romance movie userid execute query bigquery editor let find best romance movie userid execute query bigquery editorin article learned use bigqueryml create ml model using sql key takeaway article hope learned something article see next article want know build run machine learning model sql check machine learning sql blog http iamhimanshutripathi medium com machine learning sql eci provided image link image mine let get connected linkedin twitter instagram github facebook medium shown article owned analytics vidhya used author discretion
13,https://www.analyticsvidhya.com/blog/2022/06/a-guide-to-exploratory-data-analysis-explained-to-a-13-year-old/,article published part data science blogathon might wandering vast domain ai may come across word exploratory data analysis eda short well something important yes looking answer question right place let get started also showing practical example eda dataset recently stay tuned exploratory data analysis critical process conducting initial investigation data discover pattern spot anomaly test hypothesis check assumption help summary statistic graphical representation lot jargon let get simplified going say may seem absurd bear eda mean getting know data meaning get familiarity dataset working understanding underlying pattern visualizing data via graph etc graph include bar scatter pie box plot many many eda also mean explore data word mean thing visualizing data understanding make decent machine learning model ai based application data cleaning also part eda mean datasets get real life unorganized missing value even wrong value need fixed create optimal ai application undoubtedly essential go ahead start coding ai application without understanding data working end huge mess believe understanding data language use communicate ai application understand useless must get hang data find underlying pattern spot oddity know facing actually code ai application keeping structure dataset mind nice question end answer simple use python programming language even r job python common among young practitioner professional well python support many library help play dataset instance panda see dataset get info description dataset fix missing value wrong value particular offer data structure operation manipulating numerical table time series data well could different computation based data row column create delete row much much visualize dataset use matplotlib library python allows u create several type graph basic bar graph spectrogram truly fascinating show practical example eda recently dataset got kaggle let get coding note code available github access give star download experiment much like http gist github com muhammadanas cdbbabeafecfbthe first thing going eda download dataset working done shown used link download dataset project repo github save time easy others download well project code readen others crucial make readable easily understandable done downloading dataset could see dataset tabular form via panda library shown snippet show first row second one show last row dataset python code great well need understand dataset possible via dataset website case kaggle tell meaning column e anemia etc done understanding dataset column get going data cleaning exploring via panda library get visualizing minute following snippet created copy dataset changed numerical value e yes main reason created copy future creating machine learning model string work using original dataset numerical value number much better keep application optimized human understand word better hence explore data string understandable vague way explain bear following snippet show awesome stuff could panda library firstly got info dataset column like first column say age show u filled missing value data type float word decimal form e second code snippet dropped time column require cool stuff could check github repo info code code snippet show frequency sex column word value gender candidate case observe around candidate male around female intuitive way get know data believe thing via panda much easier remember seems bit fun well another awesome plot known lmplot simple informative case observe age high correlation platelet defined also tell u platelet seem common male rather female average value platelet seems gender one graph able tell much data basic plot complex one well covering today want keep simple possible one thing really simple give blog clap follow platelet platelet colorless blood cell help blood clot platelet stop bleeding clumping forming plug blood vessel injury following snippet visualizes catplot observe high blood pressure age gender show u candidate blood pressure high could also add something show death rate graph well try might bit complicated harm trying could use another graph like line graph graph slight note take data entry much perfect practicing eda skill may good making ai application though doable graph made notebook access sure give start http gist github com muhammadanas cdbbabeafecfbcongrats first ever eda project hope understood said enjoyed read put nutshell say python give variety tool help u get know data basic graph like lmplots tell u much data even create correlation graph spectrogram imagination limit exploratory data analysis getting know familiar data exploratory data analysis crucial building machine learning model prediction classification etc data language need know underlying pattern take pattern account choosing machine learning algorithm linear regression support vector machine neural network etc result make ml model much optimized accurate python r language could use though use recommend python due variety library large community need create fancy graph even simple thing like lmplot tell lot data way properly learn data science data analysis exploratory data analysis machine learning keep practicing pick datasets want explore begin keep watching tutorial reading book article etc practice key perfection least year old learned data science practice bit youtube well might want read following blog hope got answer question finally get know hell eda recommend picking million datasets available eda one suggest following dataset containing info flight price great dataset get hand dirty http github com muhammadanas data science project eda blob main flight price analysis flights_data csvi hope enjoyed article exploratory data analysis nice talking come back time muhammad ana reach twitter medium shown article owned analytics vidhya used author discretion
14,https://www.analyticsvidhya.com/blog/2022/06/a-comprehensive-guide-to-heroku-postgres/,article published part data science blogathon recently analysis analysis wanted take data remote database previously used azure delete database learning otherwise deduct money azure balance good experience student searching free database worry money came know heroku postgres heroku offer free plan hosting postgresql database helpful beginner want quickly hosted database experiment want extend database capacity pay little amount even student afford article going explain create database using heroku get credential accessing database create table database run query without ado let get started create database heroku follow step mentioned go http www heroku com account already heroku press log button otherwise press sign button make new account following instruction log page herokuafter logging create new app give name app app used accessing database name matter creating app open app go resource tab search heroku postgres add ons section select option enable successfully created database heroku access database know credential click heroku postgres add navigated another window go setting tab find credential note delete database writing article security purpose time add data database use ide choice using datagrip cross platform ide database sql jetbrains unfortunately software free pay student get subscribing github student developer pack datagrip worry explain visual studio code datagrip follow step datagrip scroll read connecting database visual studio code section first create new project double click datagrip icon desktop see dialog box click new project option asked name project give name wish click ok new project created creating project create data source click database explorer left side screen navigate data source postgresql see dialog box asking credential fill field click ok wait everything fine connected heroku postgres familiar postgresql know already schema available postgresql database named public see image public schema also see folder named table folder create table right click folder see import data file option click option select file want import table file format must csv file format work selecting file see dialog box shown make change according need press import voila successfully created database heroku postgres database using visual studio code ide free everyone want connect heroku postgres database v code follow step already familiar v code already know without extension v code nothing connecting database install database named postgresql chris kolkman see image postgresql extensionjust typing keyword post easily see extension coming top search result also install clicking link restart v code ready go connect database press ctrl shift p open command palette v code type postgresql display command related postgresql extension select postgresql add connection command add connection commandafter selecting command asked credential one one fill field hit enter one additional field asked connection type secure connection standard connection case select secure connection everything fine see error successfully connected database click postgresql explorer left pane database connected see something like image right click connection select new query option open new file right side file run query want create table let run query creates table public database schema write query select run right clicking everything fine see table created demo table try experiment better understanding one thing face difficulty datagrip easily import data file right clicking case postgresql extension give functionality use psql command line tool open start menu search psql get result something like image asked credential opening psql command line tool one one give command see image make clear everything fine get output copy want use file follow link download csv file confuse copy command psql copy postgresql need superuser access run copy command copy need hurray made easily access heroku postgres ide process connecting remote database nearly cloud database provider understand process connecting specific remote database face difficulty others come back another article stay tuned medium shown article owned analytics vidhya used author discretion
15,https://www.analyticsvidhya.com/blog/2022/06/the-technology-of-automation-and-artificial-intelligence/,article published part data science blogathon automation intelligence two technology widely used modern system making cutting edge without two technology automation intelligence computer system program revert outdated design development concept currently obsolete le productive two concept frequently confused one interchanged however two technology distinct used different way depending situation case two may used interchangeably others may quite distinct intelligence automation combined single system case may case intelligence could lead automation system designed run thing without need assistance direct action external control automation refers system ability move function perform without controlled externally utilizing internal capability set predetermined variable definition defines automation ability self sufficient independent single system perform expected task using sub system without external assistance considered automated capacity ability system function perform without controlled externally presence changing random variable known intelligence unlike automation intelligence distinguished ability function perform unexpected situation unexpected circumstance similar random variable system take performing expected task ability use random variable unexpected circumstance distinguishes intelligence mere automation take example self driving car car able move without need human driver turn steering wheel apply brake capable referred automation order make self driving car called cutting edge technology also need able teach perfectly hold break make turn regarding constant dynamic changing situation highway intelligence needed accomplish self driving car could designed perform function simply moving fixed distance able accomplish simply automating process using fixed set fixed path two distance without intelligence car may lose control fall track unexpected item enters picture intelligence feature allows car example hold break even unexpected object way car make random dodge miss obstacle come path depending sophistication system addition automation fixed variable necessitate ability work random variable repetitive circumstance automation require intelligence yet constitute artificial intelligence automation simply go business based preprogrammed expected fixed condition always need intelligence system system implementing intelligence costly complicated result may able achieve goal solely automation problem domain dealing open system dynamic constantly unpredictable use intelligence let look car manufacturing system example assembling machine housed room dedicated purpose company common business declare environment restricted allow employee access result factory becomes functioning closed system everything expected fixed assembly line example attaching door repetitive task machine simply set correct length angle well task repetitive automated set machine set move turn fixed length angle may trick machine may fail expected operate outside factory unexpected event occurs measured factored system design machine improved capable handling even unusual scenario intelligence emerges intelligence hand necessitates automation case even say intelligence goal automation except case automation intelligent automation general refers concept independence self control summarize intelligence combination independence automation single system robot example could intelligent must automated smart robot one us intelligence extent implemented determines robot level sophistication say robot often associated automation intelligence robot mechanical system move independently performing task advancement system addition intelligence functionality smart system term used market describe system high level intelligence one able adapt perform admirably variety situation hybrid system cutting edge concept involving intelligence automation considered advanced key take away term automation intelligence interchangeable single system contain automation intelligence intelligence required necessary automation intelligence requires automation independence achieved automation intelligence entail autonomy automation robot use automation intelligent intelligent automated system referred smart system sophisticated system combine automation intelligence medium shown article owned analytics vidhya used author discretion
17,https://www.analyticsvidhya.com/blog/2022/06/linear-algebra-for-data-science-with-python/,article published part data science blogathon linear algebra branch mathematics much useful data science mathematically operate large amount data using linear algebra algorithm used ml use linear algebra especially matrix data represented matrix form know l used let get start basic vector mathematics physic vector term refers colloquially quantity can not expressed single number element vector space
18,https://www.analyticsvidhya.com/blog/2022/06/introduction-to-redis-using-python/,article published part data science blogathon redis popular memory key value data store type nosql database redis chiefly used cache database application end find many article explaining redis one database kind application article understand connect use redis python redis officially supported window however install redis setting window subsystem linux configuring alternatively run redis container using docker covering article first step install docker window machine download docker desktop installation process fairly simple direct docker machine enter following command command prompt fetch redis image docker hub used build run container done third step start container using redis image downloaded earlier click run button please refer picture reference congrats successfully started redis server machine connect use redis python using python module called redis py installed running following command command prompt everything ready let get hand dirty dive programming part performing crud operation first need connect redis server direct redis py need set decode_responses parameter true get response byte format connected redis server let start performing simple crud operation set key value pair use set function accepts key value parameter please note key always either string data type byte get value specific key use get function accepts key want value returned set multiple key value pair use mset function stand multiple set accepts multiple key value pair parameter set key value pair value set data type use sadd function set data type contains unique element unlike list data type let store set fruit without fruit repeated get value fruit stored use smembers function since seen store set data type value redis let u learn store list data type done using lpush function let u store list programming language using function get element list use lrange function help u traverse element list denotes function expected return element want return first element list replacing note traverse till index three return four element total let u try save nested object redis storing nested object make use hset function allows one level nesting want store deeply nested object different data type serialization technique like using json pickle used let see action better understand extract information stored directly use get function undo stringification performed json since memory data store important old key value pair get deleted rather expired make room storing new data redis key expiry option available let u try store key value pair expiration time make use setex function set expiry key value pair accepts ttl second want set ttl millisecond use psetex function know much time remaining key expire use ttl function return time remaining second similarly pttl function return millisecond key expired returned value negative suppose want check key expired make use exists function return available expired delete key value pair redis use delete function accepts key want delete article discussed covered following function discussed article necessary understanding performing basic crud operation redis lot different function readily available learn function refer official redis documentation official website separate page covering command available redis access page clicking link mentioned earlier redis one database introduction stack interested learn stack try cover redis stack another article article hope enjoyed reading article learned something new thanks reading happy learning medium shown article owned analytics vidhya used author discretion
19,https://www.analyticsvidhya.com/blog/2022/06/free-machine-learning-summer-training-with-analytics-vidhya/,summer begin u get excited start planning fun activity would like day u love focus upskill upgrade term skillset happy announce analytics vidhya launching summer training programme ml enthusiast machine learning application around u everywhere example typing simple email notice suggestion appear similarly industry adopted machine learning algorithm make business automated one way grow time want feel left one essential concept data science read machine learning lifecycle wondering join exciting machine learning summer training let take look step click register webinarstep click enrol summer training coursestep click register ml training hackathon course meant student looking learn ml start understanding python data science importance statistic eda underlying intuition behind several algorithm solve case study using concept learned course get information installation part course highly recommended take course order designed gain maximum knowledge yes given certificate upon completion summer training course doubt facing difficulty register summer training please reach u email protected
20,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-introduction-to-mlops/,anish mahapatra conducting interactive datahour session u anish lead data science consultant various fortune customer long time helped employee data science profession explaining mlops referred machine learning operation present future state machine learning well tool technology register got back promise trigger fomo mlops abbreviation machine learning operation basic component machine learning engineering focus optimising process deploying machine learning model subsequently maintaining monitoring mlops collaborative function frequently includes data scientist devops engineer mlops allows automated testing machine learning artefact e g data validation ml model testing ml model integration testing datahour join anish mahapatra explaining mlops present future state ml discussing future mlops following subject discussed anish lead data science consultant various fortune customer long time helped employee data science profession msc data science technical writer top data science magazine always ready help passion learning data science future drop u email email protected chat speaker directly session take advantage fantastic opportunity register datahour missed past episode datahour may watch recording youtube channel read summary previous datahour session blog clicking trouble enrolling would like conduct session u contact u email protected
21,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-causal-inference-in-practice/,dear reader getting prabakaran chandran board lead interactive datahour session u working mu sigma prestigious company data decision scientist specializes problem solving since skilled sql python r advanced analytics statistic field computer vision natural language processing deep learning worked team two people develop ai based solution fortune company explaining causal inference demonstrate may applied specific use case python register got covered vow cause fomo causal inference defined intellectual subject examines assumption study design estimating methodology enable researcher draw causal conclusion data causation illuminates link cause effect causal inference new field help u comprehend explain relationship two variable datahour join prabakaran chandran explaining causal inference demonstrating may applied specific use case python prabakaran working data decision scientist mu sigma prominent problem solving firm since knowledgeable advanced analytics statistic python r sql part two person team developed ai based solution fortune company area computer vision natural language processing deep learning interest learning data science fundamental statistic drop u email email protected query facing difficulty registering session register datahour take advantage wonderful opportunity visit youtube channel view recording missed previous episode datahour blog may read summary datahour session already held trouble enrolling would like conduct session u contact u email protected related
22,https://www.analyticsvidhya.com/blog/2022/06/web-scrapping-of-nifty-50-historical-data-using-python/,article published part data science blogathon india pandemic drawn private govt sector adopt work home resulted people time research investing stock many start ups apps came picture investing stock lot people wanted use different tool technique analyze stock coding knowledge want analyze nifty historical data get insight article proper historical data analyze trend start investing stock thinking get nifty historical data couple website provide nifty historical data please find website provide historical data http www nseindia com http finance yahoo com http investing com article concentrate investing com see extract nifty historical data website article scrap nifty data passing date range passing date range get data web scraping tool module used getting data nifty dataselenium also used web scraping selenium requires webdriver interface browser use chrome browser selenium use chrome driver webdriver control chrome browser request library module used making http request site beautifulsoup library module used parsing html document step followed web scraping nifty historical data investing comhttps chromedriver chromium org downloadsfor linux please follow stepsthe code invoke chrome browser passing url driver get url initialize list required column search required column content got website output command shown belowwe search required detail append list perform similar activity required field sample data converting dataframe see screenshot data date proper format column data string format data first remove comma column data convert required format like string date string float sample data shown belowto summarize article learned extract nifty data investing com using web scraping python got basic understanding installing selenium window linux also got basic idea finding element content using beautifulsoap storing list also got basic idea combining required column dataframe also got basic idea cleaning data converting proper format nifty historical daily data start analyzing upward downward trend roll data weekly monthly quarterly next start exploratory data analysis data plotting different graph checking missing data checking outlier checking correlation etc convert extracted data timeseries data using different library convert data time series use multiple model forecasting like facebook neuralprophet auto arima long short term memory lstm etc please find github link code window linux fork repo implement code accordingly http github com bashamsc nifty_web_scrapingthe medium shown article owned analytics vidhya used author discretion
23,https://www.analyticsvidhya.com/blog/2022/06/data-ingestion-featuring-aws/,article published part data science blogathon big data everywhere continues gearing topic day data ingestion process assist group management make sense ever increasing volume complexity data provide useful insight article elucidates cloud based etl tool leverage data ingestion process beginner new aws would like explore high level workflow data ingestion keep reading aws demand cloud computing platform provides several service globally easily load diverse data rapid pace generate cost effective scalable solution business considering broad range service provided aws learning gaining experience service would take lot effort time let talk utmost priority service useful data engineer tackle today data challenge complete day day task cover database storage analytics many list topic covered three phase follows modern day application use store petabyte data application ability process numerous request contain feature like low latency high availability aws fulfills need application database service dbaas provides two different data source relational database service non relational database service custom built according application requirement initially database set accessed aws cli software development kit later database migrated cloud minimal downtime table discus example advantage amazon database based business application source data application database either combined individually ingested central repository location analysis come next phase ingesting data pipeline approach full application generate large amount data today every action digital world leave trace considering different type source data build two main type data ingestion pipeline complete ingestion activity batch data pipeline streaming data pipeline let learn one widely used service amazon emr run dynamically scalable ec cluster provides managed hadoop ecosystem emr move data amazon database data store like amazon dynamo db emr detail cluster node amazon emr follows master slave architecture involves cluster node master refers master node slave core node let dive concept cluster node cluster collection multiple ec instance ec instance described node three type node different depending role master node node storing data hdfs assigning task core node single master node core node node equipped certain software component run store data hdfs least one core node cluster task node node similar core node store data hdfs optional node cluster life cycle lifecycle cluster emr amazon emr security elastic map reduce provides several way secure emr cluster data iam policy creating iam user assigning policy restrict access user performing unnecessary action b iam role using role enable restrict emr access service aws c data encryption data cluster instance transit data encrypted using key aws provided encryption analyzing data real time producing insight better service user knowing user behavior trend data massive tb per hour generated processed stored analyzed aws kinesis platform help working streaming data kinesis detail kinesis fully managed service involve taking care underlying infrastructure processing started data stream begun instead waiting data accumulated processed record data sent dashboard used generate alert dynamically change pricing let discus type aws kinesis capability achieve different task kinesis data stream data streaming technology allows user ingest process store analyze high volume high velocity data variety source real time b kinesis data firehose service used load massive streaming data service like elastic search splunk redshift c kinesis data analytics due growth device internet usage data streamed rapid pace looking real time data monitor business quickly leverage new opportunity serve kinesis data analytics kinesis video stream help stream video real time hundred camera device aws cloud watch store ml operation real time analytics get insight data ingestion process workflowtill discussed service used ingest data let move next phase talk service used store data concept data lake emerges use storage service named amazon categorize maintain data data lake amazon simple storage service amazon amazon described object storage every file loaded served object service store retrieves amount data anywhere web store data work two resource called bucket object bucket object bucket defined storage object object file metadata describes file step storing processing data step firstly data stored raw single bucket raw bucket original data ready analysis call region raw layer step query data clean find remove duplicate check missing record help several service like aws lambda emr aws glue process data step processing finished data written back processed bucket ready perform analysis region named processed layer step finally datasets transferred central catalog named glue catalog achieved making use glue crawler code depending application glue catalog keep track datasets object location schema information bio classification follow best practice mentioned creating bucket particular data lake built amazon bucket name must unique partition b compliant dns naming convention c use different aws account store production bucket enable versioning protect deletion e create separate bucket replicated data ec service provides scalable capacity compute cloud aws lambda serverless compute service used run background task aws glue data integration service make simple cost effective categorize clean data several source glue crawler run glue crawler periodically populate updated data table glue catalog iam identity access management provides access control across service resource implement ingestion solution compute storage service like amazon rds amazon dynamo db amazon emr amazon employed without pay large sum money key takeaway data ingestion dynamodb outweighs rds term table storage size emr help scaling running cluster depending workload manually spin terminate cluster schedule provisioning use right kinesis service according capability object bucket cleaned deleting version object can not undone ensure backup incoming data provide bucket data access owner using amazon iam medium shown article owned analytics vidhya used author discretion
24,https://www.analyticsvidhya.com/blog/2022/06/getting-started-with-apache-pig/,article published part data science blogathon reading heading apache pig first question hit every mind word pig apache pig capable working kind data similar pig eat anything pig nothing high level extensible programming language designed analyze bulk data set reduce complexity coding mapreduce program yahoo developed pig analyze huge unstructured data set minimize writing time mapper reducer function apache pig abstraction mapreduce used handle structured semi structured unstructured data high level data flow tool developed execute query large datasets stored hdfs pig latin high level scripting language used apache pig write data analysis program reading writing processing data provides multiple operator easily used developer pig script get internally converted map reduce task get executed data available hdfs generally component apache pig called pig engine responsible converting script mapreduce job usually programmer struggle performing mapreduce task good java work hadoop case pig work situation booster programmer reason make pig must use platform using pig latin programmer skip typing complex code java perform mapreduce task ease multi query approach used pig reduce length code example instead typing line code loc java programmer use apache pig write loc pig latin easy understand sql like language pig also known operator rich language offer multiple built operator like join filter ordering etc following feature apache pig extensibilitypig extensible language mean help existing operator user build function read write process data optimization opportunitiesthe task encoded apache pig allow system optimize execution automatically user focus semantics language rather efficiency udfsudfs stand user defined function pig provides facility create programming language like java embed pig script ease programmingit difficult non programmer write complex java program map reduce using pig latin easily perform query rich operator setpig latin multiple operator support like join sort filter etc although apache pig abstraction mapreduce overlapping function make difficult differentiate apache pig related mapreduce task work entirely different manner major difference apache pig user friendly high level data flow language mapreduce low level paradigm data processing apache pig require compilation process mapreduce operation need significant compilation process join task pig performed much smoothly efficiently mapreduce multi query functionality apache pig enables write line code make operation efficient mapreduce support feature comparison pig mapreduce need write time line code perform operation basic knowledge sql enough working pig deep understanding java concept required work mapreduce firstly submit pig script execution environment apache pig written pig latin using built operator pig script undergo various transformation multiple stage generate desired output let discus phase separately source http www tutorialandexample com apache pig architectureat first pig latin script sent hadoop pig handled parser basically parser responsible various type check script like type check syntax check miscellaneous check afterwards parser give output form directed acyclic graph dag carry logical operator pig latin statement logical plan dag logical operator script represented node data flow represented edge retrieving output parser logical plan dag submitted logical optimizer logical optimization carried optimizer includes activity like transform split merge reorder operator etc optimizer basically aim reduce quantity data pipeline process extracted data optimizer performs automatic optimization data us various function likepushupfilter multiple condition available filter filter split pig push condition individually split condition earlier selection condition helpful resulting reduction number record left pipeline limitoptimizer limit operator applied load sort operator pig convert operator limit sensitive implementation omits processing whole data set columnpruner function omit column never used hence reduces size record function applied operator prune field aggressively frequently mapkeypruner function omit map key never used hence reducing size record receiving optimizer output compiler compiles resultant code series mapreduce task compiler responsible conversion pig script mapreduce job last come execution engine mapreduce job transferred execution hadoop mapreduce job get executed hadoop provides required result output displayed screen using dump statement stored hdfs store statement data model pig latin allows handle variety data pig latin handle simple atomic data type int float long double etc well complex non atomic data type map tuple bag example data set atom scaler primitive data type single value pig latin irrespective data type atomic value pig string int long float double char array byte array simple atomic value byte data known field example atom kiran kolkata etc tuple record formed ordered set field may carry different data type field tuple compared record stored row rdbms mandatory schema attached element present inside tuple small bracket used represent tuples example tuple kiran kolkata unordered set tuples known bag basically bag collection tuples mandatory unique curly brace used represent bag data model bag support flexible schema e tuple number field bag much similar table rdbms unlike table rdbms mandatory number field present tuple field position data type example bag kiran kolkata aisha ketki agra map nothing set key value pair used represent data element key unique must type char array whereas value type square bracket used represent map hash symbol used separate key value pair example map name kiran age name aisha age introduction pig latin command communicate pig powerful tool used called grunt shell grunt shell interactive shell establishes interaction shell hdfs local file system open remote client access software like putty start cloudera type pig enter grunt shell grunt shell allows write pig latin statement query structured unstructured data start pig grunt type pig following basic intermediate pig latin operation reading data read data pig need put data local file system hadoop let see step step create file using cat command local file system step transfer file hdfs using put command step read data hadoop pig latin using load command syntax relation load input file path information using load_function schema relation provide relation name want load file content input file path information provide path hadoop directory file stored load_function apache pig provides variety load function like binstorage jsonloader pigstorage textloader need choose function set pigstorage commonly used function suited loading structured text file schema need define schema data passing file parenthesis running pig statement run pig latin statement use dump operator display result screen used debugging purpose syntax grunt dump relation_name describe operator display schema relation describe operator used syntax grunt describe relation_name illustrate operator get step step execution sequence statement pig command illustrate operator used syntax grunt illustrate relation_name explain review logical physical map reduce execution plan relation explain operator used syntax grunt explain relation_name filter select required tuples relation depending upon condition filter operator used syntax grunt relation filter relation condition limit get limited number tuples relation pig support limit operator syntax grunt output limit relation_name number tuples required distinct remove duplicate tuples relation distinct operator used syntax grunt relation distinct relation foreach generate specified data transformation based column data foreach operator required syntax grunt relation foreach relation generate required data group group data one relation group operator used group data key syntax grunt group_data group relation_name key_column grouping multiple column group operator also group data one relation using multiple column syntax grunt group_data group relation_name column column column group group relation column group operator used syntax grunt group_all_data group relation_name cogroup cogroup operator much similar group operator major difference cogroup suited multiple relation whereas group suitable single relation syntax grunt cogroup_data cogroup relation column relation column join purpose join operator combine data two relation firstly two declare key nothing tuple relation key matched consider two particular tuples matched displayed output otherwise unmatched record dropped following type join self join inner join left outer join right outer join full outer join let understand type join self join join table treat single table two separate relation self join used load data multiple time different alias name perform join operation syntax grunt relation join relation key relation key inner join commonly used join inner join also known equijoin inner join compare table say b return row match using join predicate combine column value creates new relation syntax grunt output join relation column relation column left outer join left outer join compare two relation left right relation return row left relation even match right relation syntax grunt relation join relation id left outer relation column right outer join right outer join compare two relation left right relation return row right relation even match left relation syntax grunt relation join relation id right outer relation column full outer join full outer join compare two relation left right relation return row match one relation syntax grunt relation join relation id full outer relation column cross calculate cross product two relation cross operator used syntax grunt relation cross relation relation union merge content two relation union operator used necessary condition perform merging column domain relation must identical syntax grunt relation union relation relation split divide relation two relation split operator required syntax grunt split relation relation condition relation condition conclusion guide learned apache pig analyzes type data present hdfs discussed pig feature architecture component guide also discussed interact grunt shell perform various linux based command also made comparison pig mapreduce install grunt shell explicitly instead open cloudera run linux based pig command hope guide pig helped gain better understanding pig work query let know comment section medium shown article owned analytics vidhya used author discretion related communicate pig powerful tool used called grunt shell grunt shell interactive shell establishes interaction shell hdfs local file system open remote client access software like putty start cloudera type pig enter grunt shell grunt shell allows write pig latin statement query structured unstructured data start pig grunt type following basic intermediate pig latin operation read data pig need put data local file system hadoop let see step step create file using cat command local file system step transfer file hdfs using put command step read data hadoop pig latin using load command syntax relation provide relation name want load file content input file path information provide path hadoop directory file stored load_function apache pig provides variety load function like binstorage jsonloader pigstorage textloader need choose function set pigstorage commonly used function suited loading structured text file schema need define schema data passing file parenthesis run pig latin statement use dump operator display result screen used debugging purpose display schema relation describe operator used syntax get step step execution sequence statement pig command illustrate operator used syntax review logical physical map reduce execution plan relation explain operator used syntax select required tuples relation depending upon condition filter operator used syntax get limited number tuples relation pig support limit operator syntax remove duplicate tuples relation distinct operator used syntax generate specified data transformation based column data foreach operator required syntax group data one relation group operator used group data key syntax grouping multiple columnsgroup operator also group data one relation using multiple column syntax group relation column group operator used syntax cogroup operator much similar group operator major difference cogroup suited multiple relation whereas group suitable single relation syntax purpose join operator combine data two relation firstly two declare key nothing tuple relation key matched consider two particular tuples matched displayed output otherwise unmatched record dropped following type join self joininner joinleft outer joinright outer joinfull outer joinlet understand type join self joinwhen join table treat single table two separate relation self join used load data multiple time different alias name perform join operation syntax inner jointhe commonly used join inner join also known equijoin inner join compare table say b return row match using join predicate combine column value creates new relation syntax left outer jointhe left outer join compare two relation left right relation return row left relation even match right relation syntax right outer jointhe right outer join compare two relation left right relation return row right relation even match left relation syntax full outer jointhe full outer join compare two relation left right relation return row match one relation syntax crossto calculate cross product two relation cross operator used syntax unionto merge content two relation union operator used necessary condition perform merging column domain relation must identical syntax splitto divide relation two relation split operator required syntax guide learned apache pig analyzes type data present hdfs discussed pig feature architecture component guide also discussed interact grunt shell perform various linux based command also made comparison pig mapreduce install grunt shell explicitly instead open cloudera run linux based pig command hope guide pig helped gain better understanding pig work query let know comment section medium shown article owned analytics vidhya used author discretion
25,https://www.analyticsvidhya.com/blog/2022/06/ai-ml-use-cases-for-supply-chain-management-scm/,article published part data science blogathon supply chain core component organization hence implementation supply chain management scm business process crucial success improving bottom line organization organization often procure scm solution leading vendor sap oracle among many others implement implementing erp solution scm solution offer configurable process covering end end supply chain operation right procurement raw material sale finished product scm solution implement traditional algorithm optimization part backend logic rarely use ai ml algorithm article explores ai ml use case improve scm process thus making far effective use case presented article conceptual level need analysis detailing implement sum successful use ai ml scm expected provide path toward intelligent automated self healing supply chain thus certainly important area focus scm definition purpose key process summarized following paragraph definition purpose supply chain management defined design planning execution control monitoring supply chain activity create net value building competitive infrastructure leveraging logistics synchronizing supply demand measuring performance typical supply chain typical large supply chain may network consisting dozen source central warehouse hundred storage point thousand po cum storage endpoint linkage network referred transportation lane define product transported stored multi echelon supply chain inventory management extremely complex diagram typical supply chain business process core business process scm solution briefly described demand planning dp business process demand capture implemented using combination simple statistical function moving average demand number manually entered dp often done multiple time horizon short term month medium term quarter long term year short term demand number finalized based multiple input viz statistical prediction entered sale team derived long term planning organization often implement approval step demand number verified modified approved scm manager considering factor business target dp also includes many functionality splitting demand entered higher level hierarchy e g product group lower level granularity e g product grade based proportion derived earlier etc needle say time horizon size time bucket reduces say daily level forecast accuracy drop significantly demand number thus finalized released next module supply planning desired time bucket day week etc supply planning supply optimization supply network planning snp module generates optimal supply plan considering current inventory level storage point inventory norm push pull strategy production capacity constraint defined many design aspect supply chain core snp involves generating solving large mathematical optimization problem using mixed integer linear programming milp technique operational research tool repository milp effective optimization technique variable defined either continuous integer taking binary value integer variable typically used define step function similar constraint optimization problem generated scm solution based various configuration master data e g transportation lane capacity etc constraint production capacity course demand number output snp module e optimal supply plan released next production planning module production planning scheduling output snp used production planning detail scheduling based specific constraint production environment e g sequencing product dependency batch processing etc addition capacity every step delivery load builder integration transactional erp involves generation delivery prioritized avoid stockouts endpoint generation truckload etc also integrated transactional erp system erp vendor leading vendor include sap oracle among many others challenge issue faced commonly experienced challenge implementing scm process listed large number product skus high level inventory non moving product skus lost sale area due shortage surplus area reactive logistics non adherence plan e discipline issue departmental focus e g sale team would like enter demand number line sale target generally higher side often result bullwhip effect entire supply chain scm kpis typical kpis used monitoring scm improvement demand fulfillment index inventory day supply average forecast accuracy weighted average delivery performance dispatch adherence production adherence procurement adherence end end cycle time procurement sale ai ml application explored implemented scm space discussed section ai ml based demand planning leading scm vendor offer functionality regression modeling causal analysis forecasting demand functionality embedded dp module however rigorous advanced approach desired one forecast demand number outside scm system using advanced modelling upload back scm system advanced modeling may include using advanced linear regression derived variable non linear variable ridge lasso etc decision tree svm etc using ensemble method model perform better embedded scm solution due rigor involved process addition one implement weighted average ranking approach consolidate demand number captured derived different source viz modelling entered sale team long term planning etc whether deep learning neural network help forecasting demand better way topic research neural network method shine data input image audio video text available however typical traditional scm solution readily available used however maybe specific supply chain digitized use deep learning demand planning explored ai ml based segmentation clustering product generally implementing scm solution abc analysis skus classifying product based importance e sale value volume quantity margin etc done classification used configuring applying implementing customized strategy every class analysis make implementation effective class product need completely different treatment compared c class example class product organization may allow change number predicted model better approach segmenting skus using clustering e g k mean applying different strategy segment however interpretation segment cluster done manually business analyst data scientist segment may interpreted differently different analyst maybe future ai based algorithm available provide better interpretable solution clustering problem reinforcement learning rl advisor reinforcement learning rl science decision making third paradigm machine learning supervised unsupervised learning learning optimal behavior environment obtain maximum reward optimal behavior learned interaction environment observation responds like child exploring world around understanding action help achieve goal rl extensively used playing game like chess similarly supply chain environment rl algorithm observe planned actual production movement production declaration award appropriately hence rl algorithm used fine tune transaction supply chain unlike game like chess rl used advisor supply chain however real life application rl business still emerging hence may appear conceptual level need detailing ai ml based autocorrection supply chainnormally supply production planning process run batch job weekly fortnightly monthly basis feasible run daily possibly impossible run real time basis rather may make sense run real time create confusion however lot change daily basis ai ml algorithm amend adjust refine plan daily basis without running logic embedded scm system useful business user suggested approach include rule based heuristic ai ml algorithm analyze cumulative status supply chain e g date month amend supply production plan coming day week ai ml based production planning scheduling supply planning supply network planning optimizes production using production capacity broad level however optimization scheduling done using advanced optimizer may consider additional constraint sequencing constraint specific production process industry feasible optimize using milp optimization algorithm specialized approach like genetic programming used one borrow sophisticated optimizers readily available python library digital twin supply chain modern supply chain well connected iot device transaction updated real time hence possible compute majority kpis real time information kpis made available management real time using suitable dashboard using ai ml one simulate balance period predict kpis achieved end period given status supply plan past trend decision maker able take corrective action month without waiting month end many scenario kpis reported month end quarter end sometimes becomes ritual time scm team would already initiated action towards next period ai based chatbots core scm teama chatbot useful various user department sale purchase production others access scm database support query using nlp module effective approach answering query written natural language classifying odd sale movement productsa report showing odd product movement production declaration useful help management focus specific movement however obviously need labeling done past period e classifying labeling movement odd ok delivery truckload generation involves generation delivery truckload considering following stock replenishment order delivery priority based inventory status delivery time inventory norm far possible generating full truckload storage point make sense order simplify logistics avoid multi drop scenario suboptimal partial truckloads avoid excess delivery pushed storage point forming truckload course depending inventory available production centre currently various strategy optimizers used generate delivery schedule truckload future ai ml may able provide perfect solution problem balance requirement mentioned today scm solution quite mature offer good solution streamline improve supply chain adoption theabove mentioned ai ml based use case progress toward automated intelligent self healing supply chain maybe scm vendor slowly incorporate solution architecturally may appear shown diagram key takeaway article existing scm process algorithm explained area ai ml application scm identified use case ai ml make scm solution effective real time self healing intelligent medium shown article owned analytics vidhya used author discretion related
26,https://www.analyticsvidhya.com/blog/2022/06/stop-storing-data-in-csvs-vs-feather/,article published part data science blogathon use panda package process transfer data around working project performs admirably datasets intermediate size dataset many observation however process storing loading data grows slower kernel eats time forcing wait data reloads result csv file format loses appeal time csv type data storage available fact probably last option think sticking plan manually alter saved data waste time money consider following scenario acquire vast amount data store cloud chose csvs since conduct study file type cost control cut half simple change change guessed changing file format learn feather data format today quick lightweight binary format storing data frame feather portable file format us arrow ipc format store arrow table data frame language like python r feather designed early arrow project proof concept rapid language agnostic data frame storage python panda r feather file format limited python r programming language used major programming language data format meant kept long time original goal facilitate interchange r python script well short term storage general one stop dumping file disc forgetting year efficient form available may used python using panda standalone library next post teach combine two follow along need install feather format terminal command follows pythonlet start basic loading library generating sizable dataset follow along need feather numpy panda seven column ten million row random number dataset python code next save locally save data frame feather format panda execute following command use feather library way much difference two file stored locally use panda specialized library read first let look panda syntax using feather library change represents binary data disc using apache arrow columnar memory specification speed read write operation especially significant encoding null na value kind variable length utf string feather part apache arrow project whole disk representation feather creates reduced schema metadata following column type currently supported know following section compare feather file format csv file format term file size read time write time answer simple utilize feather csv need update data fly still let put thing test time took save data frame previous part locally shown graph significant difference native feather time faster csv matter use panda work feather file however speed boost tremendous compared csv next look reading time long take read identical datasets various format big disparity yet csvs take long time read sure take disc space much following visualization provides answer question csv file see take twice much space feather file choosing right file format critical store gigabyte data daily basis aspect feather demolishes csvs preceding graph clearly show native feather ideal file format utilize order save time space money reduces file size half could possibly better summarize replacing csv read csv feather read csv reading feather save significant amount time disc space take consideration working next big data project shown one stop shop saving time space seen change data employing various storage file format next time work data make smarter decision know feather see medium shown article owned analytics vidhya used author discretion
27,https://www.analyticsvidhya.com/blog/2022/06/data-engineering-a-journal-with-pragmatic-blueprint/,article published part data science blogathon recent day consignment data produced innumerable source drastically increasing day day processing storing data also become highly strenuous emerging technology data engineering artificial intelligence machine learning algorithm help u handle data intelligently make useful business need article concentrate much step data engineering performed real life example process data engineering make data available accessible data driven job achieved via etl extract transform load pipeline first step data engineering extract data various source like iot device batch video uploaded periodically medical report etc data could type aim take data reliably process extraction preprocess data convert required format data various source always fit querying analysis modifying make suitable format transform stage etl pipeline following step usually normalization modeling data cleaningnormalizing data help make accessible user also achieved major step removing duplicate data handling fixing conflict data making sure data specified data model step also added major step involved data normalization data cleaning associated data normalization main difference data cleaning include step make data uniform complete data normalization focus making heterogeneous data fit data model major step involved data cleaning typecasting data required keeping date time field format elimination corrupt unusable data identifying missing field fill appropriate data possible transform next step etl pipeline load store transformed data particular location specified format better accessibility user modified data stored data lake data warehouse usually data stored data lake immediately extraction raw data give high agility data processing limited data warehouse fixed configuration also unprocessed data used data scientist exploratory data analysis hand data warehouse central repository data stored query able form data warehouse able store structured unstructured data include various file format like pdfs image audio video file nowadays data engineer mainly using lambda architecture processing real time data lambda architecture la help support unified pipeline real time data processing la designed based layer batch layer serving layer speed layer help process real time data based batch processing stream processing method hybrid approach batch layer main work layer manage master dataset pre compute batch view whatever data come fed batch layer well speed layer help real time processing serving layer layer take near real time view speed layer batch view come batch layer index index queryable format latest batch view speed layer layer creates real time view recent data provide complete view data user process since data engineering field grown significantly huge demand engineer work traditional batch processing method nowadays many organization following multiple batch processing organization started working stream processing data pipeline mainly complexity issue involved stream processing method work expected complexity overcome introduction powerful easy use framework apache flink apache spark etc example let u consider real time scenario help u get better understanding field data engineering example consider paid website provider million user sign use service account netflix amazon prime etc multiple user different country use account password lead huge loss company providing service company want avoid fraudulent behaviour also track user behaviour login logout purchase create delete account setting change etc image architecture processing example takenthe proposed system process data using apache kafka help receive data various source store reliably also allows different application read data order received apache flink read event apache kafka topic process identify fraudulent account user action logging fraudulent account detection account considered fraudulent account based account login history thing need considered monitoring login activity fraudulent activity found alert triggered event alerting system hand recorded user action loaded database apache kafka topic real time analysis action taken based identified behavior processing data apache flink delineated example understand importance data engineering process data efficiently secure manner main role data engineer design pipeline way acquire data without loss clean optimize store properly make accessible user disparate division across organization key takeaway article listed medium shown article owned analytics vidhya used author discretion
28,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-writing-reproducible-pipelines-for-training-neural-networks/,hey reader getting andrey lukyanenko kaggle grandmaster board lead interactive datahour session u working senior data scientist consulting solution firm careem ten year extensive experience field analytics data science explaining neural network writing reusable reproducible pipeline training neural network book seat know want experience fomo data science concerned reproducibility company world investing heavily explore build data science application datahour join andrey demonstrating working deep learning training pipeline using pytorch lightning wrapper pytorch code hydra configuration file management andrey lukyanenko work senior data scientist solution consulting firm careem ten year extensive experience field analytics data science want create deep learning apps could benefit user customer making life better also adding value business grandmaster kaggle notebook ranked first kernel ranking kaggle discussion kaggle competition passion understanding data science working mastery machine learning python email u email protected chat speaker directly session register datahour take advantage wonderful opportunity visit youtube channel view recording missed previous episode datahour blog may read summary datahour session already held trouble enrolling would like meet u contact u email protected
29,https://www.analyticsvidhya.com/blog/2022/06/hypothesis-testing-in-inferential-statistics/,article published part data science blogathon hypothesis testing one important technique applied various field statistic economics pharmaceutical mining manufacturing industry suppose want know something took place certain medicine effective group differ one variable predicts another variable source ety comall want predict data collected statistically significantly different another article anyone want know understand concept hypothesis testing significant component inferential statistic step taken conduct hypothesis testing explained detail alright let begin hypothesis testing inferential statistical method required use sample data solve assumption population parameter characteristic describes population want define inferential statistic main reason study statistic make inference population interest using sample data hence inferential statistic branch statistic refers probability sample data represent population data sample obtained word like saying conclude population based know sample instance wanted know average salary domestic worker covid le impact people take vitamin c manufacturer produce quantity litre milk many example inferential statistic applicable following graphical representation step conduct hypothesis testing step select appropriate statisticsthe test chosen based whether looking relationship difference group whether assumption particular test satisfied however step involves exploratory data analysis help u choose test conduct hypothesis looking difference group hypothesis test used follows independent sample test parametric procedure test used compare two independent sample mean one group effect mean group term test whether sample mean statistically significantly different example measuring woman weight one group affect importance men another group dependent sample test parametric procedure test also referred matched paired sample test related sample test repeated measure test used compare mean two related sample short test whether average difference mean different zero instance experimental design matched pair pre test post test experiment use dependent sample test one sample test parametric procedure test allows u compare sample mean known population mean provided population standard deviation unknown check mean statistically significantly different mann whitney u test non parametric test corresponds independent sample test categorised group ranked sum test used measure two independent sample calculated ordinal level follow ranked order wilcoxon signed rank test non parametric procedure test corresponds one sample test used test sample median statistically significantly different hypothesised population median value non parametric procedure test corresponding dependent sample test measure whether difference two related dependent matched sample calculated ordinal level follow ranked order friedman test non parametric procedure test corresponds repeated measure anova used measure whether statistically significant difference median three group subject score show group class non parametric procedure test corresponding one way anova used measure whether statistically significant difference median three independent group repeated measure anova parametric procedure test measure whether mean three dependent variable statistically significantly different one way anova parametric procedure test measure whether mean three independent group statistically significantly different looking relationship group hypothesis test made used follows simple linear regression parametric procedure test us general linear model analyse relationship independent dependent variable measured scale level continuous simple linear regression us independent variable x axis predict dependent variable axis u instance predict crime rate increase population relationship variable described mathematically equation straight line ax b intercept represents constant value x represents constant value x b represents slope straight line x represents value used prediction pearson correlation coefficient parametric procedure test measure strength association relationship two quantitative variable range indicates perfect positive linear association independent dependent variable meanwhile indicates perfect negative linear association independent dependent variable spearman rank correlation coefficient non parametric procedure test corresponding pearson correlation coefficient measure strength direction relationship two ordinal ordered variable also range negative one positive one chi square test non parametric procedure test used determine relationship two qualitative variable example suppose want determine relationship gender male female highest educational level apply chi square test step state null alternative hypothesisin hypothesis testing two hypothesis null hypothesis state difference mean difference would occurred due chance randomness denoted ho alternative hypothesis state one group mean statistically significantly different denoted ha h research designstep select level significancethis step requires u state whether test one tailed two tailed select level significance also known alpha level help u setting critical value commonly used alpha level source statistic made easystep calculate test statistichere test statistic calculated either hand test value found using specific formula particular test compared critical value student statistic table using software like r python spss excel datatab etc software calculates test statistic computes probability value p p le alpha level null hypothesis rejected step make decisionhere decision based following rule calculated test statistic exceeds critical value test significant null hypothesis rejected implying test statistically significantly different calculated test statistic le critical value test significant fail reject null hypothesis implying test statistically significantly different research designin article discussed concept hypothesis testing branch inferential statistic followed definition inferential statistic furthermore explained step taken conduct hypothesis testing step one select appropriate statistic step two state null alternative hypothesis step three select level significance step four calculate statistic step five make decisionyou reach via email email protected medium shown article owned analytics vidhya used author discretion
30,https://www.analyticsvidhya.com/blog/2022/06/spring-security-oauth2-with-keycloak-pkce-authorization-code-flow/,article published part data science blogathon time focused securing application strong security mechanism always missed protecting credential hacker directly use client id secret without considering threat attack instead giving credential going use pkce mechanism getting topic recommend read previous article get better understanding previous spring security oauth keycloak article already covered basic oauth understood authorization code flow grant type implement using spring mvc application check previous article pkce stand proof key code enhanced authorization code flow grant type created used public facing client like web application developed using angular react vue etc also mobile application pkce authorization code flow mainly considered best practice follow using public client main reason using kind different authorization code flow remember previous spring security oauth keycloak article part authorization code flow get access token idtoken pair authorization server client need make post request client id client secret authorization code risky developer use source code find client secret store client thing applies mobile application apk file de compile source file reason new type authorization code flow designed pkce authorization code flow flow similar authorization code flow couple additional step need maintain client secret anymore inside application source code pkce authorization code flow additionally using code_challenge code_challenge_method parameter code_challenge base encoded random string generated hashing encoding another value generated client called code verifier code_challenge_method configured inside authorization server first configuring client recommended value cryptographic hashing function client make request authorization server server responds login page asking user authenticate user logged authorization server return authorization code similar authorization code flow request access token client send code verifier value along authorization code part post request token endpoint authorization server receives post request validates authorization code code verifier value responds access token idtoken pair let see flow practical example keycloak dashboard configuration need install run keycloak server start login key cloak administration console make change enable access type public standard flow enabled provide value valid redirect uri http localhost angular app address provide web origin allow cors access authorization server going provide permit origin note using production application please provide value provide valid origin redirect uri mean front end application running server provide host detail server instead allowing origin next value need configure pkce enhanced code_challenge _method find value advanced setting client configuration let dive code open angular project first need install npm package called angular oauth oidc adding package run npm install command ad package package installed creating new file called auth config t inside file providing six field issuer issuer uri contains list configuration endpoint exposed authorization server http localhost realm oauth demo realm redirecturi value configuring client keycloak section instead hard coding value providing window location origin clientid keycloak value oauth demo pkce client responsetype going code following authorization code flow mechanism strictdiscoverydocumentvalidation something relevant angular oauth oidc library used list endpoint exposed issuer uri endpoint contain base uri issuer uri case issuer uri us base uri http localhost use true provide false scope providing default scope inject oath service class provide configstill configured file calling html file let configure app component html login logout buttonfinally define oauth module app module tsnote upcoming configuration done spring boot application configuration finished create component app service tshere make http get request throughout resource server spring boot application let call app component tslast need bind html pagerefresh browser login view textlogoutafter finishing configs let start application npm start first adding dependency pom xml spring boot starter oauth resource server enable resource server capability inside spring boot application spring security oauth jose enables java script object signing encryption framework used securely transfer claim party mean transferring jwt json web token jws json web signature jwe json web encryption jwk json web key spring boot starter security enable spring security configure resource server propertiesok configured resource server let create endpoint creating controller homerestcontroller enabling rest api cross originlast thing configure spring securityfor creating config package creating class securityconfig class extend web security configure adapter way overwrite spring boot default security config inside method customize spring security behave first going make sure request resource server authorized first adding authorizerequest anyrequest authenticated finally completed front end back end implementation let dive demo testing navigate endpoint localhost angular endpoint host click login redirect keycloak login page check request parameter see code_challenge code challenge methodonce entered credentialsif check request parameter see code angular use code make post request token endpoint code verifier value used verify code challenge method response contains access token refresh token id tokeni hope understood article full flow explanation could see need share credential network call also learned pkce authorization code flow configuring front end client case angular application configuring resource server case spring boot application configuring authorization server case keycloak server please continue reading article learn keycloak latest technology trend learned far medium shown article owned analytics vidhya used author discretion related
31,https://www.analyticsvidhya.com/blog/2022/06/energy-analytics-101/,ai machine learning two time buzz worthy business term result business across industry looking implement technology improve automate core process energy industry exception renewable energy company clean energy corporation comprising solar wind hydro battery nuclear harvested power machine learning year possible reduce cost improve prediction increase rate return portfolio trend likely continue even significant speed company involved energy industry consumes large amount electricity chance machine learning artificial intelligence boost performance exactly let take closer look many way machine learning ai utilized transform energy sector positively listed popular application currently development grid management systemin energy field grid management one exciting application artificial intelligence electricity generated delivered customer set complex network also referred power grid one challenge power grid power generation demand must always match otherwise blackout failure occur although many mean storing energy common method pumped hydroelectric storage involves pumping water specific elevation harnessing allowing fall onto turbine difficult predict much electricity grid produce using renewable energy contingent upon several factor sunlight wind b demand forecastlarge swing demand make expensive country produce energy using renewable energy source country shifting towards green energy becoming increasingly difficult react effectively swing demand example germany plan use renewable energy cover electricity consumption country germany two significant challenge overcome first swing demand uncommon electricity demand skyrocket specific day period year example christmas second whether volatility either case supplemental station fossil fuel powered facility may required satisfy excess demand wind blow sky cloudy c predictive maintenance fault detection apart matching energy production energy consumption artificial intelligence also significant factor ensuring power grid reliability robustness event massive blackout ohio triggered low hanging high voltage power line brushed overgrown tree indication incident occurred power system alarm failed electric company discovered issue three power line fell similar reason ultimately oversight failed entire grid addition blackout lasted two day affected million people people died billion lost demand side response interruptionsmany country partner company analyze predict weather data electricity forecast amount wind solar energy generated given time germany initiated project eweline result country cover excess electricity demand using non renewable energy whenever required incorporate enormous historical data set train machine learning algorithm effectively predict weather power change well data collected wind turbine solar panel also utilize historical data set train machine learning algorithm e trade schedule automationcritical application sourced using ai machine learning ascertain best way use energy flexibility site within energy market real time include common dataset used forecast automating energy control based several source includes evolution internet thing iot using machine learning technique predictive maintenance implemented sensor collect operation time series data power line machinery station data timestamp machine learning algorithm detect whether component fail x amount time n step furthermore also expect remaining useful life machinery subsequent failure occur addition predicting machine failure efficiently preventing out downtime algorithm optimize maintenance activity periodicity thus reducing maintenance cost example united kingdom started installing phasor measurement unit prevent power line failure measure result event london blackout prevented entirely ultimately ai machine learning help energy company shift reactive predictive maintenance identify new resourcesin addition weather forecasting renewable energy source optimization artificial intelligence used explore drill fossil fuel energy source robotic device used explore gather data floor conduct analysis based information identify new location drilling oil natural gas smart energy consumptiona switch renewable energy source matter government electric company company like google microsoft reducing energy consumption impact environment bottom line positively google known intelligent data center established world data center generate enormous amount heat need great deal electricity cool deepmind ai implemented state art machine learning model churn energy cooling google data center reduced utility cost helped reduce overall emission reducing carbon tax obligation courtesy image via visual capitalistwith developed country striving completely green economy maintaining secure balanced reliable power grid priority machine learning artificial intelligence certainly long way go energy sector implementing renewable energy source ai controlled power distribution grid smart management create digital power grid enables two way communication utility company consumer goal smart grid provide consumer continuous data collection display improve energy consumption behavior smart meter sensor alerting device well predicting demand improving performance reducing cost preventing system failure also fed machine learning algorithm smart grid implemented several developed country still long way go implementing renewable energy source ai controlled power distribution grid management energy machine learning booming new application emerging every year everything except energy generation set improved data technology key balance ai good theory can not expense continued emission greenhouse gas e g computer vision geospatial analysis still used identify natural gas reserve pushing u away commitment net zero fueling continued reliance fossil fuel ai undoubtedly future clean green energy revolution optimistic implementation application green residential commercial construction development grid management predicting uncertainty non synchronous fuel well exciting medium shown article owned analytics vidhya used author discretion
32,https://www.analyticsvidhya.com/blog/2022/06/gradio-app-for-detecting-whether-the-audio-content-is-sexist/,article published part data science blogathon sexism discrimination prejudice based individual gender belief one sex worthier valuable role play society like form prejudice discrimination serf perpetuate status power disparity group society although sexism affect anybody regardless gender woman girl disproportionately affected rising number developmental educational psychologist expressed concern sexism directed woman child considering gravity situation critical establish effective method detecting sexism school college office public transit etc accommodate need attempt create app detect whether sexist language exists spanish audio key concept feed spoken audio input asr module module convert spanish speech text resulting audio transcription transferred sexism detection module module discern whether resulting audio transcription hence spoken audio objectionable sexist content figure diagram depicting flow application module asr module converting spanish speech text leveraging jonatasgrosman wavvec xl r b spanish pre trained model hugging face hub model trained contributed jonatas grosman basically fine tuned version facebook wavvec xl r b spanish common voice dataset leveraging model one need ensure speech input sampled khz module sexism detection pre trained model used sexism detection twitter_sexismo finetuned robertuito exist essentially fine tuned version pysentimiento robertuito base uncased exist dataset furthermore also utilize gradio interface class create ui machine learning model subsequently deploy app hugging face space step guide creating gradio app acquiring spanish audio transcription determining whether resulting transcription hence audio contain sexist content please go website create hugging face account already one finished creating account go top right corner page click profile icon sent new page asked name repository want create give space name choose gradio sdk option clicking create new space button result repository app established difficulty setting please watch demonstration video included select add file button create new file name requirement txt list necessary dependency broken code section clarity make thing easier understand go section code one one importing necessary library defining function loading correcting speech loading feature extractor instantiating asr pipeline instantiating pipeline sexism detection defining function asr instantiating pipeline sexism detection defining function output audio transcription result sexism detection module creating ui model using gr interfacenext utilize gradio interface class establish ui machine learning model providing function desired input component desired output component allow u quickly prototype test model case function asr_and_sexism_detection providing audio input use microphone drop audio file regard use code providing input since intended output string use output gr output textbox label output text displaying string output finally launch demo call launch method wish test audio file stored locally ensure sure uploaded location listed example shown code snippet worth mentioning component specified either instantiated object string shortcut upload audio file click following tab order listed file version contribute upload file get error please go see log tab right next spot runtime error shown take cue error log fix error space running error free work like link space http huggingface co space cvmx jaca tonos spanish audio transcription based sexism detectionto load space hugging face hub recreate locally pas space interface followed name space let load space built hugging face space limitation associated pre trained model used asr pre trained model asr play critical role audio accurately detected transcription wrong good chance output sexism detection module erroneous solution aforementioned limitation get around limitation pre trained asr model trained example audio closely reflect auditory environment app used tested meet need limitation associated module evaluating pre trained model used sexism detection module extensively found scenario model performing well however certain input providing correct output solution aforementioned limitation evaluating determining kind audio input sexism detection module fails accurately output pre trained model trained kind example audio get reliable output creating one stop app sexism detection using audio transcription country indigenous language country indigenous language find one two module ie asr sexism detection module functioning use case interested perhaps consider training particular model targeted improvement class model often misclassifies try building app using gradio block article learned create gradio app detect whether audio content sexist two step problem ie speech text conversion sexism detection overall accuracy dependent module individually result asr module erroneous sexism detector output likely erroneous well make app robust attempt develop system take multi modal input account discerning whether audio event qualifies sexist lastly sum saw create app detecting whether audio contains sexist content load recreate built space hugging face space potential application audio transcription based sexism detection limitation way circumvent limitation thanks reading question concern please post comment section happy learning medium shown article owned analytics vidhya used author discretion
33,https://www.analyticsvidhya.com/blog/2022/06/top-10-web-3-0-technologies-that-will-shape-our-future-world/,article published part data science blogathon internet come long way since web hypertext markup language html defines layout delivery website web web technology web html remain core layer connects data source data source stored may change previous generation web web age practically website application rely type centralised database supply data allow functionality instead using centralised database web apps service employ decentralised blockchain primary notion behind blockchain sort distributed consensus rather arbitrary central authority web might built ai semantic web omnipresent characteristic purpose using ai provide end user quicker relevant data website employ ai able sort provide information belief certain user would find useful result include website voted people social bookmarking search engine produce better result google human hand control outcome ai might used distinguish real fraudulent result resulting outcome comparable social bookmarking social medium without negative feedback image http forkast news web shape crypto future semantic web goal categorise store data manner system learn specific data mean put another way website able understand word used search query manner human would allowing create distribute better content ai used system well semantic web educate computer data mean ai utilise information several significant web technology including following assist define third generation web likely blockchain decentralised ledger record peer peer transaction participant confirm transaction without requirement central clearing authority using technology potential application include financial transfer settling transaction voting many difficulty important conceive blockchain technology form next generation business process optimization software business standpoint collaborative technology blockchain promise enhance business procedure organisation cutting cost trust dramatically result may provide much better return per dollar invested typical internal investment image http coinpedia org news blockchains cross chain communication needed financial organisation looking blockchain technology may used revolutionise everything clearing settlement insurance article assist comprehending development determining response effort tamper record break chain central authority manage since validated copy database scattered internet blockchain technology may used retain record transaction application although people relate cryptocurrencies discus next cryptocurrency sometimes known crypto currency crypto type digital virtual currency employ encryption safeguard transaction cryptocurrencies operate without central issuing regulating body instead relying decentralised system track transaction create new unit blockchain technology used cryptocurrency keep track much money circulation owns much peer peer system allows anyone send receive payment anywhere cryptocurrency payment exist solely digital entry online database describing specific transaction rather physical money carried around exchanged real world transaction make cryptocurrency fund recorded public ledger cryptocurrency held digital wallet image http www finance monthly com bitcoin motion website need succeed cryptocurrency moniker cryptocurrency come fact employ encryption authenticate transaction implies storing sending bitcoin data wallet public ledger requires complex code encryption goal ensure security safety bitcoin first cryptocurrency created still well known today much fascination cryptocurrencies stem desire trade profit speculator driving price high time semantic web next step web growth semantic web enhances online technology demand creating sharing connecting material search analysis based capacity grasp meaning word rather keywords number ultimate goal semantic web according creator tim berners lee allow computer better modify data behalf go say word semantic context semantic web mean machine processable computer data term web connotes navigable space interconnected object uri resource mapping machine learning technology major field artificial intelligence exploded popularity recent year technology crammed inside cellphone application like apple siri function talk intelligent agent parse asking thanks natural language processing nlp image http sourceedge com digital apps ai mlmachine learning also used forecast human want behaviour processing vast volume data real time intelligent network connected gadget everywhere thanks internet thing iot open lot possibility gathering data turning something useful combining power natural language processing computer web able discern information way people resulting faster relevant result meet need user grow increasingly intelligent term metaverse relate single form technology rather general movement people engage communicate technology today rapidly changing environment image http www dreamstime com illustration metaverse city htmlthe metaverse another ill defined notion appears overlap interconnect web concept either ever materialises metaverse concept human interact internet future produce consistent integrated user experience largely relies virtual reality vr augmented reality ar digital good mix natural environment metaverse engage web far embodied way computer graphic opposed computer graphic graphic make use three dimensional representation geometric data stored computer conduct computation display visuals web three dimensional design widely employed website service may seen museum guide computer game ecommerce geographical setting place decentralised autonomous organisation daos powerful secure method collaborate like minded people across world consider internet native business member jointly govern built treasury one access without group permission proposal voting used make decision ensuring everyone company get say ceo spend money whim risk shady cfo tampering account everything open dao spending restriction encoded code permissionless aka trustless blockchain organisation policy encoded utilising novel contract technology traditional organisation require extensive expensive administrative division established keep thing running every transaction history subject public examination daos make nearly difficult conduct fraud dao smart contract backbone contract establishes organization policy safeguard group fund one may modify rule contract live ethereum except vote fail someone attempt something covered code rule logic treasury also specified smart contract one may spend money without permission organisation eliminates requirement central authority daos instead group make choice collectively payment granted automatically vote passed edge computing networking paradigm focus placing processing near feasible source data decrease latency bandwidth use edge computing simple word involves executing fewer process cloud relocating local location user pc iot device edge server bringing processing network edge quantity long distance transmission client server reduced data may example processed locally transported central site aggregated implies pool processing power device network edge create massive decentralised supercomputer adequate computational capacity analyse data collected billion iot internet thing sensor smart home industry retail store significant issue edge computing provides solution satisfy demand also saving bandwidth delivering data rapidly smart contract self executing contract condition buyer seller agreement put directly line code code well agreement contains disseminated throughout decentralised blockchain network transaction trackable irreversible programming regulates execution smart contract eliminate need central authority legal system external enforcement mechanism carry trustworthy transaction agreement distant anonymous participant smart contract allow provision financial service creation legal agreement party considerably lower cost traditional contract also lot fairer manipulated turned smart contract computerised transaction protocol implement contract condition according nick szabo american computer scientist devised virtual currency called bit gold transaction traceable transparent irrevocable smart contract deployed blockchains utilising centralised programme utilise cloud based service like google doc google full access data document including ability read manage exchange may store data cloud work others quickly take advantage slew cloud app benefit could benefit cloud service without adhere centralised authority decentralised apps sometimes known dapps come play apps run online computation ethereum blockchain computation compensated ethereum gas fee decentralised apps dapps digital programme application operate blockchain peer peer pp network computer rather single computer dapps also known dapps exist outside control jurisdiction single authority dapps frequently constructed ethereum platform may used wide range application including gaming banking social medium dapps run either peer peer pp blockchain network bittorrent tor popcorn time example software operates computer part peer peer pp network numerous user consuming feeding seeding material role time web longer dream reality least many case least many case reality cognitive technology like expert ai making feasible language crucial many aspect internet possibility unlimited semantics natural language processing fundamental component medium shown article owned analytics vidhya used author discretion
34,https://www.analyticsvidhya.com/blog/2022/06/getting-started-with-google-big-query/,article published part data science blogathon today data driven age enormous amount data getting generated every day various source social medium e commerce website stock exchange transaction processing system email medical record etc data combination structured unstructured semi structured data come stream batch form data keep growing exponentially time source http kochasoft com fr uncategorized fr google bigquery datawarehouse hana system organization across globe focusing get useful insight generated data data warehouse came picture data warehouse system aggregate data produced different data source data analysis effective decision making big query serverless data warehouse platform service paas service provided google cloud platform big query also provides built capability machine learning query engine sql article discus overview big query underlying architecture big query benefit use case big query import query data big query big query serverless scalable distributed data warehouse platform service paas service provided google cloud platform store analyze data big query access data directly external source using federated query big query store data capacitor columnar storage format optimized analytical query machine learning predictive analytics capability also provided using big query ml big query provides business intelligence support using data studio looker etc big query provides high security using identity access management iam big query provides high flexibility compared data warehouse separating compute engine storage choice user source http cloud google com bigquerywe seen brief overview google big query till let understand architecture big query detail big query serverless flexible architecture allows storage computing scale independently type architecture offer cost control customer built engine sql help effectively analyze data without worrying system engineering database operation source http cloud google com blog product data analytics new blog series bigquery explained overviewbeneath surface big query architecture us large number multi tenant service driven low level google infrastructure technology colossus jupiter dremel borg colossus storing data big query us colossus colossus ensures data security performing data replication data recovery distributed management dremel dremel multi tenant cluster used computing purpose internally sql query turned execution tree dremel executing query jupiter jupiter network used storage computing talk borg borg performs orchestration big query source http cloud google com blog product data analytics new blog series bigquery explained overviewbelow benefit using google big query improved business intelligence using big query integrate data multiple source whether external data source data service gcp easy manage data big query contributes better decision making process scale data large may contain complex data relationship b enhanced data quality consistency big query store data columnar storage format compressing data using compression algorithm data stored format ensures data uniformity thus big query provides enhanced data quality consistency c scalability real time performance easily increase decrease amount data number data source big query also directly query external data source big query using federated query security provides various security option using identity access management role e high availability fault tolerance data stored big query replicated across different region ensure high availability better fault tolerance use case google big query get started working big query either create account gcp use big query sandbox use sandbox visit link log google account http console cloud google com bigquerynow follow step step logging sandbox click create new project step enter project name parent organization folder name click create project get created select view project sandbox step visit http www kaggle com datasets surajjha store area sale data download supermarket store branch sale analysis dataset step click resource big query projectname create datasetstep provide store dataset id data location click create dataset step click store create tablestep select create table upload choose file downloaded kaggle provide table name table type schema partitioning setting step go created table clicking go table button see schema detail row access policy edit schema want step query table click query button open query editor step find detail store count daily customer greater run query query editor result generated query execution article seen store huge amount data using google big query gain useful insight collected data major takeaway big query seen big query provides machine learning predictive analytics capability using big query ml got deep understanding architecture big query function performed various low level google infrastructure technology seen benefit using big query learnt import query data big query help sandbox apart also saw use case google big query used medium shown article owned analytics vidhya used author discretion
35,https://www.analyticsvidhya.com/blog/2022/06/gato-a-new-generalist-artificial-intelligence-agent/,article published part data science blogathon simulation human intelligence process machine particularly computer system known artificial intelligence expert system natural language processing speech recognition machine learning machine vision example ai application task using different type system ai example artificial intelligence computer vision based model able handle nlp related task vice versa similarly text classification model machine learning fail handle machine translation problem vice versa ever thought model capable task without model architecture change single model mimic human brain multiple task without significant external world influence right track thinking artificial general intelligence agi intelligent agent ability understand learn intellectual job person known artificial general intelligence artificial intelligence ai concept creating machine think act learn way people artificial general intelligence agi intelligence machine capable cognitive task human system artificial general intelligence would capable understanding world well human well learning wide range activity key goal certain artificial intelligence research nowadays well popular subject science fiction futurist study majority ai research happening today trying obtain least minimum level agi end product recently deepmind british artificial intelligence subsidiary alphabet introduced latest promising agi model gato large volume data scientist world suggests gato world first agi blog trying introduce basic interesting detail gato model using single neural sequence model task lot advantage eliminates need hand craft policy model area proper inductive bias sequence model consume data serialized flat sequence enhances amount diversity training data furthermore even cutting edge data computation model scale performance continues increase mentioned introduction section kind neural architecture multiple task known multi model neural network system called artificial general intelligence agent several multimodal architecture available today showing minimum level agi nature gpt model detail regarding gpt please check previous article deepmind claimed day ago developed generic ai perform task google claim accomplish job closest come human level performance variety setting deepmind instantiated gato single large transformer sequence model another important point every task done gato us weight gato generate caption photograph stack block real robot arm surpass human atari game navigate simulated landscape obey direction single set weight datasetsgato trained using variety datasets including agent experience simulated real world setting well natural language image datasets table describes datasets used gato training data contained final dataset used train gato model widely spread different domain aregato trainingduring training data variety job modality serialized flat sequence token batched processed transformer neural network work way big language model loss function applied target output text certain action due masking training phase gato described figure gato main design approach train much relevant data possible text image view discrete continuous data enable training multiple nature data gato serialize data flat sequence token process called tokenization multiple way tokenization method mentioned employ canonical sequence ordering transforming data token etcthe idea organize everything structure certain sequence based task seen training phase image tokenization sequencing operation perform depending nature inputthe embedding vector generated billion transformer gato us layer b parameter decoder transformer embedding size post attention feedforward hidden size model remains linguistic model predicting following word based sequence model given continuous value proprioceptive input joint torque set supplementary subwords mapped top text vocabulary range gato one method converting rl problem conditional sequence modelling problem model like gato use big context window forecast next best action rather approximating state value function learning policy gato inference auto regressively trained mean anticipates next input example receives text attempt guess next statement action occur case game model receives embedding make prediction based carried simulated environment current state tokenized embedded sent back model produce another prediction check image understanding prediction process gato see also fixed prompt component simply tell model kind response expect collection input lead multitask behaviour model mean model fed previously recorded token sequence specific task rather task type id thereby priming context window analysisnow time showcase power gato gato final result various activity shown belowgato image captioninggato conversational agentwhen observe gato result deeply understand result promising extent single model able handle multiple task extent major breakthrough data science research community even though obtained result meeting human level obviously gato powerful model still far human level perception gato model size scaling curve highly promising although largest model utilized billion parameter decoder tiny transformer nowadays dall e param count billion glide param count billion anyway work demonstrates add rl task transformer based generalist text image model hat deepmind nowadays research work artificial intelligence trying achieve artificial general intelligence behaviour result perspective gato gamechanger domain deepmind put lot effort bring generalized nature behaviour gato decoder model us billion parameter size transformer sequence model work well multi task multi embodiment policy variety setting including real world text vision robotics also show promise learning shot distribution assignment instead starting scratch model could utilized default starting point learning new behaviour prompting fine tuning future even though capable multiple task gato size small compare newly published model ai like gpt dall e result gato multimodel architecture scalable wide range blog tried explain basic property gato model detail kindly check official base paper end day see evaluation result gato sure still wait arrival new model reach human level behaviour many task let hope meet new real agi model soon happy coding medium shown article owned analytics vidhya used author discretion related
36,https://www.analyticsvidhya.com/blog/2022/06/data-visualisation-alluvial-diagram-vs-sankey-diagram/,article published part data science blogathon people tend minimal set information alluvial sankey diagram reality two diagram superficial similarity flip side two diagram completely different underlying superficial assumption let discus two diagram briefly article type flow diagram represents different form change made flow material energy cost measurable resource certain time bound considering physical appearance alluvial diagram contribution flow diagram named immediately alluvial fan note fan formed natural way soil deposited streaming water alluvial diagram variable calibrated vertical axis always parallel besides value represented block every axis height block meant represent height streamflow size container field represents size element within block connected stream field directly normally alluvial diagram considered alternate form parallel set although used categorical variable mainly used determine trend time long run sankey diagram form visualization responsible depicting given flow one set another thing connected named node connection node called link sankey diagram mainly used especially want showcase many many mapping different domain google analytics mainly adopts mechanism showcase traffic flow one page page website sankey diagram named captain sankey invented diagram steam engine efficiency sankey alluvial diagram different feature make two confusing two diagram node represented bar placed vertically height delivering idea volume count link applied node consist curve thickness related height node encodes count volume way node height operates also diagram user need highlight stripe across diagram help identifying wide stripe different point alluvial diagram mainly focus showcasing quantity appearance one state another throughout different process alternatively sankey diagram streamlined flow chart easily visualize quantitative value every phase whole process alluvial sankey diagram noteworthy difference considering consistency node line sankey diagram line set uniform length analytics alluvial diagram lot variation alluvial diagram mainly applied multi dimensional data analysis case emphasis mainly focus frequency proportion different dimension relate sankey diagram applied case require quantity visualization various stage entire process note sankey diagram visualize quantity incoming outgoing part flow help locate part dominant contribution place quantity get lost within process google chart tool easy simple use making sankey diagram opt use attractive chart data tool note process making sankey diagram using google chart easily customizable mechanism instigated viki sogn based norway highcharts allows user make sankey diagram online edit code well whenever need arises free tool used creating sankey diagram using drag drop option drag drop data website import online displayr online tool primarily meant making alluvial diagram come bunch customization option give user added advantage using besides tool free version allow user download final output diagram automated online tool requires web based application help generate alluvial diagram give chance upload data generate final output desire easily download final output share using email alluvial diagram v sankey diagram many thing common despite common feature two similar review internal feature need deep observation physical attribute useful capability well help identify remarkable similarity difference two alluvial plot alluvial plot considered form sankey diagram notable tool exploring categorical data play major role grouping categorical role flow easily tracked analyzed diagram sankey diagram good secret sankey diagram applied different scenario daily diagram mainly applied visualization material energy flow cost alternatively showcase energy mass flow form arrow proportional size flow create alluvial diagram download dataset questionnaire response dropbox click open raw drag preferred data box choose alluvial diagram list opt various dimension increase width visualization amped visualization alluvial structure alluvial diagram type flow chart represents change made flow material energy cost measurable resource time sense help identify pattern trend named alluvial fan flow water extending surface becomes flat refers shape flow medium shown article owned analytics vidhya used author discretion
37,https://www.analyticsvidhya.com/blog/2022/06/handling-streaming-data-with-apache-kafka-a-first-look/,article published part data science blogathon mention bigdata one type data usually talked streaming data streaming data generated continuously multiple data source say sensor server log stock price etc record usually small order kilo byte character like water stream flow without stopping hand say sale data company historical data processed batch data processed etl extract transform load pipeline data warehouse analysis streaming data can not afford luxury batch processing requires processed real time plethora software tool handle streaming data like amazon kinesis azure stream analytics google cloud dataflow confluent kafka etc would like focus apache kafka distributed event streaming platform kafka written java scala python api look basic apache kafka handle streaming data coding exercise kafka python event refers action result user action another application example click check button e commerce application creates event may trigger event downstream earlier event generated generally stored database one application may writing event onto database another may reading event database processing writing database reading database carried batch meant delay processing event imagine event handling banking transaction transaction online event get processed delay unacceptable today critical event need handled processed real time maybe even pushed downstream another event handling application downstream termed event driven application may noticed moment place order e commerce website receive intimation mobile device email event taken place event could trigger another event driven application vendor premise get ready shipment apache kafka one popular open source tool handling streaming data kafka central messaging system event streaming platform incoming message data come kafka outgoing data read kafka kafka intermediary producer consumer data source http sookocheff com basic understanding kafka log data source send message message arrive sequentially kafka consumed multiple consumer diagram message arrived message arrived message message published kafka can not changed want modify event create new event store log kafka message organized topic producer sends message topic consumer read topic mentioned kafka distributed system ie kafka run cluster cluster group computer instance broker node kafka cluster called kafka broker topic stored kafka broker message stored one broker provide redundancy aspect referred replication factor replication factor mean message stored broker mentioned event action happened world context business write read data kafka form event conceptualize event data key value timestamp optional metadata header say event bob withdrawn atm event look like producer client application publish write event kafka consumer application subscribe read event producer writes particular topic use case write producer application using python consumer application read kafka sends acknowledgement read message ready move next message kafka move offset next message thereby keeping track message read consumer anatomy kafka topic indicates topic divided partition visualized topic spread several bucket located different broker data particular topic split across multiple broker partition placed separate machine architecture enable multiple consumer read topic parallel result high message processing throughput new event published topic written one topic partition event event key written partition source http kafka apache org introin figure topic four partition p p two different producer application publishing topic independently writing event partition producer write partition event publishing appropriate partition mentioned earlier message within partition unique identifier called offset offset kind immutable message ordering sequence maintained kafka see specific message kafka cluster identified topic partition offset within partition kafka component called zookeeper manages kafka cluster status node list topic message discussion internet kafka version without zookeeper understand release yet ready production topic retention policy defined much long want keep data available kafka also pertinent note kafka provides tool called kafka connect application like postgresql database big query table etc solidify theoretical understanding kafka getting hand experience writing simple code python installation kafka vary o o one easier option available u use docker run kafka local machine write one docker compose file take care everything need run kafka docker docker compose installed get running irrespective o running local machine docker open source platform deploying managing containerized application docker allows packaging application along dependency container simplifies distribution application would recommend installation docker desktop link provided reference installed docker desktop setting tab dashboard enables docker compose go ahead create project directory create virtual environment kafka project used conda create virtual environment use convenient tool creating virtual environment activate virtual environment named av_blog_kafka confirm installation docker docker compose create docker compose yaml file project directory content file follows using image wurstmeister zookeeper wurstmeister kafka run kafka service need ensure port available local machine may look alternate port spinning two container zookeeper kafka container environment variable passed kafka kafka zookeeper running run following command terminal project folder get kafka running local machine tag run kafka service background terminal available exploration run following command check container running also check running container docker desktop dashboard use kafka python write simple producer consumer application use following command use pip conda install kafka python virtual environment verify installation following command command display version kafka python installed pip command head python editor choice used visual studio code write python code producer consumer start importing kafka producer kafka initialising new kafka producer argument bootstrap_servers localhost set host port use simple python loop create series message value message includes loop sequence timestamp string format loop send message topic named kafka topic using send method producer loop take second break end one iteration similarly write python script consumer py need import kafkaconsumer kafka initialize kafkaconsumer pas argument bootstrap_servers producer auto_offet_reset parameter indicates consumer start consuming message case breakdown interruption parameter set earliest consumer reading latest committed offset parameter set latest consumer consuming end log used another loop print message received consumer print topic partition offset key value message terminal got kafka set going producer consumer script ready execute script see event streaming running terminal open two terminal side side hint mac user used tool called iterm vertically split terminal one tile run producer py another tile run consumer py see producer publishing message message consumed consumer interrupt consumer py use ctrl c short restart use command python consumer py watch offset consumer start reading resumption make aspect auto_offset_reset parameter consumer instance clear producer consumer code used quite basic would recommend reader look parameter producer consumer instance documentation current scenario large amount real time data need processed real time generated sensor financial market etc kafka popular event streaming platform article introduces basic concept kafka key learning understanding basic kafka follows reference http kafka apache org intro http www docker com product docker desktop medium shown article owned analytics vidhya used author discretion
38,https://www.analyticsvidhya.com/blog/2022/06/customer-churn-prediction-using-mlib/,article published part data science blogathon customer churn prediction one enlightened problem statement nowadays possibly everything done make profit business profit come customer company hold product service goal organization hold permanent customer analyze potential one may choose alternative condition known customer churn blog build potential model predict customer churn help pyspark mlib library let take article let suppose working marketing agency hired u draw prediction potential customer might stop buying marketing service e customer churn working real world project let understand flow firstly one important thing mention new_customer independent data eventually used testing data model development phase also need create classification algorithm would help classify based feature fed model whether customer churn data marketing agency altogether feature target variable want know dataset go link starting first phase required library imported setting spark environment starting spark session always mandatory step get started pyspark output inference first step spark session module imported pyspark sql library building creating sparksession builder getorcreate method used respectively note looking gui version session see app name version spark location session created section reading dataset includes feature required predict customer likely churned think alternative inference line code read csv formatted data using read csv function put inferschema header parameter true see real essence dataset output inference printing schema data one best practice know type column like kind data hold output shown onboard_date string type following code feature required convert proper date format needed let statistical analysis dataset describe method alone provide lot insight statistic dataset output inference first inference draw null value dataset count feature hence got rid dealing missing value looking mean standard deviation name column conclude string type contribute anything statistical analysis output inference column object used get name column current instance dataset variable hold output one see well aware feature selection one important step data preprocessing select feature based knowledge would best fit model development phase hence valid numerical column taken account inference working mlib know format data mlib library accepts hence use vectorassembler module club selected feature together one column treated feature column summation feature thing see parameter section assembler object inference transforming data much necessary work commit statement e transaction change processed seen real dataset see hence used transform method output inference looking output thing get clear aiming first column feature selected column label column e churn following beginning article might question already separate testing data splitting dataset right answer keep phase splitting validation model perform routine would dealing new data already split different csv file inference help tuple unpacking stored data train_churn test_churn using pyspark random split method reaching phase article proof already cleaned data completely ready fed classification algorithm model specifically logistic regression note model building deal new customer data code breakdown would complete explanation step required model building phase using mliboutput inference summary object mlib library returned lot insight trained logistic regression model statistical information available conclude model performed well mean standard deviation churn actual value prediction predicted value close stage customer churn prediction analyze model trained dataset evaluating decide whether go model twitch required inference one notice first step imported binaryclassificationevaluator quite logical well dealing label column binary value evaluate method come existence take testing data total dataset parameter return multiple field evaluate model manually output inference output one see column returned evaluation method finally come last stage article till already built evaluated model prediction made completely new data e new customer dataset see well model performed note stage step dataset different according situation inference yes yes nothing extra discus already gone step main thing notice performing training complete dataset final_data know already testing data csv file hence splitting dataset required output inference testing data different file becomes necessary read way case customer_churn dataset saw schema new dataset concludes exactly schema inference assembler object already created main feature selected assembler object used transform new testing data inference transformation feature using assembler object similarly also need transformation final model top new customer output inference come data aiming achieve could know company like cannon benson barron robertson sexton golden park robbins need assign account manager decrease churn customer important aspect article try give brief everything article like assign account manager customer decrease rate churn particular company discus step brief repo link article hope liked article data preprocessing using pyspark filter operation opinion question comment connect linkedin discussion medium shown article owned analytics vidhya used author discretion
39,https://www.analyticsvidhya.com/blog/2022/06/become-a-web-3-developer-in-2022/,article published part data science blogathon web blockchain system intends deliver data code decentralization open sourcing cutting edge technology transform life variety way help smart contract web making impact society user able acquire share transfer asset form business without need middleman thanks contract become web developer detailed important measure follow starting career web developer blog post continue reading find everything need know want work web developer must familiar following three concept web revolution next step society progress decentralized apps data code exchange feature included transformation undoubtedly alter way operate perceive world part web revolution must first understand offer u individual web blockchain based technology new form technology blockchain digital ledger securely record network transaction web created combining smart contract blockchain smart contract computer programme run without human intervention certain condition met particular action made connection contract two party web developer someone creates blockchain based technology name implies individual attempting bring decentralized world faint heart web developer usually strong background computer science mathematics one go becoming web developer must well versed blockchain technology order even contemplate evaluated beginning also need exceptional problem solving creative communication ability must able put together smart contract based solution necessitate understanding programming language c javascript list talent web developer posse order successful list ability prospective web developer web developer must familiar programming fundamental well blockchain technology solidity python javascript c go key language used web developer must also strong understanding data structure algorithm need outstanding analytical skill good web developer reading newest industry advance comprehending related topic like encryption blockchain technology fantastic method acquire skill might also want look project like data science dummy data science handbook offer beginner friendly insight modern data science approach three key step must take become web developer first step blockchain developer learning blockchain technology first step involves looking cryptocurrencies like bitcoin ethereum others also entail becoming familiar programming language used create virtual currency second step learning programming next step learn use programming language come web platform development several programming language learn solidity python two popular ready create smart contract learnt one language third step create smart contract finally deploying smart contract main network test live blockchain platform like ethereum bitcoin test net result able avoid frequent blunder protect flaw code could result money loss data integrity difficulty also learn create ethereum dapps truffle popular development framework make decentralized app creation simple assist avoiding frequent blunder creating project programming critical grasp data structure working important aspect project knowing help write better code writing solidity example need know smart contract work may learn studying experimenting documentation language http solidity readthedocs io debugging testing made much easier understand data structure learning cryptography one first stage toward becoming web developer cryptography one complicated area computer science virtually limitless application used government business individual ensure security protecting information allowing secure communication understanding blockchain transaction function necessitates understanding cryptography necessitates knowledge mathematics statistic computer science start learning cryptography want become web developer prepared come learning blockchain technology skill web developer need construct next generation software outlined article ability highly diverse anyone interested working profession learn time section conclusion article provides resource assist learning skill need become web developer key takeaway know construct web application rocket science believe anyone interested technology belief become web developer furthermore realized variety route might follow voyage online education rise choose variety free paid option result leave determine next however following rule followed get finish line least amount resistance medium shown article owned analytics vidhya used author discretion
40,https://www.analyticsvidhya.com/blog/2022/06/which-is-better-bitcoin-vs-ethereum/,article published part data science blogathon article see two main cryptocurrencies bitcoin ethereum let u learn difference coin best investment make particular coin better nowadays cryptocurrencies quite popular important learn nowadays data science blockchain interrelated data scientist using blockchain technology authentication purpose also tracking data trading cryptocurrency involves blockchain technology future cryptocurrency change world become part life risk fraud using cryptocurrency make transaction super safe give full control people money let get started basically use normal currency like rupee dollar pound cryptocurrency also similar normal currency virtual currency physical currency normal currency controlled central authority centralized whereas cryptocurrency decentralized central authority third party involved transaction using cryptocurrency particular transaction done majority people using cryptocurrency agreed digital currency act medium exchange like normal currency cryptocurrencies use high standard cryptography secure transaction cryptocurrency us blockchain technology know blockchain chain block block set record bitcoin one cryptocurrencies also leading cryptocurrency among us strong encryption technique send receive money secured manner bitcoin us sha algorithm make secure decentralized cryptocurrency mean government bank control working bitcoin major advantage bitcoin cryptocurrency payment done secured strong cryptography identity sender receiver kept anonymous transaction fee also low affordable source cnbcethereum also known ether short ether cryptocurrency created ethereum provides ether token user ether token used build deploy decentralized application ethereum used pay service transaction fee service include computational power required block added blockchain ethereum also used peer peer payment like bitcoin source coindeskbitcoin first cryptocurrency created bitcoin released bitcoin created unknown person name satoshi nakamoto technology came concept blockchain popular nowadays side ethereum released recently ethereum created vitalik buterin researcher programmer creation ethereum vitalik buterin used concept bitcoin blockchain improved provided many functionality creating ethereum platform distributed application smart contract smart contract work way certain set predefined rule satisfied particular output take place ether used create kind smart contract know bitcoin enables peer peer transaction bitcoin act like normal currency like use normal currency need buy anything supermarket similarly transaction done bitcoin used transaction peer side ethereum also provides peer peer transaction along ether also provides user create execute smart contract blockchain already discussed smart contract smart contract allow user exchange anything value like kind contract share money real estate many bitcoin transaction validated using something known proof work similarly ethereum transaction also validated using proof work blockchain transaction validated miner proof work involves miner around world trying solve complicated mathematical puzzle first one add block blockchain soon ethereum may switch something known proof stake exactly proof stake proof stake validator validate transaction block based many coin person owns number coin mining power source coincu newswhen come reward miner reward mining case bitcoin btc per block reward become half original every block come ethereum miner get ether every time hen block added blockchain bitcoin transaction transaction fee low also optional pay much want order get attention miner towards transaction case ethereum optional pay ether every time block added blockchain later transaction fee converted gas converted gas drive computation allows transaction added blockchain add block blockchain bitcoin take minute case ethereum take average second cryptocurrencies use strong hashing algorithm make secure bitcoin us sha hashing algorithm whereas ethereum us ethash algorithm market million bitcoins million ether coin bitcoin market capitalization billion usd whereas ethereum market capitalization billion usd even though ether coin bitcoins market value ether can not reach market value bitcoin average around bitcoin transaction every day ether transaction every day block size bitcoin kb whereas block size ether kb major question arises bitcoin ethereum cryptocurrency better far discussed two main us called peer peer transaction smart contract better cryptocurrency decided based user requirement user requirement peer peer transaction bitcoin best user requirement smart contract definitely ethereum best option bitcoin ethereum two leading cryptocurrencies market nowadays transaction secure algorithm strongly encrypted cryptographic algorithm overall article seen cryptocurrency cryptocurrencies bitcoin ethereum use blockchain technology compared two coin several aspect finally concluded understanding coin better hope found article useful connect linkedin medium shown article owned analytics vidhya used author discretion
41,https://www.analyticsvidhya.com/blog/2022/06/getting-started-with-aws-s3/,article published part data science blogathon
42,https://www.analyticsvidhya.com/blog/2022/06/most-frequently-asked-google-big-query-interview-questions/,article published part data science blogathon big query serverless enterprise data warehouse service fully managed google big query provides nearly real time analytics massive data big query data warehouse provides global availability data easily connected google service flexible data ingestion mechanism data warehouse solution developed using big query highly secured scalable source http cloud google com architecture bigquery data warehousewe discus important concept question related google big query article important big query interview question big query big query serverless enterprise data warehouse service fully managed google big query built query engine make easy run sql query big query help organization analyze large volume data find meaningful insight source http www businesscommunity com marketing google bigquery tutorial marketer advantage using google big query advantage google big query big query data warehouse highly scalable provides pay go costing model b compatible etl tool like informatica c provides high data security query cache big query query cache speed data retrieval big query query get executed first time big query query result get stored temporary cached result table known query cache time decorator big query time decorator allow u access past data big query example accidentally deleted table one hour ago use time decorator get deleted data convert one data type another explicitly big query convert one data type another explicitly using conversion function big query example cast expression string use syntax various way big query cloud data warehouse accessed access big query cloud data warehouse using one way jdbc driversb odbc driversc web uid bq command line toole python library way optimizing query computation big query way optimizing query computation big query prefer use native udfs instead javascript user defined functionsb retrieving latest record use aggregate analytic functionc perform optimization table joining pattern scenario window function used big query scenario window function used big query calculating moving averageb calculating cumulative totalc rank row based specified criterion data team building new real time data warehouse client client want use google big query performing streaming insert get unique id event timestamp whenever data get inserted row guaranteed data sent clause function use write query ensures duplicate included interactively querying data ensure duplicate included use row_number window function partition based unique id row equal analytics company handle data processing different client client use suite analytics tool client allowed direct query access via google big query want ensure client can not see data step perform inside big query ensure data security client ensure client could see data following step could taken client load data different dataset b restrict client dataset approved user access datasetc security use relevant identity access management iam role client user client provides company daily dump data flow google cloud storage csv file would build pipeline analyze data stored google cloud storage google big query data may contain row formatted incorrectly corrupted build pipeline scenario follow step import data google cloud storage big query running google cloud dataflow b push corrupted row another dead letter table analysis work analyst e commerce company use google big query correlate customer data average price common product sold including laptop mobile phone television etc every minute average price good updated step follow ensure average price data stay date easily combine data big query cheaply possible follow step ensure average price data stay date easily combine data big query cheaply possible create regional google cloud storage bucket store update average price datab use created cloud storage bucket federated data source big query source http cloud google com blog topic developer practitioner bigquery explained data ingestion team lead large analytics company currently organization using demand pricing model big query quota concurrent demand slot per project current pricing model developer within company sometimes get enough slot execute query change pricing model resolve issue resolve issue switch current pricing model flat rate pricing model apart also set hierarchical priority model project article seen important big query question got good understanding different big query terminology major takeaway article seen query caching benefit using big query learned various way performing query optimization big query got understanding retrieve accidentally deleted data using time decorator big query apart also saw scenario based question google big query medium shown article owned analytics vidhya used author discretion
43,https://www.analyticsvidhya.com/blog/2022/06/is-adult-income-dataset-imbalanced/,article published part data science blogathon many row sample data required size training dataset required build machine learning model predict fraudulent transaction credit card fraud detection dataset containing around row surprised know row better row convinced read concept imbalanced datasets technique applied correct imbalance machine learning algorithm predict without bias major research area data science article shall revisit fundamental concept pertaining imbalanced datasets establish termed imbalanced dataset really need apply imbalance technique article let u consider adult income dataset public dataset available uci machine learning repository widely quoted public dataset machine learning literature used introducing supervised machine learning algorithm binary classification dataset contains row attribute census data pertaining adult income prediction task determine whether person make k income year number row k minority class constituting data literature publicly available dataset term dataset imbalanced fact wherever read dataset start statement dataset example imbalanced dataset also one datasets considered imbalanced research paper introduced smote imbalance technique machine learning question really meant imbalanced dataset adult income dataset really imbalanced warrant application imbalance technique correct shall first delve basic concept imbalanced data characteristic assume dataset contains majority class minority class classification algorithm completely ignores data minority class noise considers majority class predicts correctly get default accuracy obviously algorithm failed ability classify minority class normally objective problem modelled something like happen try predict skewed dataset default accuracy basic model classification problem always high size majority class hence classification problem never go default accuracy measure efficiency model best metric accuracy balanced accuracy measure available sci kit learn library alternative measure like precision recall f area roc curve different interpretation implication problem hand evaluated choosing relevant metric real world problem almost never get dataset percentage minority class anywhere near majority class fact truly imbalanced dataset one requires application technique correct imbalance percentage minority class minimal somewhere around let u delve nature imbalanced datasets shall briefly look important characteristic problem imbalanced datasets imbalance nature problem hand nothing lack datathe first basic thing understand imbalance nature problem classification prediction always huge deviation percentage majority minority class nature real world classification problem involve minority class constituting low percentage need predicted correctly consider example fraud detection among thousand transaction could one two transaction would fraudulent thereby leading low negligible percentage row represent fraudulent transaction hence dataset expected contain low percentage fraudulent transaction e minority class information sufficiencyin example fraud anomaly detection term relationship various independent variable row majority class represent several way transaction type transaction normal nature given set feature various possible value theoretically determine various way normal transaction set would constitute row majority class contain unique information normal transaction row majority class would add size majority class without adding additional unique information used classify two class hence technique like sampling majority class would work well case duplicate superfluous information determine continue sampling may end losing information could important model similarly case minority class oversampling duplicating minority class row may add additional information model resort sampling sampling additional row added removed reflect attribute new way fraudulent transaction nature problem modeled suppose based domain knowledge determine least transaction historically fraudulent transaction dataset might data represent fraudulent sample minority class effort acquire additional real world data data becomes complete reflects real nature problem hence case also can not resort oversampling however suppose also determine existing fraudulent transaction among typical represent possible way fraudulent transaction oversampling order make classification algorithm work better method like smote create synthetic sample ignore actual real life pattern may succeed creating model mimic real world problem effectiveness classification algorithm decrease increase imbalance increase decrease sample sizethe nature machine learning algorithm effectiveness making correct prediction decrease increase imbalance however effectiveness algorithm increase decrease sample size decrease sample size reduces degree imbalance technique sampling likely perform better base model imbalanced dataset photo stephen dawson unsplashas written one previous article machine learning algorithm use traditional statistical method part algorithm work well low size training dataset bigger size training dataset increase effectiveness accuracy algorithm difference accuracy training dataset size v max case imbalanced datasets size training dataset decrease accuracy tends improve based argument made foregoing section two way categorize dataset imbalanced dataset perspective machine learning algorithm perspective dataset termed imbalanced significant deviation percentage majority minority class recognition imbalance dataset based perspective important default accuracy measure arrived misleading explained previous section machine learning algorithm designed predict correctly majority class hence biased minority class however important point made article imbalance perspective problem ability machine learning algorithm dataset problem machine learning algorithm implementation imbalance technique may improve accuracy may even lead erroneous result fact contrary dataset imbalanced perspective machine learning algorithm application imbalance technique might required get good prediction shall explain paradox illustration subsequent section perspective real world problem modeled perspective dataset termed imbalanced percentage minority class reflect real world problem perspective focus nature real world problem proportion majority v minority class dataset reflect real world problem example adult dataset percentage adult income k figure reflect real world problem people earn income k constitute adult population answer yes dataset imbalanced extent warrant application handling imbalanceif answer dataset imbalanced dataset imbalanced perspective first step strive get real world data data represents real world problem can not get additional data minority class may resort implementing technique mitigate imbalance like smote nearmiss tomek link etc adult income dataset available uci machine learning repository considered reflect real world skewness term percentage minority class viz earning k hence dataset imbalanced perspective machine learning algorithm huge number row representing majority class may provide superfluous redundant information discussed previous section show dataset used give sufficiently higher accuracy reducing size training dataset without use technique imbalance shall using random forest classifier model building result logistic regression decision tree classifier also k fold cross validation also used verify mean accuracy section shall demonstrate following point made previous section using adult income dataset effectiveness machine learning algorithm decrease imbalance increase effectiveness machine learning algorithm increase sample size decrease case imbalanced datasets dataset imbalanced perspective real world problem imbalance technique may required make good prediction size training dataset need size minority class make good prediction hence base model consisting entire dataset perform poorly compared reduced size training dataset without proportionate reduction size minority class effectiveness machine learning algorithm decrease degree imbalance sample sizethat machine learning algorithm biased towards majority class known fact also shown effectiveness increase decrease degree imbalance mean dataset imbalanced perspective machine learning algorithm e significant deviation two class full dataset always give model effective reduce size training dataset accuracy improve move nearer minority class though highest accuracy need necessarily occur scikit sampling method case basic model balanced_accuracy python code shall build basic model entire datasetwe get balanced_accuracy score test data without anything imbalance case size training dataset balanced accuracy suppose change size training dataset get accuracy much impact accuracy case model ignores redundant information training dataset size take dataset training dataset create model subset training dataset contains maximum possible number minority class similar sampling however shall test model remaining training dataset basically reducing dataset used fitting model row containing row minority class e row rowsthough reduced data used fitting model though exposing model test data set size getting balanced_accuracy far greater achieved full dataset show imbalance decrease along decrease sample size accuracy increase much reduce size sample training dataset accuracy increased word many record earning k income required make valid prediction discussed section reduce size dataset point information contained dataset sufficient enough make prediction case size minority class generally truly imbalanced dataset percentage minority class negligible example percentage fraudulent transaction publicly available european credit card dataset suppose get adult income dataset data row income k since concluded general percentage adult income k get dataset minority class dataset considered imbalanced even perspective real nature problem per categorization earlier case use imbalance technique like oversampling lead right result shown reduced dataset size minority class e row would greatly reduce accuracy importantly oversampling improve accuracy shall oversample dataset increasing minority row duplication output show balanced_accuracy drastically gone around even though used size training data fit model dataset used build model truly imbalanced dataset reflects lack real world data help algorithm arrive accurate prediction technique like oversampling sampling may lead higher accuracy shown case imbalance due lack real world data balance can not achieved acquiring data may resort technique synthetically create new data e g smote algorithm optimize existing data like nearmiss thus argument made article verified using random forest classifier algorithm adult income dataset reiterate adult income dataset correct proportion two class data imbalanced perspective real world nature problem hence better accuracy achieved simply reducing training dataset maximum possible minority class word imbalance technique required dataset imbalanced perspective machine learning algorithm statement also shown true case imbalanced datasets example case publicly available european credit card fraud detection dataset row data show similar illustration would require row count fraudulent transaction get best accuracy making rest row redundant article explains basic concept underlying classification imbalanced data difference percentage row majority minority class differentiated lack sufficient data v actual real nature problem hand imbalance classification datasets arises due nature problem also due lack sufficient data applying technique correct imbalance difference clearly distinguished could case additional accuracy prediction obtained adding feature case implementation technique handle imbalance could lead erroneous result hence necessary study problem thoroughly given domain context compare dataset information real world problem making decision regarding handling imbalance data effectiveness classification algorithm increase decrease sample size owing reduction imbalance hence imbalance due bias machine learning algorithm problem resolved reducing sample size thereby reducing degree imbalance long information content sufficient dataset key takeaway imbalanced datasets nature many real world problem hence imbalanced datasets need necessarily require implementation technique handle imbalance default accuracy measure used model imbalanced datasets dataset whether considered imbalanced reflect nature real world problem nature production data deployed imbalance also due lack sufficient data one class case dataset augmented obtaining real world data imbalanced income datasets thus categorized imbalanced two perspective viz perspective algorithm perspective whether dataset represents real world problem sampling sampling lead erroneous prediction dataset reflect real world problem term information sufficiency sampling sampling use method like smote done reduce information content augments thereby increasing capability machine learning model certain amount domain knowledge required make decision handling imbalanced data widely used adult income dataset uci though contains data one class can not termed imbalanced reflects nature real world problem degree imbalance increase effectiveness machine learning model decrease however sample size decrease imbalance decrease hence reducing sample size subject losing information content increase accuracy imbalanced dataset imbalance technique create synthetic data modify algorithm amplify information content minority class highly useful dataset imbalanced perspective whether dataset represents real world problem reference adult dataset http archive ic uci edu ml datasets adult herrera francisco et al learning imbalanced data set germany springer international publishing chawla nitesh v et al smote synthetic minority sampling technique journal artificial intelligence research http www analyticsvidhya com blog imbalanced data classification medium shown article owned analytics vidhya used author discretion
44,https://www.analyticsvidhya.com/blog/2022/06/differences-between-web-2-0-and-web-3-0/,article published part data science blogathon paying close attention blockchain industry undoubtedly familiar word web web good chance confused precise meaning phrase relate blockchain technology got internet pas lot important milestone way becoming today web web generation internet service differ significantly term way consumer engage internet result people eager learn distinction web web order understand need web following discussion assist learning depth distinguishing characteristic web web one comprehensive knowledge web web would perfect basis contemplating difference similarity web web context web internet undergone paradigm change usage web interactivity social connectedness user generated content totally supplanted web static website last year web unprecedented reach led explosion user generated material seen million people across globe practically second want get comprehensive understanding difference web web best way start web reference second generation internet service put emphasis providing user ability interact material hosted web alongside improvement interoperability usability end user web movement encouraged expansion user generated content come second generation web changing technical requirement priority hand place emphasis modifying layout website well method used web encouraged user collaborate one another communicate one another pp transaction paved way development e commerce social medium platform order develop web application web browser technology required technology include ajax javascript framework recent year framework emerged highly popular mean developing web application allows free information sorting allowing user obtain categorize data web allows access web content via tv mobile device multimedia console promotes user review online discussion information route second generation internet service emphasize dynamic content user response web participatory social web user may create share responsive content collaborate web encourages establishment new online community reference progression web utilization interaction includes transforming internet database use distributed ledger technology data used assist creation smart contract tailored requirement individual user web may provide impression developed approach internet nonetheless still lot drawback stand regard safety private information web authorized institution given power data individual user particularly result need trusted intermediary event two party want carry transaction know trust one another option available depend third party respected reliable control data storage administration however allows middleman increase hold user addition use centralized authority time crisis never successful another argument favor decentralization aim web organize information logical coherent manner third generation web make use advanced metadata system version web sometimes referred semantic web metadata system provides assistance structuring organizing different kind data order make accessible human computer web make use artificial intelligence order provide accurate result expedient manner also providing access real time insight user also given ability take advantage potential offered image graphic thanks web semantic web functionality another key component web suggests web might used help people grasp word imply consequently people machine able quickly access information web web also exemplifies significant quality increased privacy security quality may found web difference web web would also concentrate user data identity protection order protect user identity data web make use distributed ledger technology encryption provide enhanced authorization method ownershipan unparalleled level control digital asset conferred web take instance fact participating web game pay real money item inside game permanently associated user account developer game decide delete account able retrieve thing alternately value placed game thing forfeited quit playing game web enables direct ownership taken via use non fungible token nfts ownership game can not taken away anybody even developer game addition decide quit playing game may recuperate value virtual object accumulated selling trading exchanging open marketplace native paymentsbecause payment infrastructure dependent bank payment processor web can not accept payment people bank account happen located nation can not accept payment token like eth sol used web order fulfill mission enabling user transfer money directly within browser without requirement trusted third party transparencythe main goal web bring people together around data interested web combine data meaning increasing trust information thanks well known decentralization result community organically formed around web collapse web personalizes information expands possibility right web allows keep track data issue trust widespread web major business complete control personal information driving technologiesajax javascript together cs html one come often discussing web technology surge development artificial intelligence could affect web machine learning deep learning semantic web decentralized protocol cutting edge technology powering web type applicationspodcasts blog video hosting website example web application broad sense term self production content user communication may used refer almost kind information decentralized apps dapps driven ai ml web example include multi user virtual world portal integrated game soon put web misery despite fact web represents significant advance technology still several shortcoming two approach working together perfect harmony even second iteration web still upper hand web getting closer time offer potentially fruitful infrastructure essential interaction human machine first foremost web enhanced protection user privacy trust security term web also often used refer decentralized web built almost entirely top decentralized protocol hand majority online apps use still employ web primary base medium shown article owned analytics vidhya used author discretion
45,https://www.analyticsvidhya.com/blog/2022/06/recommender-systems-from-scratch/,article published part data science blogathon exponential surge availability digital content increasing number internet user potential challenge information overload created whereby timely suitable access information item interest hindered internet solve increasing plethora information retrieval digital platform attempted partially solve problem developing intelligent system would map available content user choice preference intelligent system recommender system topic discussion rest post image source http surl li cfhocrecommender system defined tool help user search history knowledge related user preference choice thus serve mean assist augment social process using recommendation others make choice firsthand knowledge experience possible alternative available system thus serve information filtering route help tackle problem information overload filtering vital information vast network information available interest preference digital user inherent capability predicting whether particular product service would preferable user based user profile recommendation system beneficial user service provider service provider perspective help increase conversion rate allowing customer find desired product also help company increase sale suggesting customer product wishlist help improve customer loyalty help create value added relationship customer company today world recommendation system found usability many area video streaming platform like youtube netflix amazon use provide video recommendation user google us system deliver relevant ad user serve primary source revenue music streaming platform like spotify leverage provide music recommendation based taste suggests new weekly playlist user one popular feature e commerce giant like flipkart amazon use targeted marketing tool provide customized product recommendation user would like buy based similar product purchased earlier social networking site like facebook instagram provide friend connection recommendation using thus application recommender system many still core us either two approach content based collaborative filtering detail discussed following section schematic representation different classification recommendation system algorithm behind content based approach domain dependent make prediction emphasizes analysis attribute item recommendation approach based profile creates user feature extracted content item user used past item mostly related positively rated item user recommended content based filtering cbf mostly use vector space model like term frequency inverse document frequency probabilistic model like naive bayes classifier decision tree neural network learn underlying similarity different content entire corpus information cbf profile user redundant since influence recommendation cbf easily handle preference user profile within short time adjust recommendation accordingly recommend new item even database contain user preference thus need sharing user profile obviated ensures privacy however cbf suffers various drawback firstly approach requires rich metadata detailed description item user profile make suitable recommendation limited effectiveness user often restricted getting recommendation similar item profile limit diversity choice collaborative filtering cf free knowledge domain beneficial content can not quickly entirely described metadata like movie music main advantage cf cbf domain independence address data aspect often masked challenging profile using cbf cf work creates matrix user item data preference item user match user relevant interest preference calculating similarity profile make recommendation user build collection called neighborhood user get recommendation positively rated positively rated neighborhood two primary area cf model based technique memory based technique schematic representation cf technique based user item matrixin technique algorithm try find user shared appreciation item user rated positively past neighbor defined different algorithm used combine preference neighbor generate recommendation approach achieved two way user based item based technique quality recommendation algorithm evaluated using different measurement covering accuracy coverage matric used depends algorithm used filtering technique mean absolute error mae root mean square error rmse common accuracy metric accuracy matrix often include receiver operating characteristic roc precision recall f score user discretion necessary selecting suitable error matrice algorithm used article vivid idea theory recommended system provided starting concept recommender system idea using system contemporary world discussed thereby portraying usefulness necessity system recommender system discussed theoretical perspective algorithm utilized building pro con approach finally evaluation matrix used validate model discussed briefly hope reading article point must ignited enthusiasm try develop one recommendation system try build movie recommendation system available dataset movielens dataset netflix prize datasetsong recommendation system available dataset million song dataset spotify music datasetgo quick try hand recommender system datasets medium shown article owned analytics vidhya used author discretion
46,https://www.analyticsvidhya.com/blog/2022/06/difference-between-web-2-0-and-web-3-0/,article published part data science blogathon third generation internet firmly taken hold modern retelling web history web usher new era decentralized blockchain based architecture transitioned decentralized protocol web centralized monopolistic platform web web prevailing narrative grasp power dissipate small number powerful web firm return power people contrast today tech giant dominate platform web user ownership share platform application prerequisite developing web architecturehttps tinyurl com spkjfzain early day web internet content consisted static web page user would visit read interact describes initial iteration subsequently developed platform significant multi functional application information moved website user read web social medium algorithm advertisement exist factor take account describing web internet program let user communicate cooperate express online referred phrase essentially better version first global web marked shift static dynamic user generated content well rise social medium rich web application web oriented architecture social web part web paradigm refers change way web page designed utilized people without technical change web business revolution computer industry caused move internet platform attempt understand rule success new platform tim reilly web described three part key feature web use case application web pro using web con using web web third generation internet based service intelligent web expression created john markoff although typically understood reference semantic web continued easy consensus web mean semantic web accurate term refers technology improves internet use comprehending meaning user rather merely way page link significant developing technology trend like semantic web data mining machine learning natural language processing artificial intelligence technology centered information machine assisted web expected connected smarter web whole better designed cater user interest need self description similar strategy used developer author individually partnership ensure information produced new context aware application meaningful user even though web yet standardized characteristic define http tinyurl com acmbsc web still process defined result many unknown regarding web eventually look like ipv address class limited amount web address used web web web era ipv offer bigger address space allowing device public ip address web focus decentralization automation intelligence likely continue foundation come next evolves defined article explores various segment transition web web real world including evolution feature difference example advantage limitation last thought want leave everyone another example started learning journey according analogy movie industry web represented black white movie web would age color basic web represents immersive experience metaverse way web dominated global business cultural landscape seems web take lead facebook changing name meta oct could prove early sign web gaining traction comparison web web show present better advantage web web present promising infrastructure interaction human machine significant aspect web improves security trust privacy many people refer web decentralized web rely heavily decentralized protocol web hand still foundation many web apps use today possible web transform popular program use today learn web make conclusion medium shown article owned analytics vidhya used author discretion
47,https://www.analyticsvidhya.com/blog/2022/06/what-is-web-3-0-journey-from-web-1-0-to-web-3-0/,article published part data science blogathon web sometimes known web web word probably heard lot recently simply refers internet next version encourages decentralised protocol try lessen reliance giant digital corporation like youtube netflix google amazon web supporter claim technology transform internet ushering new decentralised era internet managed ordinary people rather major business third generation web technology known web web web often known world wide web basic layer provides website application service internet web continuously expanding defined single globally acknowledged definition one thing certain web place heavy focus decentralised apps make considerable use blockchain based technology machine learning artificial intelligence ai also used web assist enable intelligent adaptable apps concept semantic web another component developing definition web tim berners lee founder web one many campaigned incorporation semantic technology web understand web need look history internet berners lee computer scientist cern europe pioneer early creation internet effort led creation internet web http hypertext transfer protocol invented tim berners lee allow text document exchanged across network using browser software safari chrome time known read web individual could read information website broad level earliest stage world wide web evolution referred web web content provider vast majority user content consumer personal website prevalent mostly consisted static page housed isp owned web server free web hosting service image http www royex ae blog evolution web web web mattersadvertisements website accessing internet prohibited web ofoto also online digital photography website web user could save share view print digital image web content delivery network cdn allows presentation data website suitable usage personal website charge user based number page seen feature directory allow user search specific information web period lasted roughly main feature web although tim reilly dale dougherty organised first web conference later known web summit darcy dinucci invented phrase web refers website emphasise user generated content usability interoperability end user world introduction social medium quick growth factor ushered web era ability web server interpret server side script user generated material form comment use database store information contributed shift web exponential expansion fueled important breakthrough mobile internet social network near ubiquity powerful mobile device iphones android powered smartphones image http placewit medium com must problem crack faang afdfthe participatory social web another name web relate change technical definition rather change way web page built used transition advantageous although appear case change occur web allows user interact collaborate one another social medium discourse creator user generated content virtual community web development web web development employ web browser technology ajax javascript framework ajax javascript framework become highly popular developing web site multiple web breakthrough paved way app domination second decade millennium greatly increased online participation consumption web hand big influence individual enterprise others pose existential danger following example web user communicate share view opinion experience web issue much content framework web centralised architecture open door security issue data harvesting malicious purpose privacy invasion expense furthermore network took data storage causing access challenge well concern anonymity security online data key breakthrough like mobile internet access social network well near ubiquity sophisticated mobile device like iphones android powered smartphones fueled web exponential expansion advance permitted supremacy application substantially extended online engagement usefulness second decade millennium example airbnb facebook instagram tiktok twitter uber whatsapp youtube mention image http www xrtoday com virtual reality web v web many web centric firm including apple amazon google meta previously facebook netflix become among world largest company market value result remarkable revenue growth result contact internet access critical information regarding online activity form data company utilise data create new platform provide targeted advertisement also make money selling others according theorist result situation consumer little control data used web differs web focus use technology machine learning ai give appropriate material user rather content end user contributed web enables people contribute participate site content web likely delegate task semantic web ai technology web also place heavy emphasis decentralised service authority contrast web centralization berners lee discussed notion semantic web paper published reliable technique computer process semantics language e figure actual context word phrase used berners lee ambition semantic web structure meaningful material web page allow software complex job people image http www dshgsonic com blog web start new eraweb progressed well beyond berners initial lee notion semantic web first proposed partially due high cost difficulty converting human language various subtlety variation format computer understand partly due fact web already changed significantly last two decade look three generation internet reading accessing information primary goal web reading writing data well producing content web reading publishing owning data important aspect web medium shown article owned analytics vidhya used author discretion
48,https://www.analyticsvidhya.com/blog/2022/06/understanding-loss-function-in-deep-learning/,article published part data science blogathon loss function important machine learning deep learning let say working problem trained machine learning model dataset ready put front client sure model give optimum result metric technique help quickly evaluate model dataset yes loss function come play machine learning deep learning article explore different type loss function without wasting time let start article simple term loss function method evaluating well algorithm modeling dataset mathematical function parameter machine learning algorithm simple linear regression prediction calculated using slope intercept b loss function yi yihat e loss function function slope intercept value loss function lower good model otherwise change parameter model minimize loss people confuse loss function cost function let understand loss function cost function cost function loss function synonymous used interchangeably different loss function loss function error function single training example input cost function cost function hand average loss entire training dataset regression classification autoencoder gan object detection word embeddingsin article understand regression loss classification loss mean squared error squared loss l loss mean squared error mse simplest common loss function calculate mse take difference actual value model prediction square average across whole dataset advantagedisadvantagenote regression last neuron use linear activation function mean absolute error l lossthe mean absolute error mae also simplest loss function calculate mae take difference actual value model prediction average across whole dataset advantagedisadvantagenote regression last neuron use linear activation function huber lossin statistic huber loss loss function used robust regression le sensitive outlier data squared error loss advantagedisadvantage binary cross entropy log lossit used binary classification problem like two class example person covid article get popular binary cross entropy compare predicted probability actual class output either calculates score penalizes probability based distance expected value mean close far actual value advantage disadvantage note classification last neuron use sigmoid activation function categorical cross entropycategorical cross entropy used multiclass classification categorical cross entropy also used softmax regression loss function sum k yjlagyjhat k classescost function n sum upto n sum j k yijloghijhat wherenote multi class classification last neuron use softmax activation function problem statement classessoftmax activation f z ez ez ez ez use categorical cross entropy sparse categorical cross entropy target column one hot encode class like use categorical cross entropy target column numerical encoding class like n use sparse categorical cross entropy faster sparse categorical cross entropy faster categorical cross entropy article learned different type loss function key takeaway article loss function deep learning hope liked article medium shown article owned analytics vidhya used author discretion
49,https://www.analyticsvidhya.com/blog/2022/06/distillation-of-richard-ngos-artificial-general-intelligence-safety/,article published part data science blogathon agi safety first principle sequence richard ngo attempted summarise compelling argument existential threat artificial general intelligence agi might pose near future claim present fresh point view agi safety six series report base argument richard ngo piece second specie argument problem also referred gorilla problem book human compatible ai problem control stuart russel analogous fact human evolved gorilla took gorilla lost control focus richard ngo piece defending following argument artificial intelligence ai capable lot human autonomous ai agent goal oriented sense might align humanity building ai agent might counterproductive might try rule human distillation particularly focus part alignment series report focus artificial general intelligence safety worth taking time understand term general mean context let u define intelligence intelligence understood ability well variety cognitive task defined legg reiterated richard report artificial narrow intelligence ani artificial general intelligence agi artificial super intelligence asi considered three stage progression artificial intelligence technology artificial narrow intelligence ani stage ai currently ai system programmed perform specific task ani also referred weak ai second stage e artificial general intelligence also referred strong ai ai system think perform task like human unlike ani system agi system think reason make judgement creative like human potential moving towards stage agi identified example natural language processing widely us generative pre trained transformer gpt model developed openai gpt created solely prediction next word sentence progression gpt advanced capable writing essay answering question writing code similar task mimic human intelligence another way richard see potential development agi evolution human trained specie well way transform childhood adulthood throughout learn plethora situation experience final stage e artificial super intelligence asi believed stage ai system perform beyond cognitive skill human researcher fear devastation human race also summed meme also mentioned second specie problem gorilla problem earlier richard considers ani task based ai agi generalisation based ai segment range rather two separate idea give example alphazero trained reinforcement learning played several game competed human according richard either considered two separate task generalisation took place self play competing human case single task belief categorization subjective training enormous amount data ai agent may able excel task self driving car task based approach sooner artificial general intelligence might able solve however much complicated analytical scenario like management company might something large amount data gathered training scope task involved case vast moving discussion progression artificial general intelligence artificial super intelligence richard commented viability highlighting speed passing signal transistor enormously greater neuron identified three contributing factor greater artificial intelligence replication cultural learning recursive improvement context replication richard reiterated term collective ai used bostrom according concept artificial general intelligence built single superintelligent entity collection simple decomposed task relatively easier train second factor contributing development super intelligence cultural learning e agis capable learning human tend finally recursive improvement referred concept ai able learn improve previous generation factor similar concept recursive self improvement mentioned yudkowsky research paper intelligence explosion microeconomics artificial super intelligence highest level artificial intelligence cost sole concern second specie problem power artificial intelligence system might end using acquired power way might align goal humanity brings u question ai system even acquire power richard mention three possibility regard ai system pursuit power order fulfill objective ultimate objective ai system obtain power human train way gain much power richard sequence cover first two possibility central thought first possibility nick bostrom instrumental convergence thesis main point thesis try elucidate ai agent instrumental goal whose achievement contribute final goal thus ai system pursue power instrumental goal converging final greater goal example instrumental goal survival self preservation intelligence enhancement technological gradation even acquisition resource question whether goal aligned human value realised ai system might pursue power instrumental reason main aspect concern whether ai system pursue goal align human value particularly moral value simply put christiano say ai system aligned human trying human want alignment goal human value understood two kind interpretation minimalist maximalist according minimalist approach also referred narrow approach aligned ai avoids outcome safe learns preference human try align action around example ai system learns human like weekend free clearing schedule human process even clear task restocking grocery family time etc point ai learnt human preference wrong well intends provide human free time able differentiate actually work human would really mind free time key aspect ai focus human intent preference really try deviate hand maximalist ambitious approach requires greater level understanding decision human take based concrete logic may involve personal preference intuition ethic moral etc ai system learn long period time par level human understanding maximalist approach apparently hard define range political ethical technical problem associated richard piece focus mainly minimalist approach refers intent alignment sake piece considers ai system misaligned human ai system trying human would want considering ai system aware human intention referred misalignment intention always considered something ai trying learned artificial general intelligence agi system work behave like human really sure actually perform safety concern although well trained human level data clear understanding human intention expect agi system agi care intention found instrumental goal would like achieve align human value one might pose question train ai task cause conflict human value bostrom orthogonality thesis state can not assumed ai motif value similar human instance ultimate goal super intelligent system might calculate number digit decimal pi artificial mind really put finger exactly agent might definitely try predict bostrom mention three direction regard first intelligent agent engineered way pursues goal told second behaviour predicted engineered inheritance inherit value motivation heir however noted process subject corruption might turn counterproductive third way predictability instrumental convergence reason discussed earlier two kind problem deal ai alignment outer misalignment inner misalignment outer misalignment train ai system accidentally turn dangerous may happen can not perfectly replicate exact specification ai system complex specification complex ai might able understand intention correctly reward mechanism objective function defined may even reward misaligned behaviour well objective function function need optimise maximise minimize given problem scenario reward mechanism incentive based mechanism teach ai correct providing reward wrong punishing let u understand term real life example parent teach child good habit objective function reward mechanism e appreciating child something nice punishing something wrong intuitive way handle problem provide human feedback specification ai system evaluated solution straightforward ai system might trained long term goal providing feedback step might possible seen outcome finally turn example ai system playing game might difficult understand every move ai finally win example relates instance alphago winning game go incredibly surprising move minute everyone thought mistake left even lee sedol one world best go player speechless alphago ai system trained london based ai lab deepmind another aspect ai computationally lot faster human might able match speed problem informed oversight also worth discussing know ai like black box really know exactly ai performing certain task case go wrong might able trace exactly went wrong might help case emerging concept explainable artificial intelligence xai tackle outer misalignment problem focus defining objective function safe even tackle outer misalignment problem still deal problem inner misalignment inner misalignment refers problem ai system learns intend learn ai system purpose defined optimized achieve purpose objective ai pursues different goal trained pursue referred problem inner misalignment toy example shared evan hubinger podcast ai trained solve maze reaching centre training process centre tile maze green arrow incorrectly interpreted ai problem locate green arrow rather finding centre maze problem inner alignment ai learning got deviated actual objective prove dangerous problem also identified analogous human evolution main objective increase genetic fitness along way human started achieving sub goal love status etc richard view alignment equal inner alignment outer alignment take misalignment problem inner misalignment problem trickier problem solve outer misalignment one approach mention tackling add misalignment sample training process straightforward ai black box subgoals ai trying achieve unknown u unless explainability technique used researcher fear large scale misaligned ai goal put training process complex simulate richard discussed possible human train ai system surpass intelligence turn would lead ai system taking human given various argument throughout piece belief possibly future ai focus distillation ai goal alignment learned ai system multiple reason goal align human also misalignment intentional unintentional also learned second specie problem probable outcome emergence artificial general intelligence fast complex future ai system going thus future research focus devising way train agi asi system way pose threat human align human value medium shown article owned analytics vidhya used author discretion
50,https://www.analyticsvidhya.com/blog/2022/06/qr-codes-creation-using-python/,article published part data science blogathon qr code wonderful thing every product buy matter large small expensive cheap powered qr code per definition qr code machine readable optical label contains information item attached qr code contain data either locator identifier tracker point website application best part qr code hold character text told create qr code using python using line code let get started following pre requisite needed project python ide choice anaconda distribution optional already python installed system need install anaconda distribution would suggest install anaconda distribution many advantage installing anaconda distribution one access jupyter notebook widely used notebook based ide data science project another advantage install anaconda distribution also install python part installation sequence hence working want work data science would suggest anaconda distribution project going use jupyter notebook however fixed criterion use jupyter notebook ide choice used following library going need pyqrcode pypngspeaking first module main job pyqrcode name implied creation qr code second module pypng used representing qr code png image importing pyqrcode pypng module embedding data variable converting data stored variable qr code creating image file qr code generating qr code image console optional name implied step going import pyqrode pypng module needed project happen module already installed following command install pyqrcodepypngnow let get familiarity module going use project pyqrcode library basically qr code generator written pure python module automates process creating qr code hand pypng pythonic implementation png module main responsibility creation png image file step going embed data variable put data choice usually qr code store web url number character text case going embed url variable see code created variable named link variable encoded url google comin step going convert data embedded variable qr code see code invoked create method pyqrcode module module accepts one argument basically variable embedded data result entire computation stored variable named qr_code final step project prior step converted data qr code time display qr code yay python code remember pypng module imported well method named png using going generate qr code image method accepts two argument first name qr code file followed extension second argument scale determines size resulting qr code image scale configurable argument using change size qr code image higher value scale larger resulting qr code image done successfully generated qr code data however question qr code one important thing remember output qr code file going generated location code getting executed make sure easily find qr code file set working location prior executing code using code changed working location datascience folder c drive advantage setting working location since already set location qr code image going generated location eas task finding file qr code output fileas see output qr code generated code scan qr code qr code scanner see redirects want display qr code console add additional line code code terminal method used generate qr code console method accepts two argument input first module_color decides color pattern inside qr code second background decides background color qr code article seen leverage python computational prowess generation qr code also seen embed data qr code following takeaway article qr code generation automated using python pyqrcode python module used generating qr code pypng module responsible creating png file qr code using terminal method qr code generated jupyter notebook itselfhope like article shrish mohadarkar linkedinthe medium shown article owned analytics vidhya used author discretion
51,https://www.analyticsvidhya.com/blog/2022/06/speed-up-pandas-in-python-with-modin/,article published part data science blogathon apache spark us cluster distribute panda operation speed computation modin come rescue library created data scientist uc berkeley us parallel computation process dataframe execution faster data scientist executes dataframe operation backend modin us ray dask distributes operation parallelly boost operation let look code install modin using pip command download sample dataset use new york yellow trip data download parquet file approx mb file link file link import relevant package initialize ray modin give path parquet file downloaded step run function panda modin performed several operation let summarize see faster modin example see except merge function modin beat panda remaining function seems limitation modin let look next currently modin cover api method per link currently modin support merge function case us approach first convert panda dataframe operate performance penalty going partitioned modin dataframe panda communication cost single threaded nature panda panda operation completed convert dataframe back partitioned modin dataframe way operation performed something default panda optimized modin operation supported system modin provides substantial speedup modin take advantage multiple core relative koala daskdf efficiently execute operation thanks optimized design notably koalasares often slower panda due overhead spark humble attempt explore way make panda execution faster medium shown article owned analytics vidhya used author discretion
52,https://www.analyticsvidhya.com/blog/2022/06/building-a-blockchain-in-python/,article published part data science blogathon web latest buzzword world technology web revolves around concept decentralized web primarily built using blockchain blockchain around came limelight bitcoin many get confused term bitcoin blockchain consider bitcoin best implementation blockchain technology blockchain type decentralized database immutable persistent tamper proof contains data block encrypted using hashing algorithm advantage blockchain technology type data stored mostly used store transaction detail act digital ledger idea decentralized architecture web blockchain make sure data owned single person entity downside web block blockchain unique contains hash value used differentiating block fingerprinting concept used linking block blockchain new block keep getting added end blockchain hash penultimate block used building hash new block make block blockchain tamper proof article building simple blockchain python store text information user blockchain technology used industry far complex blockchain building article enough understand blockchain technology implementation perspective block blockchain following property first block blockchain called genesis block explanation derived preceding hash can not extracted blockchain blockchain empty case preceding hash generated using secret specified creator blockchain ensures block blockchain similar structural schema blockchain difficulty level associated specifies number digit need hash satisfy condition nonce property whole number help generating hash specified number preceding zero since hashing algorithm used blockchain technology sha almost impossible find nonce pre calculating hash value trial error way calculate nonce make computationally expensive time consuming need run loop calculate nonce value process guessing nonce generates hash per requirement called blockchain mining computationally expensive time consuming necessary add block blockchain set difficulty level blockchain first letter hash mining bitcoin difficulty level set mining block bitcoin roughly take minute basic understanding blockchain building article let get hand dirty start building nothing fancy module used building blockchain native python module directly imported without install using pip using hashlib module performing sha hashing time module useful fetch block generation time define class called blockchain two property namely block secret block property store block blockchain secret variable used building previous hash genesis block define three function namely create_block validate_blockchain show_blockchain create_block function used create new block append block property blockchain property block explained earlier implemented nonce satisfies blockchain requirement four zero preceding hash calculated validate_blockchain function used validate integrity blockchain mean check fingerprinting block blockchain tell u blockchain stable block contain correct hash previous block discrepancy safe assume someone meddled block blockchain property make blockchains immutable tamper proof finally show_blockchain function used display block blockchain built blockchain class let use create blockchain add block add block blockchain validate blockchain finally print block look output python code see block present blockchain validate_blockchain function return true let meddle blockchain add new block somewhere block blockchain run validate_blockchain function see return output get see validate_blockchain function return false mismatch fingerprinting hence integrity blockchain compromised article discussed following continue project blockchain hosted deployed rest api server cloud used user store information blockchain obviously blockchain distributed sake simplicity really interested using blockchain technology database feel free look bigchaindb decentralized blockchain database provides support python nodejs alternatively gundb popular graph based decentralized database engine used web application recent time article building blockchain using python hope enjoyed reading article learned something new thanks reading happy learning medium shown article owned analytics vidhya used author discretion
53,https://www.analyticsvidhya.com/blog/2022/06/the-most-comprehensive-guide-on-explainable-ai-xai/,article published part data science blogathon love artificial intelligence like delve lot aspect follow every day see new field made latest update missed title technology today one newest rarest technology worked recently artificial intelligence end era working development new technology called interpretable artificial intelligence work clarify new concept communicate information better well ordinary person meaning work make result flexible work drawing new easy understand easy flex plan make average person work understand result large accurate way explanatory artificial intelligence xai created programmed describe purpose explanation give accuracy rationale decision making process way understood average human xai often discussed relation deep learning play important role fat ml model fairness accountability transparency machine learning xai provides lot information ai program make certain decision something taken followed way detect namely strength weakness program used first must understand xai technology needed hence ai algorithm often act black box come provide output anyway understand inner working goal xai make rationale behind producing algorithm understandable ordinary person familiar subject making fully aware subject hence assume example many ai algorithm used deep learning matter algorithm learn identify pattern based data bloating data large training data whereas deep learning neural network approach simulates way brain normal human being operates like human thought process difficult impossible determine extent deep learning algorithm reached prediction decision first define understand interpretable ai explain principle help determine expected output xai provide guidance reach desired outcome may easy well divide xai three category question asked clarify question range follows feature interface include xai interface depict output different data point explain relationship specific feature model prediction user observe x value different data point understand effect absolute error received color code understand make model easier clearer idea present normal people understand exactly interact feature work figure xai interface visualize output different data point explain relationship specific feature model prediction saw artificial intelligence widely used daily life went important point ethic artificial intelligence however increasing complexity advanced ai model lack easiness raise doubt model without understanding human can not decide whether ai model socially useful trustworthy safe fair thus ai model need follow specific ethical guideline gartner combine ethic artificial intelligence five main component import library python code article figured thing related topic xai tool recently become interest many researcher data scientist analyst process beginning come benefit technology article worked several thing namely definition technology mentioned new field got acquainted history emergence technology harm affected development also defined use technology advantage disadvantage end applied code fetched data mentioned worked tool implemented xai mentioned code project see attached link see technique work hope enjoy article several main point clarified absolute concept behind technology explained article second point understanding technology work different nucleus feature consists serf technology end made implementation code technology work medium shown article owned analytics vidhya used author discretion
54,https://www.analyticsvidhya.com/blog/2022/06/commonly-asked-ques-during-microsoft-azure-interviews/,article published part data science blogathon microsoft azure public cloud computing platform azure provides different category cloud service infrastructure service iaa platform service paas software service saas serverless organization across globe prefer use microsoft azure developing solution provides dynamic scalability disaster recovery security therefore beneficial developer good knowledge azure article discus important question azure help get well prepared interview important interview question azure differentiate public private cloud azure resource azure resource group every entity created managed azure known azure resource example azure resource include azure storage account azure sql azure virtual machine etc azure resource group hold related azure resource azure solution using azure resource group easily manage life cycle azure service present inside resource group together different protocol supported azure application gateway possible restore application gateway public ip deleted different protocol supported azure application gateway http websocket http http possible restore application gateway public ip deleted deleted accidentally scenario required create new application gateway difference service bus queue storage queue consider scenario work azure administrator organization asked team lead set default password new user added active directory possible yes possible set default password first time user azure active directory difference azure table storage azure sql service maximum number trigger single azure function one trigger present maximum inside azure function data profiling azure data profiling feature azure data catalog service examines datafrom supported data source catalog collect useful information related statistic data including profile data asset choose include data profile data source registration tool whenever register data asset issue high load application resolved man support floor using azure resolve issue use vm scale set proper condition configuration provision new vm whenever load application increase developer create manage group vms load balanced using azure vm scale set configure scale set manner count vms automatically increased decreased based pre defined schedule application demand azure vm scale set allow developer configure update large vms centrally also ensure high availability application easy support development large scale application supporting compute load big data etc using azure vm scale set delete blob snapshot azure deleting blob snapshot first delete snapshot want delete blob snapshot time use delete blob operation secure database connection string using azure key vault create secret azure key vault give name secret provide database connection string secret value thus way secure database connection string using secret azure key vault workstation created container image named containerx created azure web app container named demo use container image containerx need upload containerx azure solution provided must ensure demo use container image containerx azure service upload created container image containerx container image containerx uploaded azure container registry configure registry credential inside web app app service web app deployed able pull image azure container registry azure portal web app container named demo go container setting update image source registry save article discussed common question asked azure interview however recommended read microsoft official documentation gaining knowledge question azure service discussed major takeaway microsoft azure article learned secure database connection string using azure key vault seen data profiling azure resource azure resource group got understanding azure function one trigger maximum seen azure storage queue azure service bus queue different apart also saw scenario beneficial use azure vm scale set azure application gateway medium shown article owned analytics vidhya used author discretion
55,https://www.analyticsvidhya.com/blog/2022/06/single-table-analysis-with-mysql/,article published part data science blogathon article see use sql statement data analysis data analysis done single table multiple table particular article mainly focus single table analysis nowadays sql much popular important topic article querying analysis database overall practical section use mysql workbench run query make sure download workbench community server let get started first let u understand sql sql stand structured query language language used relational database query get data database sql let access manipulates database learning sql always fun easy many flavor sql like mysql sqlite sql server postgre sql oracle use mysql flavor much similar little difference use workbench editor mysql practice use mavenmovies database uploaded mavenmovies database sql file github repository please head download run sql file workbench get database table database related database mainly contains detail customer business inventory table row column row refer record column refer field table contain primary key foreign key primary key always unique primary key identifies unique record database whereas foreign key non unique element sql statement called big element element form structure sql statement select identifies column column need queryfrom identifies table querying fromwhere filter result based condition optional group group data result optionalhaving filter result group filtering optionalorder order result either ascending descending order optionalan sql statement look like select column name table name logical condition group column name logical condition order column namethis like operator useful need fetch record know little word allows use pattern like actually example count count recordscount distinct count distinct valuesmin find smallest valuemax find largest valueavg find average valuesum find sum valuesfetch list first name last name email customersfetch record film see rental duration fetch record customer whose first name mary fetch payment record first customer amount greater payment done th january fetch list film include behind scene special feature fetch count title sliced rental durationfetch count film along average min max rental rate grouped replacement costfetch list customer_ids le rental timefetch list film title along length rental rate sort longest shortest case statement helpful working database case statement allow u make conditional logic specify output case statement defined using top pair executes first true case complete continues test condition end statement reached multiple condition true output based first condition since first evaluated top bottom none given condition true null value returned output case statement also work logical operator used statement logical operator include
56,https://www.analyticsvidhya.com/blog/2022/06/building-scalable-secure-and-responsible-ai-solutions-in-azure/,dear reader bringing caio onboard conduct interactive session datahour u working senior cloud solution architect microsoft artificial intelligence hold future world interested learn build scalable secure responsible solution using azure sharing personal experience giving insight various aspect book seat sure want fomo day day organization evolving becoming ai driven datahour join caio moreno learn build scalable secure responsible ai solution azure deploy machine learning model production scale explore topic like prerequisite genuine interest data sciencecaio morenosenior cloud solution architect microsoftcaio moreno senior cloud solution architect microsoft responsible helping microsoft empower every person organization planet achieve using data ai experience artificial intelligence machine learning big data iot distributed system analytics streaming business intelligence data integration visualization also phd candidate complutense university madrid enjoys travelling kind sport life london wife daughter grab fantastic opportunity registering datahour attending session preliminary question topic please send u email protected could ask directly speaker session missed previously conducted datahour series head youtube channel check recording read synopsis previous held datahour session blog facing difficulty registering wish conduct session u get touch u email protected
57,https://www.analyticsvidhya.com/blog/2022/06/want-to-learn-blockchain-start-your-journey-here/,article published part data science blogathon article learn blockchain blockchain transaction made bitcoin transaction made nowadays hearing many attack computer system security guaranteed security blockchain come picture blockchain provides highest security article also see make blockchain secure nowadays blockchains popular blockchain mainly deal data security blockchain technology future data science handling data possible using blockchain quantity data represented big data say blockchain quality data blockchain ensures data integrity come data analysis difficult monitor change help blockchain distributed nature change monitored one side data science utilizing data whereas blockchain providing security let get started blockchain described collection record linked strongly resistant alteration protected using cryptography name suggests blockchain chain block contains information data stored inside block difficult change complete transaction without deal bank online wallet third party application easier way blockchain concept used let u see actually block contain block contains mainly three part data hash hash previous blockdata based type blockchain data stored block depends example take example bitcoin blockchain data contains detail transaction including detail sender receiver number bitcoins hash hash identifies block content inside block every person unique fingerprint similarly block unique hash value hash unique fingerprint two block hash block created hash created unique value hash unique value change block result change value change hence secured hash previous block every block contains hash value present block hash value previous block change one block hash value present block also change different hash value previous block present next block make entire blockchain invalid use hashing technique keep blockchains safe tampering nowadays computer faster able calculate hundred thousand hash per second attacker tamper block recalculate hash block second make blockchain valid deal blockchains use something called proof work proof work mechanism blockchain slows creation block let u take case bitcoins bitcoins case take minute calculate required proof work add new block chain mechanism make harder attacker make attack block therefore blockchain security come combination hashing proof work bank transaction done could fail many time reason may due technical issue bank side account hacked attacker daily transfer limit transfer charge solve kind issue cryptocurrencies came picture cryptocurrencies form digital currency exist ther virtual currency fake currency problem avoided using cryptocurrency require central authority protected strong complex encryption algorithm hard tamper market hundred thousand cryptocurrencies among bitcoin first position bitcoin type digital currency independent central bank record transaction maintained let u understand bitcoin transaction made taking one example imagine two person bitcoins person sends bitcoins person finally person bitcoins person bitcoins way data updated every transaction person sends bitcoins person record created form block block contains detail transaction record also number bitcoins multiple person multiple record created every transaction block linked called blockchain chain record called ledger ledger shared among person hacker can not alter blockchain user copy ledger bitcoin network multiple user user network two key two key public key private key public key address everyone network know private key unique address particular user know person want send bitcoins person wallet address person hashed private key encrypted digitally signed becoming public key person person decrypt using private key different cryptocurrencies use different hashing algorithm bitcoin us sha algorithm ethereum us ethash algorithm blockchain technology accessible anyone without permission data present verifiable zero knowledge proof data permanently stored data present can not tampered attacker third party involved entire process size block fixed hence can not scaled blockchain technology newly evolved technology can not trusted lot energy needed verification transaction adding block time taking process country banned blockchain technology creation blockchain peaked lot people interest blockchain technology used application like storing medical record creating digital notary even collecting tax blockchain technology useful provides high standard security overall article seenthe medium shown article owned analytics vidhya used author discretion
58,https://www.analyticsvidhya.com/blog/2022/06/secure-password-generator-using-python/,article published part data science blogathon article see build password generator password generate python application generate random string desired length nowadays using many application website require password setting strong password important avoid attack attacker keep information safe build application using python tkinter pyperclip let u see requirement needed u build application python python programming language use build application tkinter tkinter graphical user interface library using tkinter one easiest way build gui based application application use tkinter build window generate random password pyperclip pyperclip module python used copying pasting text application generating password also option copy password random password generated randomly generate password randomly use random module random module generates random number string string module python help creating customizing string let u move implementation part project start importing required module application import tkinter pyperclip random string library preinstalled install import installing library use pip install install basically use jupyter notebook run code open anaconda prompt run command install library use prompt install install tkinterto install pyperclipto install randomto install stringsnow import library tkinter import library import everything particular module use next step initialize window generate password giving number digit password use tkinter first initialize win variable tk function using geometry function set width height window using title function pas title window set password generator height width using configure method set background color window top window text placed saying password generator bold letter ariel font font size background color use pack function arrange widget window place input box user input number digit password contain place text password length arial font font size bold letter using intvar function set integer data function hold integer data later retrieve data spinbox provides range value user input user enter digit scroll number select length password generates password length python code text spinbox look like coming stringvar function also similar intvar function stringvar function hold string data define function called generator generates random password firstly password initialized empty string setting password contains numerical digit alphabet provide enough security system application password combination uppercase letter lower case letter numerical digit punctuation first four digit password set random uppercase letter random lowercase letter random digit random punctuation remaining value random combination uppercase lowercase digit punctuation create button follows generator command generator function defined generating password button contains text generate password blue background white foreground button user friendly nice gui need stick ui defined article change text font color many make window beautiful play around make window according expectation generate password button look like thisour next step copy password use pyperclip copy password get string pass_str function copy using pyperclip create button follows command copy_password text copy clipboard button next configure button mean button look like button contains blue color background white color foreground use ariel font font size bold letter use pack function organize widget according size frame set top pady copy clipboard button look like completion writing code execute main loop run application finally window look like see contains title password generator top password length spinbox user select number digit password contain vary default digit user click generate password button password appear empty box click copy clipboard button generated password copied clipboard future purpose run main loop execute entire application see created password digit password got sh_ny user friendly application useful application password generator interesting exciting thrilling application use secret password generator building strong password password generator store password anywhere clear data soon left window without hesitation build secret strong password using password generator article overall seen medium shown article owned analytics vidhya used author discretion
59,https://www.analyticsvidhya.com/blog/2022/06/multi-table-analysis-with-mysql/,article published part data science blogathon article see work multiple table learning sql important nowadays popular technology company using sql query used querying table process data analysis analysis done using one single table also using multiple table see analyze multiple table work query mysql editor called workbench make sure download workbench community server along workbench use mysql editor work let get started practical purpose use mavenmovies database contains relational table quite enough database u implement query detail customer like name address detail business like staff actor rental also detail inventory like film category quite interesting database uploaded sql file mavenmovies database github repository please head download sql file process structuring table column relational database way minimize redundancy also preserve data integrity called database normalization normalization includes normalization database end lot duplicate record normalization data single merged table divided multiple related table cardinality refers uniqueness value column table commonly used describe two table relate relationship table include one one one many many many table may two type key namely primary key foreign key one many relationship table created connecting foreign key one table primary key another table use join multi table querying first join table e need query resultant whole point table relationship enable multi table querying getting data multiple table mavenmovies database table related follows basically five type join select column table join table table column table columnfor inner join inner join righttablename lefttable columnname righttable columnname left join left join righttablename lefttable columnname righttable columnnamefor right join right join righttablename lefttable columnname righttable columnnamefor full join full join righttablename lefttable columnname righttable columnnamefor union union select samecolumnname secondtablenamebasically practice mostly use two join one inner join left join join return record condition satisfied write query using multiple table need specify table column name e inventory inventory_id else give ambiguity error mainly left join used need fetch record left table also record right table similarly right join used need fetch record right table also record left table case need fetch data two table column common search third table column table table third table serf bridge two table example mavenmovies database key connect customer table directly city join customer address using address_id address city using city_id address table serf bridge customer table city table fetch record film title description store_id value associated item inventory_id fetch list title figure many actor associated titlefetch list actor title appear infetch list distinct title description currently available inventory store create one list staff advisor name include column noting whether staff member advisorcreate list customer name actor name including column noting whether customer actorfetch list customer name along store_id active status address querying multiple table real use multi table analysis single table analysis mysql make easy query database simple manner logic question well understood overall mysql article seen medium shown article owned analytics vidhya used author discretion
60,https://www.analyticsvidhya.com/blog/2022/06/data-science-in-web-3-0/,article published part data science blogathon result centralization internet able serve billion user build solid foundation upon continue thrive time world wide web controlled central organization arbitrarily decide permitted difficulty solved web rather controlled major corporation web decentralized network established maintained owned user individual business sway web let take look arrived point analyze much internet influence day day activity worth thinking internet impacted society platform social networking app purchase internet undergoing yet another fundamental transition common people conceive web foundation contemporary life around since inception contrast today web substantially different initially envisaged best understood breaking brief history web two distinct period web web beginning one version web web large majority participant consumer data whereas majority contributor often developer built website delivered information mostly form text graphic time tim berners lee working protocol world wide web cern geneva proposed creation open decentralized protocol would enable anybody planet share knowledge web created berners lee almost minimal interaction user web led dubbed read web since little user generated material beginning web era coincided launch first major social medium platform read write functionality eventually replaced web original read mode instead providing information consumer corporation begun provide platform user may exchange user generated content connect one another previously company provided content user people started use internet smaller number successful organization began control increasingly disproportionate share web traffic value created income model fueled advertisement also born web user able contribute content could profit commercialization way phrase web come represent broad vision future internet blockchains cryptocurrency nfts heart web ownership based model pp technology like virtual reality metaverse internet thing may help make internet open decentralized web era underway web user data static could read information firm unaware user activity web allows people read write create online helping corporation analyze user data people may engage exchange information via social medium web algorithm aim analyze individual data customize internet user web total control data protection encryption permission case case sharing information way done keep track data using web trust issue prevalent web large corporation controlled personal data payment infrastructure web supported financial institution payment processor enable user transmit money directly inside browser without need trusted third party web make use token like eth sol identity handled quite differently web user wallet address often used link online identity web application new age customer wary revealing personal information online number data breach web era problem solved web world wide web today unrecognizable compared early day internet look back even year see significant change beginning web already starting appear although still unclear future bring play potential significantly alter way internet function also potential result tiny change web far significant advancement another technological iteration solves variety concern yet handled web technology security threat poor customer experience number conundrum currently faced browser designed web standard company data area primarily concerned analysis optimization data trend supply data driven solution result data informed choice development model transform data insight product suggestion user able sell data directly company whichever choose company able deploy data scientist integrate blend new data source existing data set enhance learning model sell resulting insight data firm may build language model provide contextual understanding design solution personalized user since data connected user interaction web additionally data firm able extract insight raw data transform insight superior product suggestion enhanced product recommendation may deliver better customer experience mostly based expectation consumer introduction web job profile data scientist set change significantly coming year aspiring professional field must educated advancement order deal successfully efficiently expertise data scientist crucial process extracting insight unstructured data matter time open database host ever increasing amount information data scientist make raw data actionable foretell future data scientist work become harder public datasets proliferate consensus rise paper outline talent road map data scientist next year firm may handle recruiting training retention remain competitive data scientist play far greater role global economy web future assist development content ai model organize interact ai model help organization enterprise tackle complicated challenge possible danger technology evolving intriguing yet significant flaw data science advance web arrival data scientist know new technology affect web promotes new technology better data collection processing visualization improve data science make simpler non technical folk utilize web provide new chance appealing model machine user engaged involved curating creating sharing data surely induce diverse commercial economic situation unanticipated trend cloud iot machine learning analytics delivering organization new customer information raising consumer expectation web immense promise organization pose problem utilizing real time information analysis deliver fantastic consumer experience medium shown article owned analytics vidhya used author discretion
61,https://www.analyticsvidhya.com/blog/2022/06/getting-started-with-docker-image/,article published part data science blogathon created flow diagram get simpler idea deep diving docker looking diagram understand really happening using docker docker installed simply call docker run command docker looking image local machine installed run image docker search image docker hub learn docker hub following article image found docker downloads image local machine creates local container start program think looking like software installation logically yes technically get answer reading article getting docker let look traditional system using instead docker understand moving docker picture see traditional web browsing method let think happen million user access server server get slower slower right solve issue let see classic strategy problem arise strategy one server hardware shutdown reason user face difficulty solve multiple solution came one best solution docker reason moving virtual machine docker virtual machine completely running full operating system requires high memory requirement use docker container rather virtual machine wow finally arrive topic docker open source platform building deploying managing containerized application ecosystem around creating running container docker software using container runtime example ecosystem contains docker client docker server docker machine docker image docker hub docker composecontainers simplify delivery distributed application become increasingly popular organization shift cloud native development hybrid multi cloud environment simply say docker make really easy install run software without worrying setup dependency client talk server docker p client need particular image ask image docker pull server server find image registry give back client registry contains image uploaded various developer freely solution normal system container workswindows http doc docker com desktop window install ubuntu http doc docker com engine install ubuntu mac http doc docker com desktop mac install see image id considered top latest id created image docker always update latest layer image exactly mean take image layer modification finally get modified image top layer every modification docker giving different id layer image getting latest layer id last simply understand layered mechanism using photoshop create one website run via one server create container name run detached mode background process access via local machine need expose port number default web server port forwarding check web server localhost see index html file inside docker imagelet create new mypage html created exit bash use ctrl dnow see layer modification using docker diff command c mean changed mean addednote big disadvantage modification previous unwanted layer also behind layer overcome issue use docker file going docker file using create image stop web server create new container using image different port identify image future let give tag repository name share image friend saving redirecting file alternatively save image using following commandafter saving file share file friend friend receives file load file using following commandyou use repository name created earlier creating repository get command upload docker push lmcshub myhttpd v example command log docker account using docker login command ask username password login success use docker push lmcshub myhttpd v example command get error container name different docker hub repository need create container name prefix lmcshub used avoid name duplication different account image finally push work hope learned using docker moved docker traditional system install docker pull image run container simple example also got clear knowledge docker command please install docker ready next article next article docker using docker file learned far docker image medium shown article owned analytics vidhya used author discretion
62,https://www.analyticsvidhya.com/blog/2022/06/insurance-charges-prediction-using-mlib/,article published part data science blogathon mlib article working predict insurance charge imposed customer willing take health insurance predicting pyspark mlib library driver execute whole process machine learning pipeline gon na work real world insurance dataset downloaded kaggle providing link reference without wait let get started first step import library pyspark required complete machine learning process e data preprocessing model building phase firstly imported sparksession pyspark set spark session correlation library eventually help finding colinearity two variable finally last imported function module allow u use predefined statistical function pyspark step spark object created access deliverable function library spark offer new virtual environment created step involved ml pipeline outputin section reading dataset using read csv function pyspark moving forward process let know dataset particular dataset essential high priority feature could help u detect life insurance charge possible embrace card holder feature followsthe charge column target independent feature output code breakdownas discussed read insurance dataset csv format using read csv function keeping inferschema parameter true return fundamental data type column along kept header true first row would treated header column last displayed data using pyspark show method time statistically analyze dataset start extracting total number record present outputinference output state total record dataset able get help count function output inference code tried get much information column present analysis conclusively got know total column name well let look schema e structure dataset would able know type column outputinference output see adjacent feature see data type also flag condition state whether column null value note closely look result one see sex child smoker region feature string format categorical variable hence coming discussion convert time see statistical inference dataset get information related count mean standard deviation minimum maximum value corresponding feature outputinference output draw multiple inference see feature count e mean null value maximum age data minimum similarly max number child minimum e child put note describes function give ample information one way see record one similar one previously come across panda data processing e head function outputinference one notice return row object assume want analyze data per record e tuple grabbing using head function could better approach correlation one kind technique help u getting accurate prediction help u know relationship two variable return likely positively negatively related particular problem statement finding correlation dependent variable continuous integer one independent variable outputinference correlation age insurance charge equivalent bmi insurance charge related value note used two variable though logically multiple option string format put analysis guy repeat process conversion step section article converting string type feature valid categorical variable machine learning model understand feature know ml model work dependent variable numerical format hence important step proceed stringindexer function come rescue note first converting required field categorical variable look line line explanation outputoutputoutputas already know pyspark need combined level feature e feature piled single column treated one single entity form list inference imported vector ml package vectorassembler feature package ml pile dependent field outputinference column one see output total column need string one instead one changed integer type inference combining feature pas relevant column name form list input column parameter see change well need transform original dataset well output inference creating new dataframe send across model model creation note show function used truncate false mean feature list show far done step required model building phase hence time split dataset training testing form one used training model one test inference random split function see training data testing data outputoutput inference describe function used top split dataset see multiple information like total count training set one phase article building model using linear regression algorithm dealing continuous group feature best go choice u current possible problem statement outputfirstly embedding linear regression object passing feature column already separated label column target feature e charge outputthen lr object created fit training data got random split method output one see returned valid information model e number feature hold far done model development phase time evaluate model get inference whether worthy model evaluate function call metric involved model evaluation phase decide accuracy model outputhere result valid evaluation metric available time make prediction output code breakdown final step prediction based model built comparing actual result predicted one finally predict insurance charge help pyspark mlib library performed every step ground level e reading dataset making prediction evaluated model let discus descriptive way whatever learned far medium shown article owned analytics vidhya used author discretion
63,https://www.analyticsvidhya.com/blog/2022/06/inserts-updates-deletes-in-sqlalchemy-1-4-2-0-core/,article published part data science blogathon sqlalchemy library python database toolkit consists comprehensive tool work database two portion e core orm use python code directly perform operation create read update delete help speeding development time often make managing data le error prone sqlalchemy core consists sql expression language sql expression language allows u create sql expression expression executed targeted database finally return dataset performing operation necessary first connect database establish connection creating object called engine object establishes connection database use method create_engine create engine object python code create object metadata create table example creating user_account table store username emailid contains primary key id also use constraint table example creating foreign key constraint using guide create foreign key constraint import foreignkey sqlalchemy shown resultsyntax example examplein example using insert statement target table value clause executing using engine object connection another way inserting value value clause generated automatically orso far inserted different row table update function used generate update statement update existing table database example basic update statement look like updating email id user username match sandy executing statement also make use column expression update statement update using bindparam many parameter set invoked using statement correlated subquery used update statement using row another table use additional table clause automatically generates update clause see resultthe delete function used create delete statement deletes row table syntax example simple delete statement like deleting multiple tablescorrelated subqueries including clause used delete function delete row related multiple table resultwe get row count indicates number row match clause update delete statement update delete method also support returning selected column row match criterion statement result object using update returning delete returning method result object iterated get row matched learned basic sqlalchemy core article learned basic sqlalchemy concept likethe medium shown article owned analytics vidhya used author discretion related
64,https://www.analyticsvidhya.com/blog/2022/06/apache-pig-high-level-data-flow-platform/,article published part data science blogathon apache pig high level programming language may used analyse massive amount data pig developed consequence yahoo development effort program must converted succession map reduce stage mapreduce architecture data analyst conversant programming approach top hadoop abstraction named pig constructed bridge gap thanks apache pig people spend time evaluating large data set le time implementing mapreduce application internally pig script translated map reduce task run hdfs data pig run code apache tez apache spark among thing apache pig process data type whether structured semi structured unstructured store result hadoop data file system every operation accomplished pig also accomplished java mapreduce yahoo researcher built apache pig earlier pig created primary goal executing mapreduce operation huge datasets time moved apache software foundation asf making open source project pig first version released programmer fluent java traditionally struggled hadoop particularly conducting mapreduce operation programmer apache pig godsend tool many feature apache pig let u look apache pig component process pig latin language various level follows parser parser receives user supplied programme conduct syntax type check procedure produce dag including pig latin sentence logical operator optimizer step sends dag logical optimizer logical optimisation compiler stage optimal logical plan turned mapreduce task stage optimal logical strategy turned mapreduce task execution engine mapreduce job sent hadoop execution final stage completion user receives desired data pig architecture divided two part image source http www guru com introduction pig hive html pig latin programme made set operation transformation executed input data generate output procedure define data flow hadoop pig execution environment convert executable form output modification sequence mapreduce task hidden programmer pig hadoop sense let programmer concentrate data rather nature execution piglatin relatively stiffened language utilises common data processing phrase like join group filter pig two execution mode hadoop local mode hadoop pig language operates single jvm us local file system mode mode useful analysing tiny datasets hadoop using pig map reduce mode mode pig latin query converted mapreduce job executed hadoop cluster cluster may pseudo fully distributed pig execute huge datasets mapreduce mode fully distributed cluster certain difference apache pig map reduce let u analyse many us application apache pig let u see four data model apache pig atom basic data type pig latin atomic also known scalar data type utilised kind string float int double long char byte primitive data type another name atomic data type field column cell value atomic data type tuple tuple ordered collection field symbol used indicate tuple index field used access field tuple bag collection tuples potentially varying structure contain duplicate bag similar table rdbms contrast rdbms table bag require every tuple include amount field column place type map associative array user write function special purpose processing task like reading writing data apache pig analyse different type data structured unstructured support many data type support user defined function allowing user write function programming language like java broad look apache pig summarize discussed far medium shown article owned analytics vidhya used author discretion
65,https://www.analyticsvidhya.com/blog/2022/06/web-3-0-all-you-need-to-know/,article published part data science blogathon every day billion people use world wide web read write share information web changed past year current application nearly unrecognizable early day evolution web frequently divided three stage web web web must heard buzzword blockchain technology bitcoin world wide web www already known u web also known web idea new iteration world wide web www based blockchain technology early day read web developed considered web later internet became interactive read write feature important feature called social medium said web web era exactly web feature web advantage web going see need know web article read web known web read write web named web new version web web going read write execute web web data content supplied static file system rather database web even interaction le web current version web anyone participate creation process one need developer participate creation process web also known semantic web era alludes web future web new version web based blockchain technology blockchain system record information way becomes difficult almost impossible hack change also known distributed ledger help machine learning artificial intelligence computer analyze data human main focus previous version tagging end user experience web main focus user empowerment privacy security driving technology previous version html cs javascript ajax web based technology like artificial intelligence machine learning blockchain technology web make use graphic well decentralized web individual able control data decentralization data network make easier data producer sell exchange data without losing ownership ii artificial intelligence ai help distinguishing data real fake help ai analyzing data personalizing result become easier hence website filter offer best result user help providing faster relevant data user iii graphic web website make efficient use graphic game also healthcare area e commerce geographical setting real estate market application iv ubiquity metaverse ubiquity term refers concept present multiple place simultaneously help feature internet made accessible everyone anywhere anytime data privacy security shared human enterprise end user regain complete ownership control data security encryption one best advantage web anyone access data anywhere world ensures user access information much possible minimum service disruption single point failure data stored distributed node ensure redundancy besides multiple backup prevent server failure individual control identity others trusted platform provided blockchains like ethereum technology data fully encrypted also rule unbreakable though benefit like efficient searching browsing efficient interaction drawback challenge faced importantly developing web practical pretty complex le advanced device able handle web bit difficult understand newcomer besides inconsistent data lead logical contradiction unpredictive analysis though ai help filtering data provided data misleading become huge problem web also known semantic web give example web apple siri best example possible create web application without metaverse involved bitcoin one example many application web sapien bitcoin everledger etc web future internet bring many feature user interact improvement browsing make reliable user friendly help blockchain technology artificial intelligence rise new social platform search engine marketplace built owned collective rather corporation besides user create platform profit reclaim data key takeaway article web future internet also known semantic web web provide read write feature also exchange information feature help searching browsing experience improved web using blockchain technology artificial intelligence machine learning developing web complex job change current scenario web browsing experience medium shown article owned analytics vidhya used author discretion
66,https://www.analyticsvidhya.com/blog/2022/06/an-beginners-guide-to-geospatial-data-analysis/,article published part data science blogathon geospatial data type data certain geographic factor like latitude longitude etc geographic component simply mean location several location take form simple point complex shape describing line boundary even elevation example include country border outline water body global supply chain etc gi geospatial information system geographic spatial data help identify geographic location feature boundary earth mean every data represented reference earth spatial data always represented combination bunch coordinate topology whenever target particular geographic event analyze solve problem using gi mechanism along geographic information time temporal information information attribute information required geopandas free open source python package used reading writing analyzing vector dataset help plotting different intellectual plot extends datatypes functionality used panda allow spatial operation geometric type simple word geopandas library built top python panda library used basic data preprocessing geopandas extends panda functionality work vector data visualization depends fiona file access matplotlib visualization data short geopandas simple python library function various dependency need install like panda numpy matplotlib fiona shapely benefit installing geopandas using conda automatically installs dependency fulfill need geopandas function also install descartes library used essential plotting geographic data need run two command also install geopandas using pip command system command prompt extra dependency need install manually case pip working google colab kaggle kernel need install geopandas notebook simple json file nepal municipal corporation download file using link using colab kaggle kernel directly read data using url first thing import library read data python code way read file multiple file format like json shp xml etc always possible get dataset direct file supposed load different source one database organization collect store data database gi data analyst load data organization database first need implement connection database working using username password database name data present database like mysql mongodb postgre sql etc need write sql query retrieve data sql table use geopandas read data load dataframe sample code reference metadata kind information data geopandas coordinate function provides information regarding cr coordinate system observe geometric type geographic data use function visualize vector dataset simply use plot function geopandas backend us matplotlib python library contains fill color blue plot graph based certain column mention column name plot function different color also various parameter define plot function example plotting district wise plot also legend property plot graph legend design using subplots position sample snippet designing graph write save dataframe vector format using geopandas save dataframe csv format panda good understanding geospatial data time pick new dataset try hand data visualization analysis using learned explore learning understand geospatial analysis popular used everywhere business planning decision making dataset similar used dataset esri district dataset many shape file esri district shape file area interest etc access data file link create new python file using python ide create new jupyter notebook working collab kaggle kernel first thing proceed analysis part import necessary library load data check type data geopandas geo dataframe different district northern island seen geometry column get created import vector file like shp file geo json file contains information corresponding geometrical property attribute working geopandas geo dataframe skip delete geometry column even accident cause loose geometrical property mean going special data anymore visualize esri shapefile using geopandasgeopandas plot function used simply plot geographic data geo dataframe draw plot interactive border district using edge color change color graph using color attribute indeed use c map property design blog good way different attribute range value use different color map well like hsv turbo etc find attribute value color map reach official documentation area interest shape filewe load many shape file want might interested know area interest might especially mean consider special extent different district northern island load area interest shape file plot plot information multiple fileswe learn analyze two file plot plot information file different representation side side one another also define plotting information different source single plot plot two figure one need define n row instead defining n column give resultant plot manner plotting multiple layerslayers simply mean plot multiple layer single plot two plot separated one single diagram mean different axis plot different plot single ax observe second area interest layer hide first layer define color attribute none use edge color limited layer add many want atm shape file dataset draw layer know different atm located working projection geopandasin case coordinate reference system layer using basically geographic coordinate reference system reason able plot thing accurately special sense contains predefined coordinate reference system check coordinate reference system data epsg working different situation gi find situation required work different coordinate reference system depending need example calculate area attribute area calculated meter square kilometer layer geographic coordinate reference system wgs going get computation decimal point look reproject geopandas geo dataframe one coordinate reference system another demonstrate reproject district layer epsg wgs projected coordinate system quite frequently used uk coordinate reference system shall converting epsg visually observe projected coordinate reference system visually changed x axis unit meter plot district multiple layer reprojected geopandas geo dataframe find intersect two layersintersect two layer basically mean find common area layer dataset find many district lie area interest find union two layersin way find union two layer used analyze complete area single plot obtain symmetric difference polygon using geopandas symmetric difference mean finding new set element either set intersection part opposite finding intersection removing intersection find difference polygon difference simply subtracting area second polygon first polygon also form vice versa output approximately similar symmetric difference output use dissolve geoprocessing operation using geopandas dissolve operation basically eliminates boundary enumeration unit common value create much larger area holding common value polygon perform union operation add one common column union geopandas geo dataframe add value one dissolve across boundary common value dissolve operation basically used combine similar feature within data layer creating bufferbuffer mean measure distance outward direction object done three type data point line area already learned projection reproject district dataset new coordinate reference system create buffer meter obtaining centroid polygonscentroid mean locate center polygon simply find using centroid function demo demonstrate centroid district union plot geopandas used python library gi analysis gi software evolving developer today mostly prefer type tool analysis make easy represent create bi report geographic analysis used every business today order scale sale business across world capture new market easily let u summarize learning tutorial key point remember utilize power geospatial data analysis medium shown article owned analytics vidhya used author discretion
67,https://www.analyticsvidhya.com/blog/2022/06/newsql-the-bridge-between-sql-and-nosql/,article published part data science blogathon newsql specific nosql relational data sql query language foundation newsql system want address nosql movement scalability flexibility lack focus difficulty consistency provided new sql within category however difference hana robust business reporting platform could also handle low transactional demand making perfect sap deployment hekaton add enhanced memory processing typical m sql server system currently non clustering designed quickly replace supplement sql deployment suppose gigabyte terabyte data require high speed transactional access may input stream sensor mobile phone network access point require per event transaction compute real time answer analytics using ingest analyze decide approach tackle issue analytics choice determined per request rather processing batch processing case newsql system combine scalable architecture nosql greater consistency best option sql query language choice transaction supported atomicity consistency isolation durability property concurrency control technique locked e g mvcc architecture significantly improved per node performance shared nothing scale architecturecompletely new database platform first type newsql system made work cluster shared nothing node node owning portion data database frequently built architecture mind feature like concurrency management flow control query processing google spanner clustrix voltdb memsql pivotal gemfire xd sap hana nuodb tidb trafodion example system category difference sql nosql new sql following figure depicts difference nicely figure sql v nosql v newsql parameter compare sql nosql newsql scalability scalable read scalable read writes horizontally scalable scalable read writes horizontally scalable schema relational schema table schema free support high availability custom high availability auto high availability built high availability storage disk cache disk cache disk cache cloud support fully supported fully supported query complexity low high high acid cap base acid cap base acid oltp fully supported supported fully supported security concern high low low example oracle mssql mysql etc mongodb cassandra redis etc cockroachdb google spanner voltdb effective newsql database newsql intriguing technology becoming widely available top newsql database market today adaptable powerful solution meticulously designed fit company requirement voltdb voltdb founded lately risen become one prominent newsql provider booming memory database market python java php c among language supported voltdb open source commercial tool designed operate oltp query includes acid guarantee sharding data replication employ deterministic concurrency management global serial order controller sharded transaction span many partition clustrixdb clustrixdb database provides good peace mind thanks automated fault tolerance mechanism designed preserve numerous data copy clustrix compatible mysql combine multi version concurrency control mvcc two phase locking input conflict resolution founded recently bought mariadb clustrix various benefit aws popular cloud based database server including sophisticated failover fault tolerance feature capacity expand perform beyond tb additional feature cluster based cloud architecture include online schema modification linearly scalable online growth nuodb nuodb founded cambridge massachusetts widely used prominent fintech company dassault systèmes french design corporation purchased november supply scalable multi cloud distributed sql database solution dassault système utilizes nuodb manage worldwide database online component maker client bringing together engineer designer world cockroachdb cockroachdb adaptable newsql database invented built cockroach lab utilized broad range well known business including jpmorgan chase cisco best buy addition cockroachdb strong tool organizational automation since simply link cloud based cluster solution like kubernetes coackroachdb created ex google personnel based spanner us open source distributed sql database horizontal scaling highly consistent transactional layer vmware tanzu gemfire newsql database previously known pivotal gemfire xc bought vmware renamed vmware tanzu gemfire gemfire multi tiered geo distributed clustered cloud database exceptional performance gemfire memory horizontally scalable design distinguishes newsql db ensuring consistent low latency even high usage newsql advantage reduces application complexity greater consistency provides complete transactional support elastic scalability dynamically rebalances partition instant event response real time alert resilient shared nothing architecture provides high availability gemfire xc standout characteristic net nodejs support different language frictionless new sql us common tool sql familiar analytics including sql extension many new sql system combine traditional data rdbms query model nosql style clustering newsql limitation newsql system intended general purpose sql system memory system may suit data set larger terabyte provides limited access sql system extensive tool conclusion newsql subset current rdbms aim match scalability nosql system read write workload online transaction processing oltp keeping acid atomicity consistency isolation durability feature traditional database system primary limitation nosql lack acid feature article explains newsql tool performed also describes significant difference sql nosql newsql newsql like bridge sql nosql newsql type database solution gaining lot attention community reference http medium com rabiprasadpadhy google spanner newsql journey beginning end nosql era beecparameters comparesqlnosqlnewsqlscalable readsscalable read writes horizontally scalablescalable read writes horizontally scalablerelational schema tableschema freesupports custom high availabilityauto high availabilitybuilt high availabilityon disk cacheon disk cacheon disk cachenot fullysupportedfully supportedlowhighvery highacidcap baseacidnot fully supportednot supportedfully supportedvery highlowloworacle mssql mysql etc mongodb cassandra redis etc cockroachdb google spanner voltdbnewsql intriguing technology becoming widely available top newsql database market today adaptable powerful solution meticulously designed fit company requirement voltdb founded lately risen become one prominent newsql provider booming memory database market python java php c among language supported voltdb open source commercial tool designed operate oltp query includes acid guarantee sharding data replication employ deterministic concurrency management global serial order controller sharded transaction span many partition clustrixdb database provides good peace mind thanks automated fault tolerance mechanism designed preserve numerous data copy clustrix compatible mysql combine multi version concurrency control mvcc two phase locking input conflict resolution founded recently bought mariadb clustrix various benefit aws popular cloud based database server including sophisticated failover fault tolerance feature capacity expand perform beyond tb additional feature cluster based cloud architecture include online schema modification linearly scalable online growth nuodb founded cambridge massachusetts widely used prominent fintech company dassault systèmes french design corporation purchased november supply scalable multi cloud distributed sql database solution dassault système utilizes nuodb manage worldwide database online component maker client bringing together engineer designer world cockroachdb adaptable newsql database invented built cockroach lab utilized broad range well known business including jpmorgan chase cisco best buy addition cockroachdb strong tool organizational automation since simply link cloud based cluster solution like kubernetes coackroachdb created ex google personnel based spanner us open source distributed sql database horizontal scaling highly consistent transactional layer newsql database previously known pivotal gemfire xc bought vmware renamed vmware tanzu gemfire gemfire multi tiered geo distributed clustered cloud database exceptional performance gemfire memory horizontally scalable design distinguishes newsql db ensuring consistent low latency even high usage newsql subset current rdbms aim match scalability nosql system read write workload online transaction processing oltp keeping acid atomicity consistency isolation durability feature traditional database system primary limitation nosql lack acid feature referencethe medium shown article owned analytics vidhya used author discretion
68,https://www.analyticsvidhya.com/blog/2022/06/overview-of-web-3-0-blockchain-and-its-aspects/,article published part data science blogathon recently attended course web presented product house product house growing startup provides learning earning opportunity web space course six week curriculum covering web blockchain aspect metaverse article explain summarize concept following curriculum structure time go course understand concept via article web decentralized version current web mean le control entity user web product secure transparent private posted web basic stuff recently check concept web terminology article cover concept following post follows roller coaster ride tighten seat belt get cup coffee try throw decentralized finance defi short open global financial system internet age let understand existing financial system web financial system relies bank store transfer money bank control transaction account activity example ban deactivates account bank hand control might recover bank account later rare case would put much time energy defi come rescue eliminating third party control defi network give complete control data account activity defi network also permissionless hence developer access build top immutable since blockchain defi activity lending borrowing token swapping rely smart contract defi protocol user lock crypto asset contract others use contract called liquidity pool liquidity pool serve high risk high reward appetite liquidity pool maintain balance mathematical formula coded alongside smart contract amms automated market maker defi liquidity get expressed term total value locked tvl according metric site defi llama tvl defi billion till april dao stand decentralized autonomous organization organization user also control decision making daos virtual entity certain set member stakeholder perhaps majority right spend entity money modify code every decision entity go voting first majority voter decide decision make two version available daos mathew ball defines metaverse network real time rendered virtual world experienced synchronously unlimited number user individual sense presence experience happen continuous flow data communication identity object history payment entitlement moreover live work shop interact fun together metaverse kind stuff anywhere time considered next big step internet revolution different internet know well let see got metaverse requires following layer operate efficiently daos defi play crucial role metaverse since defi application handle finance daos take care decision making token non interchangeable non fungible called nfts assume two bill one signature football player lionel messi simple one imagine first bill messi signature worth interchange bill anyone similar kind bill bill become nft thus nfts digital asset buy nft marketplace opensea looksrare etc nfts different use case fungible token erc etc nfts follow erc ethereum request comment standard improves ethereum network erc first nft standard many come want learn concept please check product house course web fundamental lot learn web space since space growing changing rapidly article covered overview web fundamental concept someone want build career web must understand fundamental first key takeaway article follows find article informative please share friend web enthusiast moreover please reach feedback correction suggestion medium shown article owned analytics vidhya used author discretion
69,https://www.analyticsvidhya.com/blog/2022/06/building-an-etl-data-pipeline-using-azure-data-factory/,article published part data science blogathon etl process extract data various data source transforms collected data load data common data repository help organization across globe planning marketing strategy making critical business decision azure data factory adf cloud based etl data integration service provided azure build complex etl process scheduled event driven workflow using azure data factory guide show build etl data pipeline convert csv file json file hierarchy array using data flow azure data factory source http github com mspnp azure data factory sqldw elt pipelineetl process collect data various source transforms data usable trusted resource using various operation like filtering concatenation sorting etc load data destination store etl stand extract transform load describes end end process organization prepare data business intelligence process etl commonly used data integration data warehousing cloud migration machine learning artificial intelligence etl provides mechanism move data various data source common data repository help company planning marketing strategy making critical business decision analyzing business data etl pipeline data pipeline set process used move data various source common data repository data warehouse data pipeline set tool activity ingest raw data various source move data destination store analysis storage source http databricks com glossary extract transform loadnow learn etl process work three step etl process extract first step extract raw data various data source place staging area data came different data source crm transactional processing apis sensor data etc could structured semi structured unstructured transformation required data performed staging area performance source system degraded extraction divided three type partial extractionb partial extraction update notification c full extract transform data extracted previous step raw need clean map transform data convert data format used different application data inconsistency inaccuracy removed data transformation apart data mapping data standardization also done data standardization refers process converting data type format various function rule applied prevent entrance bad inconsistent data destination repository load loading data destination repository last step etl process better performance load process optimized data delivered shared secure way required user department used effective decision making storytelling source http www informatica com hk resource article etl htmlin today world huge amount data generated every day among large volume data mostly raw unorganized however can not use raw data provide meaningful insight analyst ml engineer data scientist business decision maker therefore fulfilling requirement mechanism required convert raw data actionable business insight azure data factory adf cloud based etl data integration service provided azure build complex etl process scheduled event driven workflow using azure data factory example imagine e commerce company collect petabyte product purchase log produced selling product cloud company want analyze log gain insight customer preference customer retention active customer base etc log stored inside container azure blob storage analyze log company need use reference data marketing campaign information customer location purchase detail stored azure sql database company want extract combine data source gain useful insight scenario best use azure data factory etl purpose additionally company publish transformed data data store azure synapse analytics business intelligence bi application power bi consume storytelling dashboarding adf built connector retrieving data various data source hdfs azure service teradata amazon redshift google bigquery servicenow salesforce etc adf make process creating data pipeline easy providing data transformation code free data flow ci cd support built connector data ingestion orchestration pipeline monitoring support got good understanding etl azure data factory let move forward see top level concept adf source http azure microsoft com en service data factory featuresbelow top level concept azure data factory dataset datasets identify data within different data store used input output activity example azure sql table dataset specifies sql table azure sql database either want take insert data inside activity must create linked service link data store required service creating dataset linked service linked service define connection information needed service connect data source example azure sql database linked service specifies connection string connect azure sql database additionally azure sql table dataset specifies sql table azure sql database contains data linked service used either represent data store compute resource adf activity activity task performed data three type activity adf data movement activity data transformation activity control activity example use copy activity copy data azure blob storage azure sql pipeline pipeline logical grouping activity together perform unit work data factory may one one pipeline activity pipeline specify task performed data user validate publish monitor pipeline example create pipeline get triggered new blob arrives azure blob storage container copy data azure blob storage azure sql source http doc microsoft com en u azure data factory concept pipeline activity tab data factory mapping data flow mapping data flow provides way perform data transformation data flow designer without writing code mapping data flow executed activity within adf pipeline integration runtimes integration runtime provides computing environment activity either run get dispatched trigger trigger determine pipeline execution need kicked pipeline manually triggered based occurrence event example create pipeline get triggered new blob arrives azure blob storage container copy data azure blob storage azure sql pipeline run instance pipeline execution known pipeline run parameter parameter defined pipeline value consumed activity inside pipeline variable variable used inside pipeline store value temporarily control flow control flow allows building iterative sequential conditional logic within pipeline source http doc microsoft com en u azure data factory concept pipeline activity tab data factory code free data transformation adf provides mapping data flow executed activity pipeline mapping data flow provides way perform data transformation data flow designer thus data transformation easily performed without writing code easy migration etl workload cloud adf used easily migrate workload one data source another consumption based pricing data factory provides pay go pricing model upfront cost required customer end better scalability performance adf provides built parallelism time slicing feature therefore user easily migrate large amount data cloud hour thus performance data pipeline improved security user protect data store credential adf pipeline either storing azure key vault encrypting certificate managed microsoft pipeline logical grouping activity together perform unit work activity task performed data example use copy activity copy data azure blob storage azure sql three type activity present inside adf data movement activity data movement activity used move data one data source another example copy activity used copy data source location destination data transformation activity data transformation activity used perform data transformation adf important data transformation activity data flow data flow activity used perform data transformation via mapping data flow b stored procedure stored procedure activity invokes sql server stored procedure pipeline c azure function activity help run azure function data pipeline azure function serverless compute service help user run event triggered code databricks notebook activity help run databricks notebook azure databricks workspace pipeline e spark spark activity executes spark program hdinsight cluster control activity control flow activity used build iterative sequential conditional logic pipeline various control activity available adf activity condition activity execute pipeline lookup activity etc source http doc microsoft com en u azure data factory concept pipeline activity tab data factorynow familiar various type adf activity top level adf concept let see build etl data pipeline convert csv file json file hierarchy array using data flow azure data factory going use container azure storage account storing csv json file blob building etl data pipeline scenario required create azure storage account azure data factory follow step build etl data pipeline azure storage account provides highly available secure storage storing variety unstructured data text image binary data log file etc create azure account sign follow step create azure storage account step visit azure home page click create resource step select storage account popular azure service create step basic page select subscription name create resource group provide storage account name select performance redundancy region click next step advanced page configure hierarchical namespace blob storage security azure file setting per requirement click next frequent data access choose hot tier step networking page enable public access network configure network routing click next step click review create home page get displayed storage account created successfully select data storage container step click container provide employee new container name select container public access level click create step create blob launch excel copy following text save file named emp csv system step upload emp csv csv file employee container inside storage account step click container provide employeejson new container name select container public access level click create successfully uploaded csv file blob storage azure data factory adf cloud based etl data integration service provided azure build complex etl process scheduled event driven workflow using azure data factory using step create data factory step visit azure home page click create resource step azure marketplace search data factory select create data factory step basic page select subscription name select existing resource group provide data factory name select region data factory version click next step git configuration page choose configure git click next step networking page fill required option click next step click review create data factory created successfully data factory home page displayed open azure data factory studio new tab already created azure data factory follow step inside azure data factory studio create etl pipeline step click new pipeline rename pipeline convertpipeline general tab property section step click data flow new data flow inside data flow click add source rename source csv step source tab select source type dataset dataset click new create source dataset search azure blob storage select continue data format delimitedtext continueprovide linked service name select checkbox first row header click new create new linked service new linked service provide service name select authentication type azure subscription storage account name click create linked service created page get redirected set property page select emp csv path file path click ok step click select derived column schema modifier derived column page select create new column provide info column name inside info select add subcolumn write fname column name expression select firstname expression value inside info select add subcolumn write lname column name expression select lastname expression value select create new column provide skill column name expression select array expression element provide skill skill skill expression value parameter array click save finish step click select sink destination sink tab write outputfile output stream name select derivedcolumn incoming stream select source type dataset dataset click new create source dataset search azure blob storage select continue data format json continuewrite outputjsonfile name choose linked service created step linked service select employeejson container path file path click ok provide file name setting tab select input column output column mapping tab step convertpipeline pipeline search data flow activity activity drag designer surface select created dataflow inside data flow activity step validate pipeline clicking validate validating pipeline publish pipeline clicking publish step run pipeline manually clicking trigger step verify convertpipeline run successfully visiting monitor section azure data factory studio step see uploaded csv file successfully converted json file hierarchy array seeing file content inside employeejson container seen create etl data pipeline convert csv file json file using azure data factory got understanding etl pipeline built using azure data factory cost effective secure easily scalable learned build etl pipeline real time scenario following major takeaway etl organization use etl effective decision making extract different type data using different extraction method seen azure data factory used ingest data various source got understanding use pipeline activity linked service datasets etc azure data factory also seen easily perform data transformation using data flow learned validate publish pipeline understand connect pipeline dataflows got good understanding use control flow various type activity available azure data factory executed etl data pipeline convert csv file json file hierarchy array apart saw create container azure storage account linked service azure data factory etc medium shown article owned analytics vidhya used author discretion
70,https://www.analyticsvidhya.com/blog/2022/06/how-to-train-an-ner-model-with-huggingface/,article published part data science blogathon natural language processing nlp subfield linguistics focus computer ability understand language form text speech nlp task includes speech recognition task converting voice data text data used chatbots voice search system voice command iot device etc sentiment analysis sentiment analysis aka opinion mining nlp technique used determine whether given sentence phrase delivers positive negative neutral message named entity recognition also known ner used extract entity using pre trained category article focusing named entity recognition ner real world use case end train custom model using huggingface embeddings ner named entity recognition simple word one key component nlp natural language processing used recognition extraction entity predefined pre trained category plain unstructured text entity anything like person name location organization country city etc depending category train model let take example example model extract person organization location entity example yes indeed magic ner article go basic definition ner use case train custom ner model using hugging face flair embeddings use ner real world efficient search algorithm ner used extract relevant entity search query better search result resume parsing various company mncs ner used resume parsing extracting relevant information candidate appearing job filter best possible candidate among thousand applicant pii personal identifiable information extraction protecting user personal information one crucial task every company need take care ner help extract pii entity name dob credit card number ssn phone number etc masked chatbot typical usage ner chatbot chatbots use ner extract keywords answering user query etcetera etcetera yupp let train model use hugging face flair embedding train ner model hugging face company provides open source nlp technology significant expertise developing language processing model one problem ner need extensive data training need worry conll_ come rescue conll consists large annotated unannotated dataset training testing validation read conll_ starting training must know format ner training data ner dataset contain two column separated single space first column consists single word followed named entity tag second column note column must contain single word note connl_ consists column column containing individual word column part speech tag column syntactic chunk tag column named entity tag case column omitted optional let take example let text george washington went washington format would okay see confused worry let explain might wondering hell b per per b stand beginningi stand intermediateso phrase consists related word use annotation example george washingtonhere george first name washington last name ner know type word come together running model piece text prefix word b followed tag per loc org etc creating dataset also help model differentiate similar entity like example washington george washington last name also location another example united state america need know dataset training custom ner model let break key flair huggingface embedding used perform various ml ai task install flair embedding use following command need import following class embeddings corpus conll_ used get conll_ corpus flair support number embeddings provides different functionality combine word different way embeddings help ner perform better wordembeddings stackedembeddings flairembeddings initializing required variableas main goal perform named entity recognition provide tag_type ner since using multiple embeddings need stack together use stackedembeddings training model finally let train model resource tagger ner english path want save model train_with_dev true want use dev dataset corpus training model max_epochs maximum random shuffled iterationsnote max_epochs value better resultstesting modelnow model trained test first import required module sentence used create sentence object provide model prediction entity sequencetagger used load trained modelwith ner develop much awesome stuff customer support chatbot resume parser custom search engine content recommendation system pii entity extraction etcetera etcetera etcetera imagination limit scratch head get hand dirty might create something revolutionary key takeaway article medium shown article owned analytics vidhya used author discretion
71,https://www.analyticsvidhya.com/blog/2022/06/linear-regression-using-mlib/,article published part data science blogathon article learning linear regression using mlib everything hand e building end end linear regression model predict customer yearly spend company product talk dataset completely dummy dataset generated purpose understand concept model building continuous data using mlib getting machine learning process following step predict customer yearly spending must need initialize spark session read dummy dataset e commerce website relevant feature particular section setup spark object able create environment perform operation supported managed inference two code line successfully imported sparksession object pyspark sql package created environment using getorcreate function one thing note creating built using builder function given name e commerce section reading dummy dataset created perform ml operation along data preprocessing using pyspark inference line code read ecommerce data kept inferschema parameter true return real data type dataset posse header true first tuple record stated header schema dataset shown one could get inference kind data column hold analysis could done precision output inference used printschema function show information column dataset hold looking output one see kind data type go dataset using three different way one could also know method investigate looking data using show function return top row complete data head function need introduced quite similar head function used panda code output see head function returned row object hold one complete record tuple output let see clear version getting data item iterable combination loop head function output shown clear version row object output output mentioned earlier gon na predict customer yearly expenditure product based already know deal continuous data working type data use linear regression model reason importing linear regression package ml library pyspark section data preprocessing technique performed required make dataset ready sent across ml pipeline model could easily adapt build efficient model importing vector vectorassembler library could easily separate feature column label column e dependent column stacked together feature column independent column label column let look column present dataset output inference output column listed form list type give u enough information column select hence reason use describe method output inference go output closely find column string data type role model development phase machine learning involvement mathematical calculation number game allowed hence integer double data type column accepted based discussion column selected part machine learning pipeline follows output inference code chose vectorassembler method stack feature column together return feature column output column parameter transform function used fit real data change done assembler variable using vectorassembler function change reflect real dataset output select function selected feature column dataset showed form dataframe using show function code concatenating stack dependent feature named feature independent feature together naming final_data frame analyzed process step model building dividing data training set testing set training data one top model built hand testing data one test model well performed mlib dividing data testing training set use random split function take input form list type inference help tuple unpacking concept stored training set train_data similarly dataset test_data note random split method list passed output output inference describe method seems accurate way analyze draw difference training testing data see training set record hand finally come across step building linear regression model linearregression object used remember imported starting passed yearly amount spent column label column parameter independent column created linear regression object easily fit data e model training passing training data fit method let print coefficient feature intercept model trained training dataset also one piece information let know well model involving independent variable separately output step evaluating model e analyze well model performed stage model building decide whether go existing one model deployment stage evaluation come across evaluate function stored test_results variable use analysis one know mathematical intuition behind linear regression must aware fact residual original result predicted result e difference predicted result model original result label column output time make prediction model first store unlabelled data e feature data transform change take place output inference output see returned dataframe practically two column one complete stack feature column one prediction column section see far learned article mention nutshell gone complete machine learning pipeline linear regression algorithm repo link article hope liked article introduction linear regression using mlib opinion question comment connect linkedin discussion medium shown article owned analytics vidhya used author discretion
72,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-introduction-to-tensorflow-javascript/,dear reader bring another episode datahour series deep learning subfield machine learning inspired biological neuron brain translated artificial neural network representation learning datahour session umang take fun ride live demo sure wish miss awesome opportunity check detail forget register deep learning wonder year still developer find difficult learn implement practice come tensorflow j rescue tf j introduces wonderful world deep learning make easy deep learning browser session going tf j datahour umang sharma start basic deep learning proceed see real cool live deep learning fun demo later tf j much developer wondered deep learning worked include deep learning web apps talk prerequisite enthusiasm learning deep learning data science umang sharma senior data scientist year experience building deep learning solution multiple area computer vision natural language processing reinforcement learning developed cutting edge ai solution multiple industry finance power retail currently work leading tech product firm national science academy summer research fellow aged year old umang also active speaker ai community well keynote speaker multiple national international conference talk lecture imparted ml knowledge participant recently invited speaker indian institute management also one earlier contributor google colab called google datalab umang one early adopter tensorflow j spoken various conference including google devfest even authored book tf j grab fantastic opportunity registering datahour attending session preliminary question topic please send u email protected could ask directly speaker session missed previously conducted datahour series head youtube channel check recording read synopsis previous held datahour session blog facing difficulty registering wish conduct session u get touch u email protected
73,https://www.analyticsvidhya.com/blog/2022/06/deploy-huggingface-training-model-rapidly-based-on-metaspore/,article published part data science blogathon day ago huggingface announced million series c funding round big news open source machine learning could sign industry headed two day huggingface funding announcement open source machine learning platform metaspore released demo based huggingface rapid deployment pre training model deep learning technology make innovative breakthrough computer vision natural language processing speech understanding field unstructured data perceived understood processed machine advance mainly due powerful learning ability deep learning pre training deep model massive data model capture internal data pattern thus helping many downstream task industry academia investing energy research pre training technology distribution warehouse pre training model huggingface timm emerged one another open source community release pre training significant model dividend unprecedented speed recent year data form machine modeling understanding gradually evolved single mode multi mode semantic gap different mode eliminated making possible retrieve data across mode take clip openai open source work example pre train twin tower image text dataset million picture text connect semantics picture text many researcher academic world solving multimodal problem image generation retrieval based technology although frontier technology semantic gap modal data still heavy complicated model tuning offline data processing high performance online reasoning architecture design heterogeneous computing online algorithm born multiple process challenge hindering frontier multimodal retrieval technology fall ground pratt whitney dmetasoul aim technical pain point abstracting uniting many link model training optimization online reasoning algorithm experiment forming set solution quickly apply offline pre training model online paper introduce use huggingface community pre training model conduct online reasoning algorithm experiment based metaspore technology ecology benefit pre training model fully released specific business industry small medium sized enterprise give text search text text search graph two multimodal retrieval demonstration example reference multimodal semantic retrievalthe sample architecture multimodal retrieval follows multimodal retrieval system support text search text search application scenario including offline processing model reasoning online service core module huggingface open source community provided several excellent baseline model similar multimodal retrieval problem often starting point actual optimization industry metaspore also us pre training model huggingface community online service searching word word image word searching word word based semantic similarity model question answer field optimized metaspore searching image word based community pre training model community open source pre training model exported general onnx format loaded metaspore serving online reasoning following section provide detailed description model export online retrieval algorithm service reasoning part model standardized saas service low coupling business interested reader refer previous post design concept metaspore new generation one stop machine learning platform offline processing offline processing mainly involves export loading online model index building pushing document library follow step step instruction complete offline processing text search image search see offline pre training model achieves reasoning metaspore search text text traditional text retrieval system based literal matching algorithm bm due user diverse query word semantic gap query word document often encountered example user misspell iphone phone search term incredibly long month old baby autumn small size bag pant traditional text retrieval system use spelling correction synonym expansion search term rewriting mean alleviate semantic gap fundamentally fail solve problem retrieval system fully understands user query term document meet user retrieval demand semantic level continuous progress pre training representational learning technology commercial search engine continue integrate semantic vector retrieval method based symbolic learning retrieval ecology paper introduces set semantic vector retrieval application metaspore built set semantic retrieval system based encyclopedia question answer data metaspore adopted sentence bert model semantic vector representation model fine tune twin tower bert supervised unsupervised way make model suitable retrieval task model structure follows query doc symmetric two tower model used text search question answer retrieval vector representation online query offline doc share vector representation model necessary ensure consistency offline doc library building model online query inference model case us metaspore text representation model sbert chinese qmc domain v optimized open source semantically similar data set model express question answer data vector offline database construction user query expressed vector model online retrieval ensuring query doc semantic space user semantic retrieval demand guaranteed vector similarity metric calculation since text presentation model vector encoding query online need export model use online service go q data library code directory export model concerning documentation script pytorch tracing used export model model exported export directory exported model mainly onnx model used wired reasoning tokenizer related configuration file exported model loaded metaspore serving online serving system described model reasoning since exported model copied cloud storage need configure related variable env sh retrieval database built million level encyclopedia question answer data set according description document need download data complete database construction question answer data coded vector offline model database construction data pushed service component whole process database construction described follows following example database data format offline database construction completed various data pushed corresponding service component milvus storing vector representation document mongodb storing summary information document online retrieval algorithm service use service component obtain relevant data search text text image easy human relate semantically difficult machine first perspective data form text discrete id type one dimensional data based word word time image continuous two dimensional three dimensional data secondly text subjective creation human being expressive ability vibrant including various turning point metaphor expression image machine representation objective world short bridging semantic gap text image data much complex searching text text traditional text search image retrieval technology generally relies external text description data image nearest neighbor retrieval technology carry retrieval image associated text essence degrades problem text search however also face many issue obtaining associated text picture whether accuracy text search text high enough depth model gradually evolved single mode multi mode recent year taking open source project openai clip example train model massive image text data internet map text image data semantic space making possible implement text image search technology based semantic vector text search picture introduced paper implemented based semantic vector retrieval clip pre training model used two tower retrieval architecture clip model trained semantic alignment twin tower text image side model massive graphic text data particularly suitable text search graph scene due different image text data form query doc asymmetric twin tower model used text search image retrieval image side model twin tower used offline database construction text side model used online return final online retrieval database data image side model searched text side model encodes query clip pre training model guarantee semantic correlation image text model draw graphic pair closer vector space pre training large amount visual data need export text side model online metaspore serving inference since retrieval scene based chinese clip model supporting chinese understanding selected exported content includes onnx model used online reasoning tokenizer similar text search metaspore serving load model reasoning exported content need download unsplash lite library data complete construction according instruction whole process database construction described follows online service overall online service architecture diagram follows multi mode search online service system support application scenario text search text search whole online service consists following part user request perspective service form invocation dependency back front build multimodal sample need run service front back first remember export offline model put online build library first article introduce various part online service system make whole service system step step according following guidance see readme end article detail query preprocessing service deep learning model tend based tensor nlp cv model often preprocessing part translates raw text image tensor deep learning model accept example nlp class model often pre tokenizer transform text data string type discrete tensor data cv class model also similar processing logic complete cropping scaling transformation processing input image preprocessing one hand considering part preprocessing logic decoupled tensor reasoning depth model hand reason depth model independent technical system based onnx metaspore disassembled part preprocessing logic nlp pretreatment tokenizer integrated query pretreatment service metaspore dismantlement relatively general convention user need provide preprocessing logic file realize loading prediction interface export necessary data configuration file loaded preprocessing service subsequent cv preprocessing logic also integrated manner preprocessing service currently provides grpc interface invocation externally dependent query preprocessing qp module retrieval algorithm service user request reach retrieval algorithm service forwarded service complete data preprocessing continue subsequent processing readme provides detail preprocessing service started preprocessing model exported offline cloud storage enters service debug service improve efficiency stability model reasoning metaspore serving implement python preprocessing submodule metaspore provide grpc service user specified preprocessor py complete tokenizer cv related preprocessing nlp translate request tensor deep model handle finally model inference carried metaspore serving subsequent sub module presented lot code http github com meta soul metaspore compare add_python_preprocessorretrieval algorithm service retrieval algorithm service core whole online service system responsible triage experiment assembly algorithm chain preprocessing recall sorting invocation dependent component service whole retrieval algorithm service developed based java spring framework support multi mode retrieval scenario text search text search graph due good internal abstraction modular design high flexibility migrated similar application scenario low cost quick guide configuring environment set retrieval algorithm service see readme detail user entry service considering retrieval algorithm service form api interface difficult locate trace problem especially text search image scene intuitively display retrieval result facilitate iterative optimization retrieval algorithm paper provides lightweight web ui interface text search image search search input box result display page user developed flask service easily integrated retrieval application service call retrieval algorithm service display returned result page also easy install start service done go http see search ui service working correctly see readme end article detail multimodal system demonstration multimodal retrieval service started offline processing online service environment configuration completed following instruction example textual search shown enter entry text search map application enter cat first see first three digit returned result cat add color constraint cat retrieve black cat see return black cat strengthen constraint search term change black cat bed return result containing picture black cat climbing bed cat still found text search system color scene modification example cutting edge pre training technology bridge semantic gap different mode huggingface community greatly reduce cost developer use pre training model combined technological ecology metaspore online reasoning online microservices provided dmetaspore pre training model longer mere offline dabbling instead truly achieve end end implementation cutting edge technology industrial scenario fully releasing dividend pre training large model future dmetasoul continue improve optimize metaspore technology ecosystem medium shown article owned analytics vidhya used author discretion
74,https://www.analyticsvidhya.com/blog/2022/06/introduction-to-hadoop-architecture-and-its-components/,article published part data science blogathon hadoop open source java based framework used store process large amount data data stored inexpensive asset server operate cluster distributed file system enables processing tolerance error developed doug cutting michael j cafarella hadoop us mapreduce editing model quickly store retrieve data node framework owned apache software foundation licensed apache license year processing capacity application server grown exponentially data information lagged behind due limited capacity speed however today many application produce large amount data processed hadoop play key role providing much needed conversion database business perspective direct indirect benefit using open source technology le expensive cloud server sometimes premise organization gain significant cost saving additionally ability collect large amount data well information obtained compiling data result better business decision real world ability focus right consumer sector remove weed fix faulty process improve environment task provide relevant search result perform predictable analysis hadoop solves two important challenge common information power hadoop store large volume data using distributed file system called hdfs hadoop distributed file system data fragmented stored cluster asset server since asset built simple computer hardware configuration economical easy grow data grows speed hadoop store retrieves data quickly hadoop us mapreduce editing model perform processing across data set therefore query sent website instead managing data sequentially task split simultaneously applied distributed server finally output function collected returned application greatly improving processing speed benefit hadoop big datawith big data statistic hadoop life saver data collected people process material tool etc useful sensible pattern emerge turn lead better decision hadoop help overcome challenge big data size resilience data stored node duplicated cluster node ensures tolerance mistake one node go always backup copy data available collection stability unlike traditional system limit data retention hadoop measurable operates distributed environment demand increase set easily expanded include server end petabyte data low cost since hadoop open source framework without license purchased cost much lower compared related database program use inexpensive hardware hardware also work harvest keep solution economical speed hadoop distributed file system simultaneous processing mapreduce model make easy run complex query second data diversity hdfs ability store different data format formatted e g video built e g xml file formatted storing data need verify previously defined schema instead data discarded format later retrieved data sorted added schema needed provides flexibility obtaining different data using data hadoop single application rather platform various key component allow storage processing distributed data component combine form hadoop ecosystem key element forming basis framework others additional component bring additional functionality hadoop world main component hadoop hdfs hadoop pillar maintains distributed file system make possible store duplicate data across multiple server hdfs namenode datanode datanodes server server data actually stored namenode hand contains data metadata stored different location application work namenode connects data node required yarn stand yet another resource negotiator manages organizes resource determines happen data area central master node manages processing request called resource manager resource manager work node manager every slave datanode node administrator perform task mapreduce editing model google first used showcase search function concept used divide data smaller set work basis two function map minimize analyzes data fast efficient way first map task group filter filter multiple data set conjunction produce tuples key value pair minimize function combine data tuples produce output want following widely used component hadoop ecosystem hive data storage system help query large database hdfs prior hive developer faced challenge creating complex mapreduce task query hadoop data hive us hql hive query language similar sql syntax developer coming sql domain hive easy find board advantage hive jdbc odbc driver act visual interface application hdfs expose hadoop file system table convert hql mapreduce function vice versa developer site manager benefit processing large data set use simple common question accomplish founded facebook group hive open source technology pig launched yahoo like hive eliminates need create mapreduce job query hdfs similar hql language used called pig latin closer sql pig latin layer high quality data flow language top mapreduce pig also operating time area meet hdfs text language java python also embedded pig although pig hive similar function one work better another different situation pig useful data preparation phase make complex joining query easier also work well various data format including semi structured unstructured pig latin close sql also varies sql enough learning curve hive however work well organized data result work best data storage used server side collection researcher programmer often use pig client side collection business intelligence user like data analyst find hive good fit flume great data import tool serf courier service multiple data source hdfs collect compiles transmits large amount distributed data e g log file event generated application social networking site iot application ecommerce site hdfs flume rich feature distributed structure ensures reliable data transfer tolerant mistake able collect data batch real time measured horizontally handle additional traffic needed data source communicate flume agent agent source channel sink source collect data sender channel temporarily store data finally sink transfer data destination hadoop server sqoop sql hadoop another data import tool like flume although flume operates informal le structured data sqoop used send data import data related database since business data stored related website sqoop used import data hadoop analyst review webmaster developer use command line interface export import data sqoop convert command mapreduce format sends hdfs using yarn sqoop also able tolerate mistake perform simultaneous task like flume zookeeper service integrates distributed application hadoop framework act administrator central register information set distributed server manage key function storage configuration information shared configuration data status composition service naming per server sync service handling deadlock race status data incompatibility leader selection selecting leader server consistently collection server zookeeper service operates called cluster group chooses leader among group acting fan writing task client need delivered leader reading go directly server zookeeper provides high reliability durability unsafe synchronization atomicity message editing kafka widely used messaging system used hadoop faster data transfer kafka collection consists group server act liaison manufacturer consumer context big data manufacturer example could sensor collect temperature data transmission back server buyer hadoop server manufacturer publish message topic consumer draw message listening topic one topic divided section message key arrive somewhere buyer may listen one portion combining message one key making buyer face certain part multiple buyer listen topic time therefore title consistent increase system output kafka widely accepted speed durability durability hbase column based unrelated site resides hdfs one challenge hdfs batch processing simple interaction question data still processed cluster leading high delay hbase solves challenge allowing single line query across large table low latency achieves using hash table inside made google bigtable line help access google file system gfs hbase measurable support failure site descends good informal centralized data therefore best inquire big data store analytical purpose although hadoop widely regarded key enabling large data still challenge consider challenge stem nature complex ecosystem need advanced technical knowledge perform hadoop operation however proper integration platform tool complexity greatly reduced therefore make working even easier steep learning curveto inquire hadoop file system program editor must write mapreduce task java incorrect involves steep learning curve also many component make ecosystem take time get used different data set require different methodsthere one size fit solution hadoop many additional component discussed designed address gap needed filled example hive pig provide easy way query data set additionally data entry tool flume sqoop help collect data multiple source many component take experience make right choice limit mapreducemapreduce excellent planning model processing large data set however limitation file based approach lot reading writing well suited real time analytical data analysis repetitive task operation mapreduce work well enough lead high delay strategy work problem apache another way closing mapreduce space data securityas big data transferred cloud sensitive data dumped hadoop server creating need ensure data security great ecosystem many tool important ensure tool right data access right need proper verification provisioning data encryption regular auditing hadoop potential meet challenge matter skill caution practice although many tech giant used hadoop component mentioned still relatively new industry many challenge arise start large solid data integration platform solve alleviate mapreduce model despite many advantage work well interactive query real time data processing relies disk writing processing phase spark data processing engine solves challenge memory retention although started hadoop sub project collection technology usually spark used hdfs use final hadoop feature processing algorithm us library support sql query streaming machine learning graph data scientist make spark lightning fast beautiful rich apis make working large data set easier although spark may seem limited hadoop work together depending need type data set hadoop spark compatible spark file system must rely hdfs solution order saved real comparison actually spark processing mapreduce model ram barrier well night operation mapreduce good fit hadoop software framework used process large amount data quickly framework contains hadoop common map reduce algorithm yet another resource negotiator hadoop distributed file system differs many way relative comparable website decided basically best way process store data medium shown article owned analytics vidhya used author discretion
75,https://www.analyticsvidhya.com/blog/2022/06/performing-neural-style-transfer-in-5-minutes/,article published part data science blogathon neural style transfer way generating image blending two different image word us two image develop new image retains core structure one image styling using image generative model popular creating high quality realistic art also useful gaming virtual reality trending topic meta article explore working architecture neural style transfer model depth however basic overview model built using two network namely feature extractor style transfer network convolutional neural network perform extremely well image datasets capable capturing spatial information extracting low level high level feature present image performing convolution image using filter kernel one main reason exceptional performance cnn image classification task compared neural network architecture feature extraction model usually pre trained deep cnn like xception resnet vgg vgg etc using pre trained deep cnn feature extractor work fact layer learn extract content image layer learn texture feature present image style transfer network usually autoencoder encoder decoder architecture accepts image input return image output refer article learn autoencoders built tensorflow fortunately many trained model various task like classification regression time series analysis etc available tensorflow hub repository high quality trained model maintained tensorflow production ready model accessed fine tuned line code article making use trained style transfer model available tensorflow hub perform neural style transfer mentioned earlier two image used performing neural style transfer one image known content image known style image content image image want apply texture style style image image texture texture extracted transferred content image produce stylized image output using picture pet content image article could anything like selfie picture parent anything general style image using extract texture apply content image generate stylized image downloaded image unsplash provide free use high quality stock image tensorflow hub required accessing trained model tensorflow hub installed like python package local machine module used implementing neural style transfer model article opencv used image loading basic image processing article mentioned earlier using trained neural style transfer model tensorflow hub using trained model tensorflow hub extremely simple easy either download trained model tensorflow hub use loading disk directly provide url trained model using url loading trained model alternatively download neural style transfer model using opencv read preprocess image opencv us bgr color format instead traditional rgb color format necessary change color format rgb also normalize image array reduce computational complexity trained neural style transfer model accepts image tensor float data type necessary perform transformation content style image trained model work best style image square image pixel side dimension image model trained resize style image specified dimension transform float type tensor trained model accepts content style image input parameter return output image performing neural style transfer content image using style image let visualize output image performing neural style transfer using matplotlib alternatively save generated picture local disk png jpg file output image using first style image output image using second style image article explored following concept used trained neural style transfer model directly get job done refer article wish manually build neural style transfer model using pre trained deep cnn perform generate unique high quality picture generated picture sold nft nft marketplace like opensea superrare hope enjoyed reading article learned something new thanks reading happy learning medium shown article owned analytics vidhya used author discretion
76,https://www.analyticsvidhya.com/blog/2022/06/cartoonify-image-using-opencv-and-python/,article published part data science blogathon article build one interesting application cartoonify image provided build cartoonifyer application use python opencv one exciting thrilling application machine learning building application also see use library like easygui tkinter select image application convert image cartoon form mainly build application using opencv python programming language let get started opencv open source library python used mainly computer vision task area machine learning artificial intelligence nowadays opencv playing major role field technology using opencv process image video task like object detection face detection object tracking opencv c c java python interface support kind system window linux android mac o io python use python programming language building application cv use cv image processing numpy mainly numpy used dealing array image use stored form array use numpy easygui easygui module used gui programming python application easygui used open file box upload image local system imageio imageio python library read writes image matplotlib matplotlib used visualization purpose plot image using matplotlib o application o used dealing path like reading image path saving image path tkinter tkinter standard graphical user interface gui package methodology going follow build cartoonify application first using easygui upload image image converted greyscale image next two step important step converting image cartoon image smoothening retrieving edge color image smoothened give cartoon look retrieve edge highlight final image next prepare mask image use bilateral filter remove noise smoothen extent final step giving cartoon effect image got previous step combine two important step finally give mask edged image look like cartoon image let move coding part start importing required library numpy cv easygui imageio sys matplotlib o required tkinter library importance library discussed python code upload image local system define upload function use fileopenbox open box choose file store file path create function cartoonify includes step converting greyscale final cartoon image first step convert greyscale image apply blur smoothen image one main step cartooning image smoothening blur effect given using median blur function retrieving edge highlighting cartoon effect another important step application applying bilateral filter built help remove noise present image provides clean image masking edged image finally plot image contains six tradition throughout process plot transition form list image saved image first resized resized resized resized resized resized single plot plot six finally plt show print image main window upload image run code new window pop give option upload image desktop set geometry like size window title window set cartoonify image set labeling style main window make cartoonify button button help uploading image desktop set button look like color button position window style text window look like run application window pop button naming catoonify image using button upload image desktop also save image local system application give cartoon effect image save image creating save button main window use following code finally run get output transition image finally get output image shown contains transition image final image cartoon image hope enjoyed application cartoon version image using application create cartoon image thrilling overall article seenthe medium shown article owned analytics vidhya used author discretion
77,https://www.analyticsvidhya.com/blog/2022/06/yolo-algorithm-for-custom-object-detection/,article published part data science blogathon article going learn object detection using yolo algorithm using yolo v version easy simpler faster see detect different object also use custom dataset training model collab notebook easy everyone use application image provided detect object present image option capture live image detect object present going let get knowledge computer vision let get started computer vision computer vision basically deal anything human see perceive computer vision enables computer system derive complete meaningful information digital photo digital video visual input object detection image classification image captioning image reconstruction object detection object detection ability detect object identify object given image correctly image classification image classification basically mean identifying class object belongs image captioning image captioning like generating text description given image looking image reconstruction image reconstruction like finding missing part image reconstructing two main approach object detection machine learning approach deep learning approach capable learning identifying object execution far different machine learning application artificial intelligence computer learn data given make decision prediction based usually process using algorithm analyze data learning make prediction determine thing based given data machine learning method object detection sift support vector machine svm viola jones object detection deep learning also referred deep structured learning class machine learning algorithm deep learning us multi layer approach extract high level feature data provided deep learning model require feature provided manually classification instead try transform data abstract representation deep learning method implement neural network achieve result deep learning method object detection r cnn faster r cnn yolo algorithm let see make custom dataset first make folder image make two folder naming train_dataset test_dataset folder make two folder name train_images train_labels test_images test_labels train_images contain image used training train_labels contains label txt file train image wherein text file annotation bounding box yolo format test_images contain image used testing similarly test_labels contain label file test image let move collab notebook start using yolo v algorithm object detection using custom dataset see draw bounding box image drawing bounding box many way see use makesense ai draw first open makesense ai browser click get started add image willing draw bounding box uploading click object detection click start project add label forget order else model identify object wrong manually write class import file class written start project completion drawing bounding box image click action top left click export annotation select zip package containing file yolo format see zip folder text file downloaded yolo stand look yolo algorithm divide image grid system grid detects object within latest version yolo v launched ultralytics yolo v algorithm best object detection algorithm available far simple easier faster detects object high accuracy visit documentation yolo contains version github link let move implementation first mount google drive mounted content drivenow create folder custom_object_detection create another folder called yolov move yolov folder content drive mydrive custom_object_detection yolovthe github link yolo v ishttps github com ultralytics yolovclone github repository yolov folder inside custom_object_detection folder create one dataset folder make two folder namely image label image folder make two folder train val similarly label folder make two folder train val train contains training image training label val contains image label respective folder used validation accordingly upload colab notebook modify custom_data yaml file present inside data folder inside yolov folder change according problem file contain like train val enter path train val image folder enter number class enter class make sure order correct label given drawing bounding box gave example class add class object detection project finally ready train model training model run following command increase number epoch increase accuracy finally find last result saved run train exphere model saved model trained training dataset ready test test model image video also detect object live video use webcam different command command first image object detection paste path image video object detection similarly also paste path video finally come webcam finally get result saved run train expso go folder find image video object detection custom object detection using yolov algorithm easier simple faster previous version yolo also advantage train model need train model testing overall article seen medium shown article owned analytics vidhya used author discretion related
78,https://www.analyticsvidhya.com/blog/2022/06/iris-flowers-classification-using-machine-learning/,article published part data science blogathon article iris flower classification dealing logistic regression machine learning algorithm first see logistic regression understand working algorithm iris flower dataset know iris dataset contains feature different flower specie independent feature dataset sepal length sepal width petal length petal width length centimeter dependent feature output model specie contains name specie particular flower measurement belongs iris dataset first dataset data science student work going creating machine learning model let u understand logistic regression first logistic regression supervised machine learning model used mainly categorical data classification algorithm seeing name logistic regression may think regression algorithm fact classification algorithm generalization linear regression model logistic regression used find relationship dependent independent variable done using logistic regression equation easy implement understand also easy method train model understand think example email getting many email spam using logistic regression find whether mail spam ham classify email label spam ham like logistic regression model take mail content input analyze finally generate label spam give spam ham give saying spam creating model training preprocess dataset preprocessing mean converting dataset understandable format using machine learning algorithm includes data transformation data reduction data cleaning many let u build machine learning model using logistic regression take iris flower dataset link dataset download store local desktop linklet u start importing important basic library matplotlib seaborn used visualization warning ignore warning encounter import dataset local desktop use panda enter path dataset file read_csv method import iris dataset let u view data frame missing value modify using dataset modifying use fillna method fill null value see value mean null value entire data frame view column name data frame use columnsview statistical description dataset contains variable like count mean standard deviation minimum value maximum value percentile column id sepal length sepal width petal length petal width use describe method view python code view data frame final data frame look like view count plot specie feature using seaborn define x x contains input variable independent feature contain dependent variable dependent independent variable output view xwe see x contains column except last column dependent column dependent feature train model next test model split entire dataset train test set training dataset used train model test dataset test model trained training dataset import train_test_split split data train test datasets view shape use shape method view going classify iris flower dataset using logistic regression creating model import logisticregression sci kit learn library train model using fit method fit method pas training datasets x_train y_train training datasets logisticregression predict result using predict method view result give result like contains specie name form array find accuracy model view confusion matrix accuracy score tell u accurately model build predict confusion matrix matrix actual value predicted value import accuracy_score confusion_matrix sci kit learn metric library array dtype int accuracy model see accuracy model percent accurate flower classification important simple basic project machine learning student every machine learning student thorough iris flower dataset classification done many classification algorithm machine learning article used logistic regression overall article seenthe medium shown article owned analytics vidhya used author discretion
79,https://www.analyticsvidhya.com/blog/2022/06/walmart-stock-price-analysis-using-pyspark/,article published part data science blogathon article analyzing famous walmart stock price dataset using pyspark data preprocessing technique start everything beginning end article one experience feel consulting project answering professional financial question based analysis let get started analyze walmart stock using pysparkthis one mandatory thing getting started pyspark e set environment spark use python pyspark library use resource session note importing kind stuff forget install pyspark library command install library pip install pyspark output output break code understand element required creating starting spark environment section reading walmart stock price data using pyspark storing variable use analysis know panda used csv function similarly use read csv function pyspark let discus inference see show function returned top row dataset note kept header type true spark treat first row header inferschema also set true return value real data type section using relevant function pyspark analyze dataset analyzing mean see dataset look like structure formatting need done cleaning part following thing covering let look column name inference output see returned list value value denotes name column present used column object see schema dataset look like output inference using print schema function actually looking data type column dataset note one thing well nullable true signifies column hold null value looping data fetching top row output inference output see returned row object row object hold real top data iterated top data using head function one way extract one multiple tuples record separately note completely optional thing involve analysis using concept want hold row different variable play data point using describe function see statistical information dataset section format dataset make clearer precise clean data easier analyze current state data inference draw give clearer picture result hence first format column nearest integer value along add one column avoiding calculationsso let format dataset perform step one one moving changing formatting data point let u first see column apply change use combination printschema describe function outputinference output see data type method returned describing function output see column hold string value good want format analyze themhence task first change data type column specifically mean standard deviation later format better understanding discussed earlier know time know answer question using format_number function function package help u code breakdownnow let create altogether new dataframe one column named hv ratio stimulate ratio high price total volume stock traded day output one see new dataframe hold ratio discussed field introduced column dataframe help column function simply performed ratio operation showed combination select show statement enough preparing dataset supposed analyze walmart stock price time section give answer question proposed u firm give u feel data science consulting project draw insight using pyspark data preprocessing techniqueinference output see got date stock price highest using orderly function selecting descending order simply used head function little bit indexing fetch data object dataoutput inference output say average closing price fetched value using select statement mean function show mean closing stock pricenote could also used describe method wanted know operation possible pysparkoutput inference get maximum minimum volume stock first imported max min function using function package like walking cake simply used function get desired result using select statement also use describe function output inference get total number day get closing value le dollar follow two step get desired result output inference get maximum value stock price year informative term collective information analyze need follow three step follows section discus whatever learned far blog walmart stock price discussing setup spark session understanding dataset formatting last answering question like consulting project medium shown article owned analytics vidhya used author discretion
80,https://www.analyticsvidhya.com/blog/2022/06/apache-spark-vs-hadoop-mapreduce-top-7-differences/,article published part data science blogathon apache spark released earlier hadoop mapreduce main focus processing large data competitor spark new widely used technology mapreduce still place specific application let take deeper look two popular tool big data mapreduce spark compare decide best specific circumstance organization receiving big data overwhelming pace due smart device increased use internet technology hence necessary handle efficient data analysis becomes crucial several technology used easily process big data two popular hadoop mapreduce apache spark apache spark open source distributed system handling big data workload improves query processing performance varying data size using efficient query execution memory caching mapreduce java based distributed computing programming model within hadoop framework used access large amount data hadoop file system hdfs mapper reducer two job performed mapreduce programming mapper responsible sorting available data reducer charge aggregating turning smaller chunk although tool handle big data let u explore main difference based feature apache spark contains apis scala java python spark sql sql user apache spark offer basic building block allow user easily develop user defined function use apache spark interactive mode running command get instant response hand hadoop mapreduce developed java difficult program interactive mode hadoop mapreduce unlike apache spark however hive provides command line interface used issuing command successive line text spark user friendly feature interactive mode hand hadoop mapreduce difficult program however tool available help apache spark perform many task data processing apache spark handle graph machine learning library mllib great performance apache spark used batch near real time processing apache spark flexible multiple use platform used handle activity rather dividing many platform mapreduce ideal solution batch processing hadoop want real time solution use impala apache storm graph processing use apache giraph earlier mapreduce used apache mahout machine learning task mahout discontinued spark came spark great provides single data framework data processing requirement spark capable handling data processing requirement hand mapreduce le flexible spark come performing variety data processing job mapreduce could one best available batch processing tool apache spark much popular speed run time faster memory ten time faster disk hadoop mapreduce since process data memory ram time hadoop mapreduce persist data back disk every map reduce action spark take lot ram operate effectively spark save process memory keep different instruction given spark used resource demanding service performance may hampered notably additionally spark performance suffer data source large fit fully memory although mapreduce provide data caching service assist little performance downturn since terminates operation moment complete mapreduce spark benefit come performance spark easily best choice big data requirement data get accommodated amount memory space dedicated cluster hand mapreduce better solution large volume data fit neatly memory need data framework coordinating service mapreduce suitable recovery failure spark since us hard drive instead ram spark come back online crashing middle data processing activity start beginning process requires time mapreduce fails job resume left restarts mapreduce based hard disk maintain position fails middle job spark hadoop mapreduce high failure tolerance hadoop mapreduce slightly tolerant apache spark security set default leaving vulnerable threat spark support rpc channel authentication shared secret event logging feature spark web uis protected using java extension furthermore spark operate yarn use hdfs benefit kerberos authentication hdfs file permission encryption node
81,https://www.analyticsvidhya.com/blog/2022/06/deploying-a-keras-flower-classification-model/,article published part data science blogathon deep learning model especially cnn convolutional neural network implemented classify different object help labeled image model trained image great accuracy tested deployed performance example trained image classification model accepts image car identifies brand make car tata maruti suzuki bmw mercedes etc way model also trained classify object pre trained model like vgg vgg resnet etc used transfer learning classify different object minimal code context flower image always pleasant look find variety attractive flower nature different shape enchanting color time see flower like can not identify computer vision help u correctly identify flower specie situation web app accessed browser using phone laptop accept flower image perform prediction identify flower image fact apps built deep learning prove immensely beneficial agriculture horticulture domain image classification important part computer vision application automotive agriculture healthcare transportation logistics traffic management space research many tutorial demonstrate easily build image classification model using kera deploy using gradio image classification model trained classify image different flower import required library package next download flower dataset tensorflow dataset downloaded obtain extract data use untar data function automatically download untar dataset copy dataset available downloading let use following command see many image dataset includes image flower organized five subdirectory dandelion rose tulip daisy sunflower following command used display subdirectory run following command see image rose daisy subdirectory resize image dataset different size specify image height width also specify batch size number image used model epoch using image dataset calling directory function read resize image database also dividing image ratio classification training ratio whereas validation ratio similarly calling directory function read validation image directory therefore total image file belonging class using image file training image file validation class name found class name property datasets alphabetical order correspond directory name also print sample image training dataset using following code create sequential model made three convolution block max pooling layer relu activation function used activate fully connected layer unit top addition rgb channel value ideal neural network use rescaling function normalize value use following code build modelthe following parameter important compile function optimizer chose adam optimizer increase improve convolution neural network performance also manages training error cnn may loss function collect error cnn experience training image dataset various class apply sparsecategoricalcrossentropy five class metric function calculates total cnn accuracy score training set value accuracy view layer network using model summary function next fit model training_images validation_images convolution neural network learn training image perform image classification also setting number epoch epoch output display cnn loss training accuracy validation accuracy score initial training loss score final training loss score show cnn error decreased time initial training accuracy score final accuracy score illustrates number epoch increased cnn performance improved value validation accuracy rise plot loss model accuracy training validation set visualize training result since trained model epoch final validation accuracy score model accuracy improved modifying cnn layer adding dropout layer increasing number layer implementing data augmentation increase size dataset demo purpose use model deploying web app next use predict input image function function work input image take four dimension seen code convert input image four dimension function use img reshape method input image classified using model predict function function produce dictionary containing predicted class associated probability likely class right prediction classification gradio used add user interface interacting trained cnn gradio machine learning library transforms trained machine learning model interactive application gradio provides simple user interface ui enable user interact trained machine learning model generates web interface via user test trained model view prediction result gradio user interface simply integrated straight python notebook either jupyter notebook google colab notebook without need install dependency gradio interacts directly popular machine learning library sckit learn tensorflow kera pytorch hugging face transformer let start installing gradio library import gradio package creating gradio user interface must specify size picture gradio input component store shown code also providing number labeled class image dataset next create gr interface function create ui take created predict_input_image function classify input image take image input output labeled class output running code provide url gradio app link app expires hour given link launch new browser window newly opened tab display gradioui place image submit image classified using gradioui let u upload image image uploaded shown output press submit button get classification result get result hitting submit button indicates successfully deployed app web using gradio tutorial learned build image classifier using kera deploy using gradio key takeaway article hope found article interesting feel free build image classifier choice deploy gradio using step mentioned article find complete code tutorial github repository medium shown article owned analytics vidhya used author discretion
82,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-introduction-to-blockchain-2/,analytics vidhya long forefront imparting data science knowledge community intent make learning data science engaging community began new initiative datahour datahour series webinars top industry expert teach democratize data science knowledge st may joined valerii babushkin datahour session introduction blockchain valerii babushkin work blockchain com head data science worked facebook whatapp user data privacy staff engineer alibaba russia vp machine learning x retail group senior director data science also valerii kaggle competition grand master ranked globally top excited dive deeper world ai blockchain got covered let get started major highlight session introduction blockchain every one must heard blockchain revolutionary technology ability reduce risk fraud scalable manner session able learn blockchain virtual computer run top network physical computer provide strong auditable game theoretic guarantee code run continue operate designed type technology designed displayed image architecture blockchain go following layer consensus layer card called heartland blockchain compute layer might call blockchain computer application list lady team motoko whatever user interface people call web layer compute layer blockchain computerexecution environment bitcoin blockchain ethereum blockchain pricing pricing let say go blockchain com deposit usdt stable coin packed usd receive interest rate nine percent attract people thing bank deposit could attract people eight percent pay le would love example insurance connected healthcare sometimes people fake examination certification receive benefit blockchain could avoid hope enjoyed session understood well employ understanding session real world case mentioned example popular use case blockchain technology cryptocurrency make use case keep learning growing datahour session
83,https://www.analyticsvidhya.com/blog/2022/06/a-complete-guide-on-hough-transform/,article published part data science blogathon hough transform ht feature extraction approach image analysis computer vision digital image processing us voting mechanism identify bad example object inside given class form voting mechanism carried parameter space object candidate produced local maximum accumulator space using ht algorithm ht method separating feature certain shape inside picture classical ht typically employed detect regular curve line circle ellipsis etc need required feature provided parametric form generalized ht used brief analytic description feature attainable limit study traditional ht due computational difficulty extended hough algorithm despite domain limitation classical ht many application manufactured part well many anatomical part investigated medical imagery feature boundary standard curve describe real benefit ht approach unaffected picture noise tolerant gap feature boundary description article provide depth knowledge hough transform give basic introduction hough transform exactly work math behind also enlists merit demerit hough transforms various application first developed automated examination bubble chamber picture hough hough transform patented u patent assigned atomic energy commission united state method mean recognizing complex pattern slope might go infinity invention employ slope intercept parameterization straight line result infinite transform space duda r p e hart use ht detect line curve picture comm acm vol pp january although standard radon transformation since frank gorman mb clowes finding picture edge collinearity feature point gorman clowes discus version ieee transaction computer vol pp hart p e ht invented ieee signal processing magazine vol issue page november tell tale contemporary form ht invented ht feature extraction method image analysis computer vision digital image processing us voting mechanism identify bad example object inside given class form voting mechanism carried parameter space first object candidate produced local maximum accumulator space using ht algorithm traditional ht concerned detecting line image subsequently expanded identifying location arbitrary shape often circle ellipsis richard duda peter hart devised ht knew today calling generalized ht paul hough related patent dana h ballard popularized transformation computer vision field journal paper generalizing ht identify arbitrary form many circumstance edge detector used pre processing stage get picture point pixel required curve image space however may missing point pixel required curve due flaw either image data edge detector spatial variation ideal line circle ellipse noisy edge point acquired edge detector result grouping extracted edge characteristic appropriate collection line circle ellipsis frequently difficult original image lanefigure image applying edge detection technique red circle show line breaking hough approach effective computing global description feature potentially noisy number solution class need provided example hough approach line identification motivated assumption input measurement reflects contribution globally consistent solution e g physical line gave rise image point line described analytically variety way one line equation us parametric normal notion xcosθ ysinθ r r length normal origin line θ orientation given figure known variable e xi yi image constant parametric line equation whereas r unknown variable seek point cartesian image space correspond curve e sinusoid polar hough parameter space plot potential r θ value specified hough transformation straight line point curve transformation collinear spot cartesian image space become clearly obvious examined hough parameter space provide curve overlap single r θ point b circle center coordinate r radius three coordinate parameter space accumulator algorithm computing complexity increase general number parameter increase calculation size accumulator array polynomially result fundamental hough approach described applicable straight line determine range ρ θ typically range θ degree ρ diagonal length edge therefore crucial quantify range ρ θ mean finite number potential value create array called accumulator dimension num rho num theta represent hough space set value zero use original image edge detection ed whatever ed technique like check pixel edge picture see edge pixel pixel edge loop possible value θ compute corresponding ρ locate θ ρ index accumulator increase accumulator base index pair iterate accumulator value retrieve ρ θ index get value ρ θ index pair may transformed back form ax b value greater specified threshold problem given set point use hough transform join point b c e solution let think equation line ax b rewrite line equation keeping b lh getb ax write equation point consider x getb following table show equation given pointnow take x find corresponding value given five equationslet u plot new point graph given figure see almost line cross point b let put value ax b equation get x x line equation link edge ht benefit requiring pixel single line contiguous result quite effective identifying line small gap due noise object partially occluded ht following drawback produce deceptive result object align accident rather finite line definite end detected line infinite line defined c value ht widely employed numerous application benefit noise immunity application object form detection lane traffic sign recognition industrial medical application pipe cable inspection underwater tracking example example application proposes hierarchical additive hough transform haht detecting lane line haht recommended accumulates vote various hierarchical level line segmentation multiple block also minimizes computational load proposes lane detection strategy ht merged joint photographic expert group jpeg compression however simulation used test method generalized hand detection model articulated degree freedom shown make use geometric feature correspondence point line line recognized pht following matched model ght identify turkish finger spelling produce interest zone scale invariant feature transform applied sift superfluous interest zone eliminated using skin color reduction also suggests using hand gesture tracking create real time human robot interaction system procedure linked component labeling ccl ht integrated sensing skin tone extract hand center orientation fingertip position outstretched finger detector line box block proposed approach decrease computing cost using hough space parallel line detection addition ht used extract planar face point cloud unevenly dispersed describes method detecting object utilizing ght color similarity homogeneous portion item take input already segmented area homogenous hue according research resistant change light occlusion distortion segmentation output distinguish item rotated scaled even moved around complicated environment us blend uniform gaussian hough transforms conduct shape based tracking camera move method find follow object even difficult backdrop like dense forest provides technique detecting geometrical form underwater robot image recognition problem transformed bounded error estimation problem opposed traditional ght autonomous underwater vehicle creates underwater system visually directed operation auv underwater pipeline identified using ht orientation determined using binary morphology ht various version various industrial commercial us uncrewed aerial vehicle uav surveillance inspection system used propose knowledge based power line identification approach applying lht pulse coupled neural network pcnn filter constructed eliminate background noise picture frame finding refined using knowledge based line clustering us warped frequency transform wft adjust dispersive behavior ultrasonic guided wave followed wigner ville time frequency analysis ht increase fault localization accuracy location boundary vertebra recognized automatically ht used conjunction ga determine migrating vertebra section show unexpected usage demonstrate ht adaptability ever expanding nature proposes data mining technique locating arbitrarily oriented subspace cluster done mapping data space parameter space defines collection arbitrarily oriented subspace may created clustering method work locating cluster may hold many database object even subspace cluster differing dimension sparse crossed cluster noisy environment method discover code line detection using hough transform python code outputover year hough transform gained lot scientific attention noise immunity capacity deal occlusion expandability transform major motivator interest developed many different form span whole form detecting range line irregular shape new variant predicted emerge bringing transformation closer recognizing complicated thing transform derivative mostly used binary image hough transform used wide range application including traffic biometrics object detection tracking medical application industrial commercial application yet opportunity future expect parallel computing particularly gpus aid development time critical application article help beginner start hough transform also lead future enhancement application medium shown article owned analytics vidhya used author discretion
84,https://www.analyticsvidhya.com/blog/2022/06/a-complete-guide-on-building-an-etl-pipeline-for-beginners/,article published part data science blogathon etl pipeline set process used transfer data one source database like data warehouse extraction transformation loading three interdependent procedure used pull data one database place another organization generate data data source data type need analytics data science machine learning initiative generate business insight grows well prioritizing initiative becoming increasingly critical crucial translate raw messy data clean fresh reliable data pursuing data engineer use etl extract transform load extract data multiple source transform data usable trusted resource load resource system end user access use downstream solve business problemsetl process conducted via etl pipeline also known data pipeline data pipeline set tool action transferring data one system another might stored managed differently pipeline also enable automatic gathering data variety source well transformation consolidation data single high performance data storage process first step extract data target source usually heterogeneous business system apis sensor data marketing tool transaction database among others clear data type likely structured output widely used system others semi structured json server log three method extracting datathe next step transform raw data extracted source format accessed different application goal stage clean map transform data operationally useful involves several type transformation guarantee quality integrity data typically data loaded directly target data source instead uploaded staging database first case something go planned easily rollback create audit report regulatory compliance analyze data issue fix finally load function process copying converted data staging area target database may may existed earlier method might straightforward complex depending application requirement etl tool custom code used complete process traditional etltraditional legacy etl meant data totally premise overseen skilled house staff whose job build manage house data pipeline database usually based time consuming batch processing session allow data transported scheduled batch ideally network traffic low difficult perform real time analysis team often construct extensive labor intensive adaptation stringent quality control extract essential analytics furthermore typical etl system struggle manage significant data volume increase forcing business choose detailed data rapid performance cloud etlcloud etl also known modern etl take structured unstructured data data source type whether premise cloud combine transforms load data centralized location accessed demand across range use case within company cloud etl often used make data quickly available analyst developer decision maker etl pipeline v data pipeline phrase etl pipeline data pipeline sometimes used interchangeably describe vastly different thing combination procedure tool activity used ingest data range source transport target repository referred data pipeline within networked system might result added activity process flow transformed data saved database data warehouse via etl pipeline data may used business analytics insight etl extract transform load elt extract load transform two different data integration process use step different order help different data management function elt etl extract raw data different data source like enterprise resource planning erp platform social medium platform internet thing iot data spreadsheet elt raw data loaded directly target data warehouse data lake relational database data store allows data transformation happen needed also let load datasets source etl data extracted defined transformed improve data quality integrity later loaded data repository used creating data repository smaller need retained longer period need updated often etl way go dealing high volume datasets big data management real time elt would best use case parameter etl elt order process data transformed staging area loaded target system data extracted loaded target system directly transformation step managed target key focus loading database computing precious resource transforming data masking data normalizing joining table flight loading data warehouse mapping schema directly warehouse separating load transform execute transforms warehouse privacy compliance sensitive information redacted loading target system data uploaded raw form without sensitive detail removed masking must managed target system maintenance requirement transformation logic schema change management may need manual overhead maintenance addressed data warehouse transformation implemented latency generally higher latency transformation minimized streaming etl lower latency case little transformation data flexibility edge case managed custom rule logic maximize uptime generalized solution edge case around schema drift major resyncs lead downtime increased latency carefully planned analysis flexibility use case report model must defined beforehand data added time schema evolution analyst build new view target warehouse scale data bottlenecked etl scalable distributed processing system implicitly scalable le processing take place elt tool distinct type etl pipeline etl data pipeline categorized based latency common form etl pipeline employ either batch processing real time processing real time processing pipeline user ingest structured unstructured data variety streaming source including iot linked device social medium feed sensor data mobile application using real time data pipeline data accurately collected thanks high throughput messaging system drive application feature like real time analytics gps position tracking fraud detection predictive maintenance targeted marketing campaign initiative taking customer care data transformation overseen using real time processing engine like spark streaming batch processing pipeline batch processing used classic analytics business intelligence application data collected converted transferred cloud data warehouse regular basis minimal human intervention user may swiftly load high volume data siloed source cloud data lake data warehouse schedule job data processing efficiently manage massive amount data repetitive operation etl batch processing collect store data event known batch window building etl pipeline batch processing create typical etl process follow step move process data source database data warehouse batch building enterprise etl pipeline scratch difficult instead use etl solution like stitch blendo simplify automate much process reference data create data collection defines range value data set list permitted country code example country data field extract information data source correct extraction data foundation success future etl procedure etl system aggregate data variety source data organization format relational database non relational database csv file xml json successful extraction put data single format processed standardized manner validation data automated process verifies data retrieved source expected value example date field database financial transaction previous year hold valid date previous year data pas validation requirement rejected validation engine continuously examine rejected record see went wrong correct source data alter extraction fix problem next batch data transformation deleting superfluous incorrect data cleaning applying business rule guaranteeing data integrity ensuring data altered source etl data dropped earlier stage creating aggregate needed need examine revenue example combine dollar amount invoice daily monthly total need create test set rule function appropriate transformation run data gathered stage normally altered data loaded target data warehouse data entered staging database first make easier undo something go wrong also generate audit report regulatory compliance well find correct data error point upload data data warehouse data loaded target table etl pipeline load new batch certain data warehouse overwrite earlier data happen daily weekly monthly circumstance etl add new data without overwriting signaling new timestamp must go ahead caution avoid data warehouse exploding due disc space performance constraint using stream processing create etl pipeline real time data often used modern data operation web analytics data huge e commerce website extract transform data huge batch scenario need etl data stream mean data managed converted saved target datastore soon client application give data data source confluent describes etl pipeline based kafka diagram data extraction kafka confluent jdbc connector pull row source table put key value pair kafka topic without using confluent commercial product subject read application interested status table kafka writes new message kafka topic client application add row source table enabling real time data stream pulling data kafka topic etl programmed extract message kafka topic avro record build avro schema file deserializes message convert kstream object transform data kstream object stream processor get one record time process output one output record downstream processor using kafka stream api data operation many message aggregation transforming message one time filtering depending condition data must streamed target system data warehouse data lake etl application still hold enriched data confluent example suggests streaming data amazon via sink connector use amazon kinesis integrate system streaming data redshift data warehouse use case artificial intelligence ai machine learning ml etl etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement data democratization etl etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration etl pipeline automated time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl data processing allows greater business agility etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money challenge moving etl elt cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical benefit etl process etl integration offer several advantage including preserve resource etl lower amount data stored warehouse allowing business conserve storage bandwidth compute resource case storage cost concern le issue commoditized cloud computing engine improves compliance delivering data data warehouse etl mask remove sensitive data ip email address company follow data privacy protection standard gdpr hipaa ccpa masking dropping encrypting certain information well developed tool data extraction transformation loading etl around decade business use variety robust platform make creating keeping etl pipeline lot easier get thorough understanding company history etl provides historical context business merging legacy data data collected new platform application used enterprise data warehouse data rest migrating data cloud simple possible improve data accessibility application scalability security etl help move data cloud data lake cloud data warehouse ever business rely cloud integration boost operation allow business intelligence derived data latency today business must evaluate variety data type including structured semi structured unstructured data variety source including batch real time streaming etl solution make easier extract relevant insight data allowing spot new business possibility make better decision deliver data clean dependable decision making throughout data lifecycle use etl technology convert data keeping data lineage traceability mean regardless data demand data practitioner data scientist data analyst line business user access exact data drawback etl process company use etl also must deal several drawback legacy etl slow disk based staging transformation required traditional etl system frequent maintenance extraction transformation managed etl data pipeline however analyst need data type source system begin output data different format schema must refactored higher upfront cost scope data integration project expanded defining business logic transformation conclusion purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion related parameter etl elt order process data transformed staging area loaded target system data extracted loaded target system directly transformation step managed target key focus loading database computing precious resource transforming data masking data normalizing joining table flight loading data warehouse mapping schema directly warehouse separating load transform execute transforms warehouse privacy compliance sensitive information redacted loading target system data uploaded raw form without sensitive detail removed masking must managed target system maintenance requirement transformation logic schema change management may need manual overhead maintenance addressed data warehouse transformation implemented latency generally higher latency transformation minimized streaming etl lower latency case little transformation data flexibility edge case managed custom rule logic maximize uptime generalized solution edge case around schema drift major resyncs lead downtime increased latency carefully planned analysis flexibility use case report model must defined beforehand data added time schema evolution analyst build new view target warehouse scale data bottlenecked etl scalable distributed processing system implicitly scalable le processing take place elt tool distinct type etl pipeline etl data pipeline categorized based latency common form etl pipeline employ either batch processing real time processing real time processing pipeline user ingest structured unstructured data variety streaming source including iot linked device social medium feed sensor data mobile application using real time data pipeline data accurately collected thanks high throughput messaging system drive application feature like real time analytics gps position tracking fraud detection predictive maintenance targeted marketing campaign initiative taking customer care data transformation overseen using real time processing engine like spark streaming batch processing pipeline batch processing used classic analytics business intelligence application data collected converted transferred cloud data warehouse regular basis minimal human intervention user may swiftly load high volume data siloed source cloud data lake data warehouse schedule job data processing efficiently manage massive amount data repetitive operation etl batch processing collect store data event known batch window building etl pipeline batch processing create typical etl process follow step move process data source database data warehouse batch building enterprise etl pipeline scratch difficult instead use etl solution like stitch blendo simplify automate much process reference data create data collection defines range value data set list permitted country code example country data field extract information data source correct extraction data foundation success future etl procedure etl system aggregate data variety source data organization format relational database non relational database csv file xml json successful extraction put data single format processed standardized manner validation data automated process verifies data retrieved source expected value example date field database financial transaction previous year hold valid date previous year data pas validation requirement rejected validation engine continuously examine rejected record see went wrong correct source data alter extraction fix problem next batch data transformation deleting superfluous incorrect data cleaning applying business rule guaranteeing data integrity ensuring data altered source etl data dropped earlier stage creating aggregate needed need examine revenue example combine dollar amount invoice daily monthly total need create test set rule function appropriate transformation run data gathered stage normally altered data loaded target data warehouse data entered staging database first make easier undo something go wrong also generate audit report regulatory compliance well find correct data error point upload data data warehouse data loaded target table etl pipeline load new batch certain data warehouse overwrite earlier data happen daily weekly monthly circumstance etl add new data without overwriting signaling new timestamp must go ahead caution avoid data warehouse exploding due disc space performance constraint using stream processing create etl pipeline real time data often used modern data operation web analytics data huge e commerce website extract transform data huge batch scenario need etl data stream mean data managed converted saved target datastore soon client application give data data source confluent describes etl pipeline based kafka diagram data extraction kafka confluent jdbc connector pull row source table put key value pair kafka topic without using confluent commercial product subject read application interested status table kafka writes new message kafka topic client application add row source table enabling real time data stream pulling data kafka topic etl programmed extract message kafka topic avro record build avro schema file deserializes message convert kstream object transform data kstream object stream processor get one record time process output one output record downstream processor using kafka stream api data operation many message aggregation transforming message one time filtering depending condition data must streamed target system data warehouse data lake etl application still hold enriched data confluent example suggests streaming data amazon via sink connector use amazon kinesis integrate system streaming data redshift data warehouse use case artificial intelligence ai machine learning ml etl etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement data democratization etl etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration etl pipeline automated time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl data processing allows greater business agility etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money challenge moving etl elt cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical benefit etl process etl integration offer several advantage including preserve resource etl lower amount data stored warehouse allowing business conserve storage bandwidth compute resource case storage cost concern le issue commoditized cloud computing engine improves compliance delivering data data warehouse etl mask remove sensitive data ip email address company follow data privacy protection standard gdpr hipaa ccpa masking dropping encrypting certain information well developed tool data extraction transformation loading etl around decade business use variety robust platform make creating keeping etl pipeline lot easier get thorough understanding company history etl provides historical context business merging legacy data data collected new platform application used enterprise data warehouse data rest migrating data cloud simple possible improve data accessibility application scalability security etl help move data cloud data lake cloud data warehouse ever business rely cloud integration boost operation allow business intelligence derived data latency today business must evaluate variety data type including structured semi structured unstructured data variety source including batch real time streaming etl solution make easier extract relevant insight data allowing spot new business possibility make better decision deliver data clean dependable decision making throughout data lifecycle use etl technology convert data keeping data lineage traceability mean regardless data demand data practitioner data scientist data analyst line business user access exact data drawback etl process company use etl also must deal several drawback legacy etl slow disk based staging transformation required traditional etl system frequent maintenance extraction transformation managed etl data pipeline however analyst need data type source system begin output data different format schema must refactored higher upfront cost scope data integration project expanded defining business logic transformation conclusion purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion related distinct type etl pipeline etl data pipeline categorized based latency common form etl pipeline employ either batch processing real time processing real time processing pipeline user ingest structured unstructured data variety streaming source including iot linked device social medium feed sensor data mobile application using real time data pipeline data accurately collected thanks high throughput messaging system drive application feature like real time analytics gps position tracking fraud detection predictive maintenance targeted marketing campaign initiative taking customer care data transformation overseen using real time processing engine like spark streaming batch processing pipeline batch processing used classic analytics business intelligence application data collected converted transferred cloud data warehouse regular basis minimal human intervention user may swiftly load high volume data siloed source cloud data lake data warehouse schedule job data processing efficiently manage massive amount data repetitive operation etl batch processing collect store data event known batch window building etl pipeline batch processing create typical etl process follow step move process data source database data warehouse batch building enterprise etl pipeline scratch difficult instead use etl solution like stitch blendo simplify automate much process reference data create data collection defines range value data set list permitted country code example country data field extract information data source correct extraction data foundation success future etl procedure etl system aggregate data variety source data organization format relational database non relational database csv file xml json successful extraction put data single format processed standardized manner validation data automated process verifies data retrieved source expected value example date field database financial transaction previous year hold valid date previous year data pas validation requirement rejected validation engine continuously examine rejected record see went wrong correct source data alter extraction fix problem next batch data transformation deleting superfluous incorrect data cleaning applying business rule guaranteeing data integrity ensuring data altered source etl data dropped earlier stage creating aggregate needed need examine revenue example combine dollar amount invoice daily monthly total need create test set rule function appropriate transformation run data gathered stage normally altered data loaded target data warehouse data entered staging database first make easier undo something go wrong also generate audit report regulatory compliance well find correct data error point upload data data warehouse data loaded target table etl pipeline load new batch certain data warehouse overwrite earlier data happen daily weekly monthly circumstance etl add new data without overwriting signaling new timestamp must go ahead caution avoid data warehouse exploding due disc space performance constraint using stream processing create etl pipeline real time data often used modern data operation web analytics data huge e commerce website extract transform data huge batch scenario need etl data stream mean data managed converted saved target datastore soon client application give data data source confluent describes etl pipeline based kafka diagram data extraction kafka confluent jdbc connector pull row source table put key value pair kafka topic without using confluent commercial product subject read application interested status table kafka writes new message kafka topic client application add row source table enabling real time data stream pulling data kafka topic etl programmed extract message kafka topic avro record build avro schema file deserializes message convert kstream object transform data kstream object stream processor get one record time process output one output record downstream processor using kafka stream api data operation many message aggregation transforming message one time filtering depending condition data must streamed target system data warehouse data lake etl application still hold enriched data confluent example suggests streaming data amazon via sink connector use amazon kinesis integrate system streaming data redshift data warehouse use case artificial intelligence ai machine learning ml etl etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement data democratization etl etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration etl pipeline automated time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl data processing allows greater business agility etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money challenge moving etl elt cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical benefit etl process etl integration offer several advantage including preserve resource etl lower amount data stored warehouse allowing business conserve storage bandwidth compute resource case storage cost concern le issue commoditized cloud computing engine improves compliance delivering data data warehouse etl mask remove sensitive data ip email address company follow data privacy protection standard gdpr hipaa ccpa masking dropping encrypting certain information well developed tool data extraction transformation loading etl around decade business use variety robust platform make creating keeping etl pipeline lot easier get thorough understanding company history etl provides historical context business merging legacy data data collected new platform application used enterprise data warehouse data rest migrating data cloud simple possible improve data accessibility application scalability security etl help move data cloud data lake cloud data warehouse ever business rely cloud integration boost operation allow business intelligence derived data latency today business must evaluate variety data type including structured semi structured unstructured data variety source including batch real time streaming etl solution make easier extract relevant insight data allowing spot new business possibility make better decision deliver data clean dependable decision making throughout data lifecycle use etl technology convert data keeping data lineage traceability mean regardless data demand data practitioner data scientist data analyst line business user access exact data drawback etl process company use etl also must deal several drawback legacy etl slow disk based staging transformation required traditional etl system frequent maintenance extraction transformation managed etl data pipeline however analyst need data type source system begin output data different format schema must refactored higher upfront cost scope data integration project expanded defining business logic transformation conclusion purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion related etl data pipeline categorized based latency common form etl pipeline employ either batch processing real time processing user ingest structured unstructured data variety streaming source including iot linked device social medium feed sensor data mobile application using real time data pipeline data accurately collected thanks high throughput messaging system drive application feature like real time analytics gps position tracking fraud detection predictive maintenance targeted marketing campaign initiative taking customer care data transformation overseen using real time processing engine like spark streaming batch processing used classic analytics business intelligence application data collected converted transferred cloud data warehouse regular basis minimal human intervention user may swiftly load high volume data siloed source cloud data lake data warehouse schedule job data processing efficiently manage massive amount data repetitive operation etl batch processing collect store data event known batch window create typical etl process follow step move process data source database data warehouse batch building enterprise etl pipeline scratch difficult instead use etl solution like stitch blendo simplify automate much process create data collection defines range value data set list permitted country code example country data field correct extraction data foundation success future etl procedure etl system aggregate data variety source data organization format relational database non relational database csv file xml json successful extraction put data single format processed standardized manner automated process verifies data retrieved source expected value example date field database financial transaction previous year hold valid date previous year data pas validation requirement rejected validation engine continuously examine rejected record see went wrong correct source data alter extraction fix problem next batch deleting superfluous incorrect data cleaning applying business rule guaranteeing data integrity ensuring data altered source etl data dropped earlier stage creating aggregate needed need examine revenue example combine dollar amount invoice daily monthly total need create test set rule function appropriate transformation run data gathered normally altered data loaded target data warehouse data entered staging database first make easier undo something go wrong also generate audit report regulatory compliance well find correct data error point data loaded target table etl pipeline load new batch certain data warehouse overwrite earlier data happen daily weekly monthly circumstance etl add new data without overwriting signaling new timestamp must go ahead caution avoid data warehouse exploding due disc space performance constraint using stream processing create etl pipeline real time data often used modern data operation web analytics data huge e commerce website extract transform data huge batch scenario need etl data stream mean data managed converted saved target datastore soon client application give data data source confluent describes etl pipeline based kafka diagram data extraction kafka confluent jdbc connector pull row source table put key value pair kafka topic without using confluent commercial product subject read application interested status table kafka writes new message kafka topic client application add row source table enabling real time data stream pulling data kafka topic etl programmed extract message kafka topic avro record build avro schema file deserializes message convert kstream object transform data kstream object stream processor get one record time process output one output record downstream processor using kafka stream api data operation many message aggregation transforming message one time filtering depending condition data must streamed target system data warehouse data lake etl application still hold enriched data confluent example suggests streaming data amazon via sink connector use amazon kinesis integrate system streaming data redshift data warehouse use case artificial intelligence ai machine learning ml etl etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement data democratization etl etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration etl pipeline automated time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl data processing allows greater business agility etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money challenge moving etl elt cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical benefit etl process etl integration offer several advantage including preserve resource etl lower amount data stored warehouse allowing business conserve storage bandwidth compute resource case storage cost concern le issue commoditized cloud computing engine improves compliance delivering data data warehouse etl mask remove sensitive data ip email address company follow data privacy protection standard gdpr hipaa ccpa masking dropping encrypting certain information well developed tool data extraction transformation loading etl around decade business use variety robust platform make creating keeping etl pipeline lot easier get thorough understanding company history etl provides historical context business merging legacy data data collected new platform application used enterprise data warehouse data rest migrating data cloud simple possible improve data accessibility application scalability security etl help move data cloud data lake cloud data warehouse ever business rely cloud integration boost operation allow business intelligence derived data latency today business must evaluate variety data type including structured semi structured unstructured data variety source including batch real time streaming etl solution make easier extract relevant insight data allowing spot new business possibility make better decision deliver data clean dependable decision making throughout data lifecycle use etl technology convert data keeping data lineage traceability mean regardless data demand data practitioner data scientist data analyst line business user access exact data drawback etl process company use etl also must deal several drawback legacy etl slow disk based staging transformation required traditional etl system frequent maintenance extraction transformation managed etl data pipeline however analyst need data type source system begin output data different format schema must refactored higher upfront cost scope data integration project expanded defining business logic transformation conclusion purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion related real time data often used modern data operation web analytics data huge e commerce website extract transform data huge batch scenario need etl data stream mean data managed converted saved target datastore soon client application give data data source confluent describes etl pipeline based kafka diagram data extraction kafka confluent jdbc connector pull row source table put key value pair kafka topic without using confluent commercial product subject read application interested status table kafka writes new message kafka topic client application add row source table enabling real time data stream pulling data kafka topic etl programmed extract message kafka topic avro record build avro schema file deserializes message convert kstream object transform data kstream object stream processor get one record time process output one output record downstream processor using kafka stream api data operation many message aggregation transforming message one time filtering depending condition data must streamed target system data warehouse data lake etl application still hold enriched data confluent example suggests streaming data amazon via sink connector use amazon kinesis integrate system streaming data redshift data warehouse use case artificial intelligence ai machine learning ml etl etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement data democratization etl etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration etl pipeline automated time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl data processing allows greater business agility etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money challenge moving etl elt cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical benefit etl process etl integration offer several advantage including preserve resource etl lower amount data stored warehouse allowing business conserve storage bandwidth compute resource case storage cost concern le issue commoditized cloud computing engine improves compliance delivering data data warehouse etl mask remove sensitive data ip email address company follow data privacy protection standard gdpr hipaa ccpa masking dropping encrypting certain information well developed tool data extraction transformation loading etl around decade business use variety robust platform make creating keeping etl pipeline lot easier get thorough understanding company history etl provides historical context business merging legacy data data collected new platform application used enterprise data warehouse data rest migrating data cloud simple possible improve data accessibility application scalability security etl help move data cloud data lake cloud data warehouse ever business rely cloud integration boost operation allow business intelligence derived data latency today business must evaluate variety data type including structured semi structured unstructured data variety source including batch real time streaming etl solution make easier extract relevant insight data allowing spot new business possibility make better decision deliver data clean dependable decision making throughout data lifecycle use etl technology convert data keeping data lineage traceability mean regardless data demand data practitioner data scientist data analyst line business user access exact data drawback etl process company use etl also must deal several drawback legacy etl slow disk based staging transformation required traditional etl system frequent maintenance extraction transformation managed etl data pipeline however analyst need data type source system begin output data different format schema must refactored higher upfront cost scope data integration project expanded defining business logic transformation conclusion purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion related etl powered ai machine learning automates key data operation ensuring data receive analysis match quality standard needed give trusted insight decision making used conjunction data quality tool ensure data output fit specific requirement etl required technical folk business user must also able find data quickly integrate system service application simple achieve including ai etl process design run time etl tool using ai machine learning may learn past data recommend reusable component business user case data mapping transformation configuration time saving automation onerous recurrent data engineering chore possible ai based etl technology improve efficiency data administration data distribution ingest process integrate enrich prepare map define catalog data automatically etl decrease effort needed gather prepare integrate data team able move swiftly etl automation powered ai boost productivity allowing data professional get data need need without develop code script saving time money cloud data warehouse data lake greater processing capability changed way data transformed many firm shifted etl elt transformation usually simple adjustment etl mapping matured point manage wide range data kind source frequency format convert mapping elt friendly format need enterprise data platform handle data provide pushdown optimization without damaging front end developer often wind hand coding query add complex transformation platform create ecosystem data warehouse specific code required time consuming procedure costly difficult inconvenient choosing platform user friendly interface ability replicate mapping run elt pattern critical etl integration offer several advantage including company use etl also must deal several drawback purpose guide provide introduction etl pipeline cover different aspect like introduction key difference different type pipeline case study area application building etl pipeline data shifted one place another various operator answer query systematically correctly rather searching diverse data source data management could improved well structured etl pipeline additionally allow data manager quickly iterate meet ever changing data requirement business medium shown article owned analytics vidhya used author discretion
85,https://www.analyticsvidhya.com/blog/2022/06/learn-swift-for-data-science-with-particle-example/,article published part data science blogathon python widely considered best effective language data science poll survey come across recent year peg python market leader space thing data science vast ever evolving field language use build data science model evolve remember r go language swiftly overtaken python julia also came last year data science another language blossoming yes talking swift data science always hope start looking new language mind opening new idea find swift definitely disappoint swift try expressive flexible concise safe easy use fast language compromise significantly least one area jeremy howard endorses language start using daily data science work need drop everything listen article learn swift programming language fit data science space python user notice subtle difference incredible similarity two lot code well let get started pytorch created overcome gap tensorflow fastai built fill gap tooling pytorch hitting limit python swift potential bridge gap lot excitement attention recently towards swift language data science everyone talking reason learn swift jeremy howard articulating good swift isbefore start nitty gritty detail performing data science using swift let get brief introduction basic swift programming language swift ecosystemthe current state swift data science primarily made two ecosystem open source ecosystem one download run swift operating system machine build machine learning application using really cool swift library like swift tensorflow swiftai swiftplot swift also let u seamlessly import mature data science library python like numpy panda matplotlib scikit learn hesitation switching swift python well covered apple ecosystem hand impressive right useful library let u train large model python directly import swift inferencing additionally also come plethora pre trained state art model use build io macos application http course analyticsvidhya com course take learn swift data science interesting library like swift coreml transformer let u run state art text generation model like gpt bert etc iphone http course analyticsvidhya com course take learn swift data science multiple library give good level functionality need build machine learning based application apple device multiple difference two ecosystem important one order use apple ecosystem need apple machine work build apple device like io macos etc overview swift language data science let get code setting environment swift available use google colab gpu tpu version using quickly get speed without spending much time installation process follow step open colab notebook swift enabled open blank swift notebook click file select save copy drive save fresh swift notebook google drive set start working swift write first line code sweet want work swift locally system link follow want install swift local system follow installation installing jupyter notebook ubuntu ubuntu also install swift docker let quickly cover basic swift function jumping data science aspect using basic swift programming let quickly cover basic swift programming concept jumping data science aspect using print function sure already used work way python simply call print whatever want print inside parenthesis variable swift swift provides two useful option create variable let var let used create constant variable whose value can not change anywhere program var similar variable see python change value stored anytime program let look example see difference create two variable b let analytics var b vidhya try changing value b b av av notice b able update value without issue give error ability create constant variable useful help u prevent unseen bug code see article use let create variable store important information want write even mistake code pro tip use var temporary variable variable want use intermediate calculation similarly use thing like storing training data result etc basically value want change mess also cool feature swift even use emojis variable name swift support unicode well create variable greek letter var pi swift datatypes swift support common data type like integer string float double assign variable value type automatically detected swift let mark let percentage var name sushil also explicitly mention data type creating variable help prevent error program swift throw error type match let weight double let quick quiz create constant explicit type float value post solution comment simple way include value string write value parenthesis write backslash parenthesis example use three double quotation mark string take multiple line writing comment code writing comment one important aspect good code true across industry role work important programming aspect learn use comment include text code note reminder comment ignored swift single line comment begin two forward slash comment multiline comment start forward slash followed asterisk end asterisk followed forward slash also comment written multiple line list dictionary swift support list dictionary data structure like python comparison though advantage unlike python need separate syntax like dictionary list let create list dictionary swift var shoppinglist catfish water tulip blue paint shoppinglist bottle water var occupationsdict malcolm captain kaylee mechanic access element list dictionary writing index key inside bracket similar python occupationsdict jayne public relation print occupationsdict code add key value pair jayne public relation dictionary output print dictionary source author list dictionary swift list var shoppinglist catfish water tulip blue paint shoppinglist bottle water print list shoppinglist dictionary var occupationsdict malcolm captain kaylee mechanic occupationsdict jayne public relation print dictionary occupationsdict basic swift programming ii working loop looping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loop similar python use source author three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related apple ecosystem hand impressive right useful library let u train large model python directly import swift inferencing additionally also come plethora pre trained state art model use build io macos application interesting library like swift coreml transformer let u run state art text generation model like gpt bert etc iphone multiple library give good level functionality need build machine learning based application apple device multiple difference two ecosystem important one order use apple ecosystem need apple machine work build apple device like io macos etc overview swift language data science let get code setting environment swift available use google colab gpu tpu version using quickly get speed without spending much time installation process follow step open colab notebook swift enabled open blank swift notebook click file select save copy drive save fresh swift notebook google drive set start working swift write first line code sweet want work swift locally system link follow want install swift local system follow installation installing jupyter notebook ubuntu ubuntu also install swift docker let quickly cover basic swift function jumping data science aspect using basic swift programming let quickly cover basic swift programming concept jumping data science aspect using print function sure already used work way python simply call print whatever want print inside parenthesis variable swift swift provides two useful option create variable let var let used create constant variable whose value can not change anywhere program var similar variable see python change value stored anytime program let look example see difference create two variable b let analytics var b vidhya try changing value b b av av notice b able update value without issue give error ability create constant variable useful help u prevent unseen bug code see article use let create variable store important information want write even mistake code pro tip use var temporary variable variable want use intermediate calculation similarly use thing like storing training data result etc basically value want change mess also cool feature swift even use emojis variable name swift support unicode well create variable greek letter var pi swift datatypes swift support common data type like integer string float double assign variable value type automatically detected swift let mark let percentage var name sushil also explicitly mention data type creating variable help prevent error program swift throw error type match let weight double let quick quiz create constant explicit type float value post solution comment simple way include value string write value parenthesis write backslash parenthesis example use three double quotation mark string take multiple line writing comment code writing comment one important aspect good code true across industry role work important programming aspect learn use comment include text code note reminder comment ignored swift single line comment begin two forward slash comment multiline comment start forward slash followed asterisk end asterisk followed forward slash also comment written multiple line list dictionary swift support list dictionary data structure like python comparison though advantage unlike python need separate syntax like dictionary list let create list dictionary swift var shoppinglist catfish water tulip blue paint shoppinglist bottle water var occupationsdict malcolm captain kaylee mechanic access element list dictionary writing index key inside bracket similar python occupationsdict jayne public relation print occupationsdict code add key value pair jayne public relation dictionary output print dictionary source author list dictionary swift list var shoppinglist catfish water tulip blue paint shoppinglist bottle water print list shoppinglist dictionary var occupationsdict malcolm captain kaylee mechanic occupationsdict jayne public relation print dictionary occupationsdict basic swift programming ii working loop looping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loop similar python use source author three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related multiple library give good level functionality need build machine learning based application apple device multiple difference two ecosystem important one order use apple ecosystem need apple machine work build apple device like io macos etc overview swift language data science let get code swift available use google colab gpu tpu version using quickly get speed without spending much time installation process follow step open colab notebook swift enabled open blank swift notebook click file select save copy drive save fresh swift notebook google drive set start working swift write first line code sweet want work swift locally system link follow want install swift local system follow installation installing jupyter notebook ubuntu ubuntu also install swift docker let quickly cover basic swift function jumping data science aspect using basic swift programming let quickly cover basic swift programming concept jumping data science aspect using print function sure already used work way python simply call print whatever want print inside parenthesis variable swift swift provides two useful option create variable let var let used create constant variable whose value can not change anywhere program var similar variable see python change value stored anytime program let look example see difference create two variable b let analytics var b vidhya try changing value b b av av notice b able update value without issue give error ability create constant variable useful help u prevent unseen bug code see article use let create variable store important information want write even mistake code pro tip use var temporary variable variable want use intermediate calculation similarly use thing like storing training data result etc basically value want change mess also cool feature swift even use emojis variable name swift support unicode well create variable greek letter var pi swift datatypes swift support common data type like integer string float double assign variable value type automatically detected swift let mark let percentage var name sushil also explicitly mention data type creating variable help prevent error program swift throw error type match let weight double let quick quiz create constant explicit type float value post solution comment simple way include value string write value parenthesis write backslash parenthesis example use three double quotation mark string take multiple line writing comment code writing comment one important aspect good code true across industry role work important programming aspect learn use comment include text code note reminder comment ignored swift single line comment begin two forward slash comment multiline comment start forward slash followed asterisk end asterisk followed forward slash also comment written multiple line list dictionary swift support list dictionary data structure like python comparison though advantage unlike python need separate syntax like dictionary list let create list dictionary swift var shoppinglist catfish water tulip blue paint shoppinglist bottle water var occupationsdict malcolm captain kaylee mechanic access element list dictionary writing index key inside bracket similar python occupationsdict jayne public relation print occupationsdict code add key value pair jayne public relation dictionary output print dictionary source author list dictionary swift list var shoppinglist catfish water tulip blue paint shoppinglist bottle water print list shoppinglist dictionary var occupationsdict malcolm captain kaylee mechanic occupationsdict jayne public relation print dictionary occupationsdict basic swift programming ii working loop looping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loop similar python use source author three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related follow step open colab notebook swift enabled open blank swift notebook click file select save copy drive save fresh swift notebook google drive set start working swift write first line code sweet want work swift locally system link follow let quickly cover basic swift function jumping data science aspect using let quickly cover basic swift programming concept jumping data science aspect using print functionwe sure already used work way python simply call print whatever want print inside parenthesis variable swiftswift provides two useful option create variable let var let used create constant variable whose value can not change anywhere program var similar variable see python change value stored anytime program let look example see difference create two variable b try changing value b notice b able update value without issue give error ability create constant variable useful help u prevent unseen bug code see article use let create variable store important information want write even mistake code also cool feature swift even use emojis variable name swift support unicode well create variable greek letter swift support common data type like integer string float double assign variable value type automatically detected swift also explicitly mention data type creating variable help prevent error program swift throw error type match let quick quiz create constant explicit type float value post solution comment simple way include value string write value parenthesis write backslash parenthesis example use three double quotation mark string take multiple line writing comment code writing comment one important aspect good code true across industry role work important programming aspect learn use comment include text code note reminder comment ignored swift single line comment begin two forward slash comment multiline comment start forward slash followed asterisk end asterisk followed forward slash also comment written multiple line list dictionary swift support list dictionary data structure like python comparison though advantage unlike python need separate syntax like dictionary list let create list dictionary swift var shoppinglist catfish water tulip blue paint shoppinglist bottle water var occupationsdict malcolm captain kaylee mechanic access element list dictionary writing index key inside bracket similar python occupationsdict jayne public relation print occupationsdict code add key value pair jayne public relation dictionary output print dictionary source author list dictionary swift list var shoppinglist catfish water tulip blue paint shoppinglist bottle water print list shoppinglist dictionary var occupationsdict malcolm captain kaylee mechanic occupationsdict jayne public relation print dictionary occupationsdict basic swift programming ii working loop looping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loop similar python use source author three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related use three double quotation mark string take multiple line writing comment one important aspect good code true across industry role work important programming aspect learn use comment include text code note reminder comment ignored swift single line comment begin two forward slash multiline comment start forward slash followed asterisk end asterisk followed forward slash swift support list dictionary data structure like python comparison though advantage unlike python need separate syntax like dictionary list let create list dictionary swift access element list dictionary writing index key inside bracket similar python code add key value pair jayne public relation dictionary output print dictionary basic swift programming ii working loop looping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loop similar python use source author three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related working loopslooping one important feature programming language swift disappoint support conventional looping mechanism etc also implement variation loopvery similar python use three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression statement execute boolean expression true boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals function swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift python swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly using python library swift give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use import python load numpy python let np python import numpy create array zero var zero np one print zero quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model machine learning swift tensorflow swifttensorflow library swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setup let understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know setup project start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library include enableipythondisplay swift ipythondisplay shell enable_matplotlib inline import foundation import python let o python import o let plt python import matplotlib pyplot o system git clone http github com mohdsanadzakirizvi swift datascience git o chdir content swift datascience implementation mnist image classification loading dataset include content swift datascience mnist swift load dataset let dataset mnist batchsize get first image let imgs dataset trainingimages minibatch batchsize makenumpyarray print imgs shape exploring mnist digit plot image dataset get idea working display first image img imgs plt imshow img reshape plt show image look like seems pretty intuitive right first digit handwritten second one defining structure model let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit import tensorflow let epochcount let batchsize lenet model var classifier sequential convd float filtershape padding activation relu avgpoold float poolsize stride convd float filtershape activation relu avgpoold float poolsize stride flatten float dense float inputsize outputsize activation relu dense float inputsize outputsize activation relu dense float inputsize outputsize activation softmax would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box choosing gradient descent optimizer similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow using gradient descent optimizer let optimizer sgd classifier learningrate swifttensorflow support many additional optimizers choose pick based project amsgrad adadelta adagrad adamax adam parameter rmsprop sgd model training everything set let train model print beginning training struct statistic var correctguesscount int var totalguesscount int var totalloss float store accuracy result training var trainaccuracyresults float var testaccuracyresults float training loop epoch epochcount var trainstats statistic var teststats statistic set context training context local learningphase training tensor let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis trainstats correctguesscount int tensor correctpredictions sum scalarized trainstats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label trainstats totalloss loss scalarized return loss update model differentiable variable along gradient vector optimizer update classifier along model set context inference context local learningphase inference dataset testsize batchsize let x dataset testimages minibatch batchsize batchsize let dataset testlabels minibatch batchsize batchsize compute loss test set let ŷ classifier x let correctpredictions ŷ argmax squeezingaxis teststats correctguesscount int tensor correctpredictions sum scalarized teststats totalguesscount batchsize let loss softmaxcrossentropy logits ŷ label teststats totalloss loss scalarized let trainaccuracy float trainstats correctguesscount float trainstats totalguesscount let testaccuracy float teststats correctguesscount float teststats totalguesscount save train test accuracy trainaccuracyresults append trainaccuracy testaccuracyresults append testaccuracy print epoch epoch training loss trainstats totalloss training accuracy trainstats correctguesscount trainstats totalguesscount trainaccuracy test loss teststats totalloss test accuracy teststats correctguesscount teststats totalguesscount testaccuracy code run training loop feed dataset example model help make better prediction training step follow iterate epoch epoch one pas entire dataset within epoch iterate example training dataset grabbing feature x label important next step using example feature make prediction compare ground truth label give u inaccuracy prediction used calculate model loss gradient gradient descent come picture update model variable keep track training stats visualizing later repeat step epoch epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch visualizing training test stats though helpful print model training progress often helpful see progress let visualize train test stats captured training model plt figure figsize let accuracyaxes plt subplot accuracyaxes set_ylabel train accuracy accuracyaxes plot trainaccuracyresults color blue let lossaxes plt subplot lossaxes set_ylabel test accuracy lossaxes set_xlabel epoch lossaxes plot testaccuracyresults color yellow plt show train test accuracy evolved training process conclusion way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion related three dot first example used denote range swift want something range b use syntax b similarly want exclude last number change three dot like b try playing around see many time get right another important point note unlike python swift use concept indentation us curly bracket denote code hierarchy use type loop similar fashion swift conditionals else swift support conditional statement like else else nested even switch statement python support syntax statement quite simple boolean_expression comparison statement write inside block executed result comparison expression evaluates true read conditionals swift function look syntactically similar function python major difference use func keyword instead def explicitly mention data type argument return type function write basic function swift like conditionals use curly bracket denote code block belongs function familiar basic swift let learn interesting feature using python library swift swift support interoperability python mean import useful python library swift call function convert value swift python seamlessly give incredible power swift data science ecosystem ecosystem still pretty young still developing already use mature library like numpy panda matplotlib python filling gap existing swift offering order use python module swift import python right away load whatever library want use quite similar way use numpy python package like matplotlib learned quite bit swift already time build first model swifttensorflow one mature library open source ecosystem swift quickly build machine learning deep learning model using simple kera like syntax native swift get even interesting swifttensorflow swift wrapper around tensorflow developed feature language widely expected become core part language near future mean amazing set engineer apple swift team google tensorflow team make sure able high performance machine learning swift library also add many useful feature swift like native support automatic differentiation would remind autograd pytorch make even compatible numeric computing use case dataset setuplet understand problem statement working within section might familiar touched deep learning field building convolutional neural network cnn model classify image digit using mnist dataset dataset contains training image testing image handwritten digit use training image classification model dataset fairly common dataset working computer vision problem describe great detail want know start building model need download dataset pre process convenience already created github repository pre processing code data let download setup code download dataset import necessary library exploring mnist digitswe plot image dataset get idea working image look like seems pretty intuitive right first digit handwritten second one let define architecture model using lenet architecture fairly basic cnn model using convolution layer average pooling dense layer last dense layer shape target class one digit would noticed code look familiar write code create model python framework like kera pytorch tensorflow simplicity writing code one biggest selling point swift swifttensorflow support multiple layer type right box similarly need optimizer function train model going use stochastic gradient descent sgd available swifttensorflow swifttensorflow support many additional optimizers choose pick based project everything set let train model code run training loop feed dataset example model help make better prediction training step follow epoch count variable number time loop dataset collection go ahead give try many epoch take achieve accuracy test set able get accuracy train test set epoch though helpful print model training progress often helpful see progress let visualize train test stats captured training model train test accuracy evolved training process way industry expert reacting swift mind boggling feel like language potential become one mainstream language data science also language going used building application based machine learning real world currently infancy library around data science numeric computing still developing yet strong industry backing behind look forward future rich ecosystem tool library maybe even better python today medium shown article owned analytics vidhya used author discretion
86,https://www.analyticsvidhya.com/blog/2022/06/a-step-by-step-on-tableau-for-beginner/,article published part data science blogathon hi welcome article visualization become necessary skill eas process communicating people outside domain person struggling get right platform start learning visualization course right place getting started see screen similar one import data visible multiple format data flat file excel csv directly load data server see tableau offer sample workbook pre drawn chart graph would suggest going later exploration best way learn get hand dirty let u start data found data united state superstore deliberating expansion wish know prospective region country could hence requires help first thing obviously need import data onto tableau quickly follow step since data excel file click excel choose sample superstore xl file get source author see three sheet screen going dealing order go ahead drag drag sheet imported data look bit different first row worry solution lie right ahead data interpreter see option use data interpreter click get following clean view messy data magically disappeared open excel data file see metadata e information data tableau import entire data file anticipating discrepancy explicitly provides solution form data interpreter wish view exact change made click review result choose order tab opened excel sheet show simply removed erroneous data data visualization soon imported dataset next data source tab near bottom screen immediately must seen go worksheet worksheet make graph click tab reach following screen get overwhelmed various element see cover one one let start dimension measure visualization tableau possible dragging dropping measure dimension onto different shelf row column represent x axis graph chart filter filter help view strained version data example instead seeing combined sale category look specific one furniture page page work principle filter difference actually see change shift paged value remember rosling chart easily make one using page mark mark property used control mark type data may choose represent data using different shape size text drag drop field onto visualization area tableau make default graph shall see soon change referring show option note every graph made combination dimension measure graph condition number type field used shall discus next different type chart tableau net statistic far pretty much covered requisite theoretical knowledge let finally begin visualization prefer start shallow side pool slowly swimming towards deeper end would suggest beginning getting overview superstore sale profit statistic would include net sale net profit growth two measure name gist making observed net sale rise profit creeping slowly also quite clearly see peak sale month could attributed various reason know explore start one thing would like recommend name worksheet done since referencing back forth throughout article easier follow let begin simplest visualization displaying net statistic number tableau smart automatically computes value measure name measure value follow step make called text table drag measure name dimension onto central empty area see text table measure name displayed automatically onto row drag row column since really need measure like row id discount etc drag mark pane get something like note get confused different colour field see remember one small trick blue mean discrete green continous net statistic part net sale profit value let delve little deeper getting sale profit value year let make another detailed text table drag order date dimension sale measure row right click green sale pill select discrete place continuous since want explicit value bar graph finally drag profit abc column get thing monthly sale profit value time change format order date year month right clicking order date row choosing month get something like line chart covered numeric part dashboard selling point line chart let quickly learn make one create chart sale profit growth drag order date column sale row profit formed sale axis see equal sign get following source author repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show pie chart previous visualization gotten brief overview superstore let dig little deeper next thing think exploring demographic sale profit state highest sale revenue one generating maximum profit source author discussing inference let first create pie chart region sale drag region onto row sale onto column go show select pie chart finally drag sale label mark pane get visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion related see screen similar one import data visible multiple format data flat file excel csv directly load data server see tableau offer sample workbook pre drawn chart graph would suggest going later exploration best way learn get hand dirty let u start data found data united state superstore deliberating expansion wish know prospective region country could hence requires help first thing obviously need import data onto tableau quickly follow step since data excel file click excel choose sample superstore xl file get see three sheet screen going dealing order go ahead drag drag sheet imported data look bit different first row worry solution lie right ahead data interpreter see option use data interpreter click get following clean view messy data magically disappeared open excel data file see metadata e information data tableau import entire data file anticipating discrepancy explicitly provides solution form data interpreter wish view exact change made click review result choose order tab opened excel sheet show simply removed erroneous data data visualization soon imported dataset next data source tab near bottom screen immediately must seen go worksheet worksheet make graph click tab reach following screen get overwhelmed various element see cover one one let start dimension measure visualization tableau possible dragging dropping measure dimension onto different shelf row column represent x axis graph chart filter filter help view strained version data example instead seeing combined sale category look specific one furniture page page work principle filter difference actually see change shift paged value remember rosling chart easily make one using page mark mark property used control mark type data may choose represent data using different shape size text drag drop field onto visualization area tableau make default graph shall see soon change referring show option note every graph made combination dimension measure graph condition number type field used shall discus next different type chart tableau net statistic far pretty much covered requisite theoretical knowledge let finally begin visualization prefer start shallow side pool slowly swimming towards deeper end would suggest beginning getting overview superstore sale profit statistic would include net sale net profit growth two measure name gist making observed net sale rise profit creeping slowly also quite clearly see peak sale month could attributed various reason know explore start one thing would like recommend name worksheet done since referencing back forth throughout article easier follow let begin simplest visualization displaying net statistic number tableau smart automatically computes value measure name measure value follow step make called text table drag measure name dimension onto central empty area see text table measure name displayed automatically onto row drag row column since really need measure like row id discount etc drag mark pane get something like note get confused different colour field see remember one small trick blue mean discrete green continous net statistic part net sale profit value let delve little deeper getting sale profit value year let make another detailed text table drag order date dimension sale measure row right click green sale pill select discrete place continuous since want explicit value bar graph finally drag profit abc column get thing monthly sale profit value time change format order date year month right clicking order date row choosing month get something like line chart covered numeric part dashboard selling point line chart let quickly learn make one create chart sale profit growth drag order date column sale row profit formed sale axis see equal sign get following source author repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show pie chart previous visualization gotten brief overview superstore let dig little deeper next thing think exploring demographic sale profit state highest sale revenue one generating maximum profit source author discussing inference let first create pie chart region sale drag region onto row sale onto column go show select pie chart finally drag sale label mark pane get visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion related see three sheet screen going dealing order go ahead drag drag sheet imported data look bit different first row worry solution lie right ahead see option use data interpreter click get following clean view messy data magically disappeared open excel data file see metadata e information data tableau import entire data file anticipating discrepancy explicitly provides solution form data interpreter wish view exact change made click review result choose order tab opened excel sheet show simply removed erroneous data data visualization soon imported dataset next data source tab near bottom screen immediately must seen go worksheet worksheet make graph click tab reach following screen get overwhelmed various element see cover one one let start dimension measure visualization tableau possible dragging dropping measure dimension onto different shelf row column represent x axis graph chart filter filter help view strained version data example instead seeing combined sale category look specific one furniture page page work principle filter difference actually see change shift paged value remember rosling chart easily make one using page mark mark property used control mark type data may choose represent data using different shape size text drag drop field onto visualization area tableau make default graph shall see soon change referring show option note every graph made combination dimension measure graph condition number type field used shall discus next different type chart tableau net statistic far pretty much covered requisite theoretical knowledge let finally begin visualization prefer start shallow side pool slowly swimming towards deeper end would suggest beginning getting overview superstore sale profit statistic would include net sale net profit growth two measure name gist making observed net sale rise profit creeping slowly also quite clearly see peak sale month could attributed various reason know explore start one thing would like recommend name worksheet done since referencing back forth throughout article easier follow let begin simplest visualization displaying net statistic number tableau smart automatically computes value measure name measure value follow step make called text table drag measure name dimension onto central empty area see text table measure name displayed automatically onto row drag row column since really need measure like row id discount etc drag mark pane get something like note get confused different colour field see remember one small trick blue mean discrete green continous net statistic part net sale profit value let delve little deeper getting sale profit value year let make another detailed text table drag order date dimension sale measure row right click green sale pill select discrete place continuous since want explicit value bar graph finally drag profit abc column get thing monthly sale profit value time change format order date year month right clicking order date row choosing month get something like line chart covered numeric part dashboard selling point line chart let quickly learn make one create chart sale profit growth drag order date column sale row profit formed sale axis see equal sign get following source author repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show pie chart previous visualization gotten brief overview superstore let dig little deeper next thing think exploring demographic sale profit state highest sale revenue one generating maximum profit source author discussing inference let first create pie chart region sale drag region onto row sale onto column go show select pie chart finally drag sale label mark pane get visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion related messy data magically disappeared open excel data file see metadata e information data tableau import entire data file anticipating discrepancy explicitly provides solution form data interpreter wish view exact change made click review result choose order tab opened excel sheet show simply removed erroneous data data visualization soon imported dataset next data source tab near bottom screen immediately must seen go worksheet worksheet make graph click tab reach following screen get overwhelmed various element see cover one one let start dimension measure visualization tableau possible dragging dropping measure dimension onto different shelf row column represent x axis graph chart filter filter help view strained version data example instead seeing combined sale category look specific one furniture page page work principle filter difference actually see change shift paged value remember rosling chart easily make one using page mark mark property used control mark type data may choose represent data using different shape size text drag drop field onto visualization area tableau make default graph shall see soon change referring show option note every graph made combination dimension measure graph condition number type field used shall discus next different type chart tableau net statistic far pretty much covered requisite theoretical knowledge let finally begin visualization prefer start shallow side pool slowly swimming towards deeper end would suggest beginning getting overview superstore sale profit statistic would include net sale net profit growth two measure name gist making observed net sale rise profit creeping slowly also quite clearly see peak sale month could attributed various reason know explore start one thing would like recommend name worksheet done since referencing back forth throughout article easier follow let begin simplest visualization displaying net statistic number tableau smart automatically computes value measure name measure value follow step make called text table drag measure name dimension onto central empty area see text table measure name displayed automatically onto row drag row column since really need measure like row id discount etc drag mark pane get something like note get confused different colour field see remember one small trick blue mean discrete green continous net statistic part net sale profit value let delve little deeper getting sale profit value year let make another detailed text table drag order date dimension sale measure row right click green sale pill select discrete place continuous since want explicit value bar graph finally drag profit abc column get thing monthly sale profit value time change format order date year month right clicking order date row choosing month get something like line chart covered numeric part dashboard selling point line chart let quickly learn make one create chart sale profit growth drag order date column sale row profit formed sale axis see equal sign get following source author repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show pie chart previous visualization gotten brief overview superstore let dig little deeper next thing think exploring demographic sale profit state highest sale revenue one generating maximum profit source author discussing inference let first create pie chart region sale drag region onto row sale onto column go show select pie chart finally drag sale label mark pane get visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion related tableau import entire data file anticipating discrepancy explicitly provides solution form data interpreter wish view exact change made click review result choose order tab opened excel sheet show simply removed erroneous data soon imported dataset next data source tab near bottom screen immediately must seen go worksheet worksheet make graph click tab reach following screen get overwhelmed various element see cover one one let start dimension measure visualization tableau possible dragging dropping measure dimension onto different shelf drag drop field onto visualization area tableau make default graph shall see soon change referring show option note every graph made combination dimension measure graph condition number type field used shall discus next far pretty much covered requisite theoretical knowledge let finally begin visualization prefer start shallow side pool slowly swimming towards deeper end would suggest beginning getting overview superstore sale profit statistic would include net sale net profit growth two measure name gist making observed net sale rise profit creeping slowly also quite clearly see peak sale month could attributed various reason know explore start one thing would like recommend name worksheet done since referencing back forth throughout article easier follow let begin simplest visualization displaying net statistic number tableau smart automatically computes value measure name measure value follow step make called text table note get confused different colour field see remember one small trick blue mean discrete green continous net sale profit value let delve little deeper getting sale profit value year let make another detailed text table drag order date dimension sale measure rowsright click green sale pill select discrete place continuous since want explicit value bar graph finally drag profit abc column get thing monthly sale profit value time change format order date year month right clicking order date row choosing month get something like covered numeric part dashboard selling point line chart let quickly learn make one create chart sale profit growth drag order date column sale row profit formed sale axis see equal sign get following repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show pie chart repeat find peak sale profit month change format order date year month get click show see different type line chart make hover get see dimension measure requirement case ever feel lost recommend referring show previous visualization gotten brief overview superstore let dig little deeper next thing think exploring demographic sale profit state highest sale revenue one generating maximum profit discussing inference let first create pie chart region sale drag region onto row sale onto column go show select pie chart finally drag sale label mark pane get visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion discussing inference let first create pie chart region sale visual pretty evident two opposite end east west leading sale game let dissect bit map chart whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well scatter plot lastly step making scatter plot sale profit analysis drag sale onto row profit onto column see one tiny circle actually represents total sale profit value get information drag state onto graph created circle bubble scatter represent individual state better understand central tendency data also added median axis reference line easily done right clicking sale profit axis adding reference line choosing median default average reference finally insight drag state time onto label mark pane get finding map chart become prominent following scatter plot inference state top right high sale high profit mean good business organisation state positive sale profit near two respective axis one scope improvement whereas state belong nd rd quarter one generating much revenue functionality tableau filter till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year source author choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion visual pretty evident two opposite end east west leading sale game let dissect bit whenever geographical data always advisable plot see map gain better insight going make map chart state sale distribution since state wish analyze drag state onto empty area automatically see map small circle follow step dragging profit next notice size circle changing represent varying value profit called symbol map going convert filled one going show selecting filled map drag profit time onto label mark pane view profit value mapped well like california new york top seller west east region unfortunately state texas colorado even good sale negative profit certainly good news superstore perceive good analysis state well lastly step making scatter plot sale profit analysis finding map chart become prominent following scatter plot inference till made simple chart actually provide cumulative data combined data lifetime superstore look sale particular year month certain product basically view distinct aspect data filter way go let head back first ever chart made peak sale profit month visual accumulation year data region state category sub category step turning dimension filter let first experiment order date formatted year drag dimension filter shelf see following pop choosing year choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet trend line traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get conclusion completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion choose value want part filter right click newly generated filter choose show filter also change format filter example whether wish dropdown list slider single value list etc feel filter applied sheet well rather repeating step simply apply filter relevant worksheet traverse back peak sale profit month chart follow step make trend line go show choose dual combination chart get chart get trend line go analytics simply drag trend line chart get completed article learn new concept tableau hope provided good insight important visualization leverage tableau beautifully communicate analysis want learn advanced concept tableau check course http course analyticsvidhya com course tableau thanks reading hope enjoyed article like share friend also please feel free comment thought improve article writing medium shown article owned analytics vidhya used author discretion
87,https://www.analyticsvidhya.com/blog/2022/06/how-ml-with-titanic-dataset-could-be-misleading/,article published part data science blogathon titanic ship disaster one infamous shipwreck luxury cruiser touted one safest launched sank thousand passenger due accident iceberg passenger passenger died due shipwreck accident made researcher wonder could led survival demise others could identifiable reason regard people died others survived passenger data titanic ship somehow made public dataset frequently used introducing teaching machine learning relevant titanic dataset machine learning titanic dataset help beginner learn fundamental machine learning easier faster many article blog course use titanic dataset introducing machine learning beginner believe wrong use titanic dataset understand working machine learning fact titanic dataset better used learn machine learning used reason advisable introduce machine learning titanic dataset machine learning model reproducible replicable applicable similar situation reproducibility machine learning model mean ability use computational procedure data tool obtain similar result would possible reproduce model prof valid similar datasets population titanic case population considered people travelling luxury cruiser titanic dataset one sample population relationship model sample dataset spurious can not reproduced applied similar real world condition example machine learning model using titanic dataset applicable luxury cruiser suppose today exists luxury cruiser similar titanic apply model find survive die cruiser meet similar accident get insight kind screen passenger travel using model initiate risk mitigation plan course titanic dataset used training understanding purpose underlying principle machine learning lost important give importance training datasets used introducing topic machine learningmachine learning model treated black box without understanding make prediction though traditional statistical model differ algorithmic model necessary least understand perspective feature used make prediction identifying feature relevant problem hand first step analyzing related variable predicted case titanic dataset important question machine learning analyst determine variable make impact survivorship determining accuracy final prediction based model important variable passenger class compared lifeboat sex survivorship number lifeboat nearness passenger help survivorship feature set selected analysis take account consideration titanic dataset passenger data collected boarding nothing possible accident consequent survivorship last section article shall explain specifically hypothesis explains survivorship found possible dictum issued evacuation woman child first part evacuation procedure find even insight blog article use titanic dataset teaching machine learning insight derived machine learning model go far beyond insight obtained exploratory data analysis facilitate prediction deployed production environment machine learning model done without analysis relationship various feature target variable assumption behind model chosen may give high accuracy accuracy measure need necessarily imply strong relationship feature set target variable statistical model like linear regression inferential statistic validate model result data available titanic dataset helpful regard unearth real variable determine survivorship take extra care building new luxury cruiser percentage survivorship increased main application machine learning model accuracy measure suppose telecom company find high volatility customer accretion many customer seem purchase telecom product also leave frequently without identifiable cause machine learning model used analyze pattern unearth cause behind volatility also help build better product perform efficient customer management predicting customer likely remain leave building machine learning model initial focus select feature set applicable target variable case titanic dataset data pertains routine passenger data collected nothing accident survivorship case titanic dataset choosing three variable viz sex parent child sibling get accuracy predict current luxury cruiser survivorship based three variable accuracy get meaningful insight applicable entire population luxury cruiser titanic dataset simply collected purpose similarly also several public datasets used learning machine learning feature anywhere relevant predicted target variable two example widely floating around datasets viz telecom customer churn customer loan prediction many publicly available datasets two area feature relevance actual prediction made building machine learning model first check relevance feature set making prediction target variable understand variable validate model construct business model process utilize machine learning insight conclude article shall highlight point first titanic dataset could chosen machine learning training reason sufficient reason using machine learning titanic disaster accident accident would expect survivorship random survived would done mostly luck case titanic survivorship random fact among survivor third class child outnumbered first class child ratio four oneb men survived womenc twice many second class men survived first class child data intrigued analyst exploratory data analysis resulted following insight inquiry commissioned investigation resulted lord mersey report following sample insight actually contrast partial fact given though would expect men survive case titanic survival rate men whereas woman b go general common sense survival rate would determined variable like sex age class compared expectation found survival first class passenger expectation le expected third class passenger kind insight made investigator analyze come happen variable resulted kind survival pattern hypothesis made evacuation procedure involved dictum woman child first found captain lifeboat sufficient enough passenger number save analysis titanic dataset show indeed true however insight derived exploratory data analysis without use machine learning algorithm still many theory hypothesis tried explain survival data example one hypothesis go like first class woman embarked cherbourg much better chance surviving third class man southampton hypothesis give importance three variable sex class embarked station several theory formulated explain data true machine learning used test theory example build decision tree model using three variable alone check accuracy prediction purpose machine learning one exercise understand dataset based exploratory data analysis may find pattern might want test test validity pattern using machine learning model pattern validated possible build machine learning model decision making machine learning model meant deployed production environment help arrive decision titanic dataset could fun apply machine learning model bring forth real utility machine learning explained previous paragraph conclusion titanic dataset used primer teaching machine learning beginner promoted even kaggle however using titanic dataset teach understand machine learning lead several misconception beginner principle underlying building true machine learning model important principle emphasized use datasets like titanic machine learning model meant solve business problem help decision making meant used one exercise gain insight machine learning model meant reproducible replicable applicable similar context example variable affect customer churning one telecom company going greatly different another telecom company given equivalent product though identical context titanic sample population passenger travel luxury cruiser machine learning model focus understanding variable feature set influence decision making relationship insight help business decision making accurate prediction measure accuracy driving force behind building machine learning model machine learning model reflect nature business problem hand dataset reflect important underlying principle building machine learning model sufficiently emphasized use datasets like titanic understand machine learning medium shown article owned analytics vidhya used author discretion related data intrigued analyst exploratory data analysis resulted following insight inquiry commissioned investigation resulted lord mersey report following sample insight actually contrast partial fact given though would expect men survive case titanic survival rate men whereas woman b go general common sense survival rate would determined variable like sex age class compared expectation found survival first class passenger expectation le expected third class passengersthis kind insight made investigator analyze come happen variable resulted kind survival pattern hypothesis made evacuation procedure involved dictum woman child first found captain lifeboat sufficient enough passenger number save analysis titanic dataset show indeed true however insight derived exploratory data analysis without use machine learning algorithm still many theory hypothesis tried explain survival data example one hypothesis go like first class woman embarked cherbourg much better chance surviving third class man southampton hypothesis give importance three variable sex class embarked station several theory formulated explain data true machine learning used test theory example build decision tree model using three variable alone check accuracy prediction purpose machine learning one exercise understand dataset based exploratory data analysis may find pattern might want test test validity pattern using machine learning model pattern validated possible build machine learning model decision making machine learning model meant deployed production environment help arrive decision titanic dataset could fun apply machine learning model bring forth real utility machine learning explained previous paragraph conclusion titanic dataset used primer teaching machine learning beginner promoted even kaggle however using titanic dataset teach understand machine learning lead several misconception beginner principle underlying building true machine learning model important principle emphasized use datasets like titanic machine learning model meant solve business problem help decision making meant used one exercise gain insight machine learning model meant reproducible replicable applicable similar context example variable affect customer churning one telecom company going greatly different another telecom company given equivalent product though identical context titanic sample population passenger travel luxury cruiser machine learning model focus understanding variable feature set influence decision making relationship insight help business decision making accurate prediction measure accuracy driving force behind building machine learning model machine learning model reflect nature business problem hand dataset reflect important underlying principle building machine learning model sufficiently emphasized use datasets like titanic understand machine learning medium shown article owned analytics vidhya used author discretion related hypothesis made evacuation procedure involved dictum woman child first found captain lifeboat sufficient enough passenger number save analysis titanic dataset show indeed true however insight derived exploratory data analysis without use machine learning algorithm still many theory hypothesis tried explain survival data example one hypothesis go like first class woman embarked cherbourg much better chance surviving third class man southampton hypothesis give importance three variable sex class embarked station several theory formulated explain data true machine learning used test theory example build decision tree model using three variable alone check accuracy prediction purpose machine learning one exercise understand dataset based exploratory data analysis may find pattern might want test test validity pattern using machine learning model pattern validated possible build machine learning model decision making machine learning model meant deployed production environment help arrive decision titanic dataset could fun apply machine learning model bring forth real utility machine learning explained previous paragraph titanic dataset used primer teaching machine learning beginner promoted even kaggle however using titanic dataset teach understand machine learning lead several misconception beginner principle underlying building true machine learning model important principle emphasized use datasets like titanic machine learning model meant solve business problem help decision making meant used one exercise gain insight machine learning model meant reproducible replicable applicable similar context example variable affect customer churning one telecom company going greatly different another telecom company given equivalent product though identical context titanic sample population passenger travel luxury cruiser machine learning model focus understanding variable feature set influence decision making relationship insight help business decision making accurate prediction measure accuracy driving force behind building machine learning model machine learning model reflect nature business problem hand dataset reflect important underlying principle building machine learning model sufficiently emphasized use datasets like titanic understand machine learningthe medium shown article owned analytics vidhya used author discretion
88,https://www.analyticsvidhya.com/blog/2022/06/translate-spanish-audio-transcriptions-to-quechua/,article published part data science blogathon article create app translating spanish audio transcription quechua leverage gradio python package creating web interface model deploy app hugging face space advent internet everything getting urbanized pro like anything else drawback new generation becomes fluent language internet language verge extinction greatly impacted solve digital solution indigenous language developed time otherwise given rate progress global change revitalizing indigenous language challenging factor mind must develop solution aid revitalization protection indigenous language get around attempt devise app translating spanish audio transcription quechua language tool communicating uniting people usually people language cultural identity tend bond closely easily language dy one incentive people communicate one another also language perish also lose access wealth traditional information may beneficial furthermore indigenous language preservation may indeed mean eradicating discrimination indigenous people strengthening link culture language identity becomes even important preserve revitalize key concept feed audio speech input asr module module transform spanish speech text resulting audio transcription transmitted quechua translator module module translate spanish audio transcription quechua refer figure converting speech corresponding text leveraging jonatasgrosman wavvec xl r b spanish pre trained model hugging face hub trained contributed jonatas grosman model fine tuned version facebook wavvec xl r b spanish common voice dataset using model one need ensure speech input sampled khz converting spanish audio transcription utilizing small finetuned spanish quechua model model fine tuned sara benel jose vílchez spanish quechua dataset furthermore utilize gradio interface class establish ui machine learning model deploy app hugging face space step walk developing gradio app spanish asr translating resulting transcription quechua already hugging face account please visit website create one created hugging face account go top right side page click profile icon new space button directed new page asked name repository want create give space name choose gradio sdk option clicking create new space button result repository app created convenience provided demonstration video create requirement txt file list python package app run successfully dependency installed help pip install r requirement txt need add transformer torch librosa pyctcdecode pypi kenlm sake clarity make thing easier understand broken code section go code block one one start importing required library define function make sure speech input sampling rate khz specifying model name loading feature extractor setting pipeline asrfor converting spanish speech text leveraging jonatasgrosman wavvec xl r b spanish model also downloading feature extractor via autofeatureextractor class calculate sampling rate illustrated instantiate pipeline calling pipeline automatic speech recognition loading model tokenizer translating spanish transcription quechua defining function asr defining function translating spanish audio transcription quechua defining function generate spanish audio transcript translate quechuafinally write function output generated transcript spanish input audio well translated transcript quechua creating ui model using gr interfacenext utilize gradio interface class establish ui machine learning model providing function desired input component desired output component allow u quickly prototype test model case function translate_spanish_transcription providing audio input use microphone drop audio file via file path regard use code providing input since intended output string use output gr output textbox label output text displaying string output finally launch demo call launch method wish test audio file stored locally ensure sure uploaded location listed example shown code snippet worth mentioning component specified either instantiated object string shortcut upload audio file simply click following tab order listed file version contribute upload file get error please go see log tab right next spot runtime error shown take cue error log fix error space running error free work like link space http huggingface co space drishtisharma spanish audio transcription quechua versiona limitation associated module pre trained model asr play critical role audio accurately detected transcription wrong good chance output quechua translator module erroneous although pre trained model asr module function excellently usual scenario however may struggle confronted wide variety voice pattern different accent pitch play speed background audio condition solution aforementioned limitation get around limitation pre trained asr model trained example audio closely reflect auditory environment app used tested meet need b limitation associated module app translate spanish transcription quechua ayacucho since dataset used training quechua translator module module derived biblical text quechua ayacucho solution aforementioned limitation accumulating diverse data training pre trained model available quechua ayacucho could used devise solution require spanish asr resulting translated audio transcription quechua sum blog post learned create gradio app translating spanish audio transcript quechua challenge encountered developing robust asr solution text text translation indigenous language certain limitation could circumvented potential application thanks reading question concern please post comment section happy learning medium shown article owned analytics vidhya used author discretion
89,https://www.analyticsvidhya.com/blog/2022/06/create-gradio-demo-for-speaker-verification/,article published part data science blogathon article build app speaker verification using unispeech sat x vector leverage gradio python package creating web interface model deploy app hugging face space ever situation felt compelled speaker verification answered yes even curious topic post blog post show use unispeech sat x vector make basic speaker verification app set record straight can not promise finest application speaker verification spoof proof attempt creating app verifies two audio individual something pique interest please continue reading researcher harbin institute technology microsoft corporation proposed universal speech representation learning speaker aware pre training unispeech sat improving unsupervised speaker information extraction two approach namely utterance wise contrastive learning utterance mixing augmentation introduced approach integrated hubert framework former utterance wise contrastive learning used improve single speaker information extraction improve downstream task speaker verification identification latter namely utterance mixing augmentation especially useful multi speaker task eg speech diarization utterance mixing method attempt simulate multi speaker speech self supervised pertaining single speaker pre training data available using unispeech sat base model hugging face model hub creating speaker verification app model pre trained hour libri light hour gigaspeech hour voxpopuli wherein speech audio sampled khz utterance speaker contrastive loss result critical make sure speech input also sampled khz furthermore model fine tuned voxceleb dataset using x vector head additive margin softmax loss furthermore utilize gradio interface class establish ui machine learning model deploy app hugging face space app take two speech sample input apply sox effect audio retrieve feature audio finally calculate cosine similarity cosine similarity score exceeds threshold value set audio individual otherwise speaker verification authentication fails following step step guide creating speaker verification app using gradio hugging face space already hugging face account please go visit website create one created account go top right side page click profile icon new space button directed new page asked name repository going establish give space name choose gradio sdk option clicking create new space button result app repository built watch demo video included create requirement txt file list python package app need execute dependency installed pip install r requirement txt backend segment broken code section clarity make thing easier understand go code one one go start importing necessary dependency autofeatureextractor class help extraction audio feature automodelforaudioxvector class load pre trained model microsoft unispeech sat base plus sv audio retrieval via x vector head general practice pytorch set variable called device hold device training cpu gpu default tensor created cpu model also initialized cpu result one manually ensure action performed gpu one want leverage gpus fast computation pytorch provides easy use api transferring tensor created cpu gpu new tensor formed device parent tensor model rationale applies result data model must transferred available device however premium subscription plan able use cpu accelerated inference case model tensor directed cpu specify audio effect used favor mono channel audio produce stereophonic wide effect channel merged one also previously stated model pre trained speech audio sampled khz must ensure speech input sampled khz well create function take audio stream two different file directory stage sox effect applied audio feature audio retrieved finally cosine similarity calculated recommend take look tutorial explores cosine similarity detail cosine similarity score exceeds threshold value set audio person otherwise speaker verification authentication fails next utilize gradio interface class establish ui machine learning model providing function desired input component desired output component allow u quickly prototype test model case function similarity_function provide two audio stream since want accomplish speaker verification evaluating cosine similarity score two audio providing audio input use microphone two different file path since intended output string use output gr output displaying string output use textbox label output text finally launch demo call launch method addition set enable_queue true force inference request served queue rather parallel thread avoid timeout longer inference time minute required additionally wish test audio file stored locally make sure audio uploaded path file provided example shown code snippet worth noting component given either instantiated object string shortcut upload audio file go file version contribute upload file order stated run error head straight see log tab located right next spot runtime error shown space running error free function follows note set threshold value app prompt authentication failed cosine similarity score fall value link space http huggingface co space drishtisharma speaker verification using unispeech sat xvectorsthere pressing need develop robust intelligent solution meet need perhaps speech audio based authentication combined textual visual touch based authentication create multi modal authentication system hence blog created speaker verification app us cosine similarity score determine two audio person future scope work may include combining speech audio based authentication textual visual touch based authentication create multi modal authentication system sum key takeaway article u follows thanks reading question concern please post comment section happy learning medium shown article owned analytics vidhya used author discretion
90,https://www.analyticsvidhya.com/blog/2022/06/the-datahour-how-to-stay-relevant-in-world-of-ai/,analytics vidhya long forefront imparting data science knowledge community intent make learning data science engaging community began new initiative datahour datahour series webinars top industry expert teach democratize data science knowledge th march joined anastasiia molodoria datahour session stay relevant booming world ai anastasiia strong math background experience predictive modelling nlp natural language processing data processing deep learning successfully integrated ml dl nlp solution retailer product tech company considering optimization automation routine daily task increasing business efficiency currently working mobidev data science team leader excited dive deeper world data science machine learning got covered let get started major highlight session stay relevant booming world ai ai session two learning anastasiia covered topic considering business case deeper understanding value ai integration solving real world problem also help get insight main step done achieving successful delivery ml product client provide right expectation business know beforehand exact output ml research prerequisite basic understanding data science let dive ocean ai basic example try understand right estimate expectation need meet click start project let begin first need figure two thing poc v mvpwhat exactly two terminology need need know let look poc mvp work take input task need solve help u getting desired output input output output given poc help maintaining balance minimum viable product type classification get optimal set feature start example donut market market flooded donut suppose newcomer business knowing make donut benefit business many brand already exist new need emerge basically idea boost business make successful idea extra feature add donut extra feature called mvp solution let see build mvp another example explanation example st way making car minimum viable product suppose something go wrong step three possible get desired product successfully effort whether money time effort go vain moreover nd way making perfect way building minimum viable product poc mvp individually answer basic question know start case yes next step goal step clear case idea choose poc project sequential development example sequential development project developing mobile application know next step get desired result data science project iterative type project need know business understanding ask client want develop data understanding assignment data client might situation able get insight data connect client get proper understanding data data preparation one major dimension data scientist look whatever project handling modelling select model fit idea best observe glitch model go back look data preparation correcting new model correction model done evaluation evaluate data whether model work work go deployment need understand business deployment deploy ai project source presenterthis different project need somehow estimate example one side mobile application le clear specific operating system need add button etc properly estimate know result advance know whether work accuracy need frequently asked question please estimate sure example idea develop mobile application based ai main core task can not solved need gather developer solving main functionality ask client start poc sure fine completely case client ask something like accuracy commitment know result advance describe client example first stage select several model going try usually model tested open source data set receive metric share look client convey client sure data try model need developed fine example let imagine test poc experiment audio file look good sure work production becomes product data see completely different like lot background noise approach working better write risk output get wrong refer point provided client beginning like described describe client page understanding great first confident estimate achieving goal main point many detail reach goal better important client tell true story cover today bit later client tell runtime mean matter better clarify source presenterwhen show something abstractive mean apply model open source data one story completely another story client provide lot data eg three picture client see insight data completely different story get trust client definitely even small amount data try use demo client make sure client understand point working data science area really easy lose people lot technical detail often client technical describe complicated stuff easy word make sure client understands plan ask question clear maybe paraphrase fine better time avoid miscommunication future really even worse last recommendation provide report client detail work really great point finish meeting share report client go detail interesting one includes optimization computer vision nlp time series let understand example cafe chain owner main goal owner support maintain business owner came two question input data client provides sql database bunch table connection table data expected solution client want number product sold enough try think wider deeper competent data scientist understand example business owner product main goal person let imagine customer support via chat need somewhere text business case chat client came two request propose client tabular data structured data form database consists row column say tabular data table store data different type matter whether boolean number alphabet tabular data make data ready insight efficient usually deal task finite least deal tabular data three main task regression classification clusterization regression task predicting specific number like price sale unit mean number decimal classification based assigning class observation example disease detection whether good sentiment clusterization know number class want group data specific amount group discussed previously customer group detection wrap people behaviour true expectation reality different expecting missing data variable well known data clean everything fine goal apply model tune reality good insight data need follow understanding data wrapping table classical table data task main first important step data understanding unknown stuff completely understandable stage need full understanding clear ask client sure data possible develop really valuable model data cleaning preparation data ready modelling feature engineering modern feature engineering great interesting step generate new feature get new insight discus client like experiment really interesting modern step task go back official engineering data cleanup working iterative process sequential modeling like experiment really interesting modern step task go back official engineering data cleanup working iterative process sequential evaluation fine evaluation validate data data set make sure overfitting everything working expected result improved basically area working text presenter found really great research click link interested read nlp market main idea amount investment nlp expecting huge growth considering number project working nlp quite huge area definitely grow research cover two key growing direction mean mean get used siri usage example want find something youtube turn tv want tap want say much easier still nlp stuff even developed nlp also like lot direction first need split text punctuation without penetration different story idea split text unique value assign token course different option spreading tokenization high level idea convert text digit work digit sequentially look easier someone worked text yet hope understanding really complicated hood two approach generally text classification key vertex similarity solved approach see task solve task classical approach maybe relevant real world main focus deep learning model trained neural network produce really great example enrich model custom data area developed much right let say task generate summary given input text two year ago extractive summary approach like everywhere abstractive really rare produce good result right situation opposite completely extractive summary approach core intuition behind text split text sentence score sentence apply rating select relevant output contain original sentence likely context abstractive summary approach paraphrase generate new sentence input let say sentence output one paraphrased like shortly summary text right popular produce really great result several year ago completely opposite image captioning idea image input detecting object generating description text could useful automatic creation text description photo interesting point annotation blind people convert voice like describe blind people shown computer vision area ai working image picture classic task computer vision example let try understand detect whether cat dog picture perform task help computer vision direction picture need converted number working number look behind read like ordinary jpg png picture look like three layered metric two dimension height width pixel next one three channel rgb example picture three channel usually different mouth usually three channel red green blue picture looking like font contains three layer based intensity color achieving result know anything computer vision scenario dataset client provide dataset want u detect person data scientist say client three option lidar really interesting maybe someone already camera mobile phone idea camera laser output picture like picture information light going plenty information lidar really high developed right gans really awesome interesting neural network computer vision main idea behind input picture modify picture apply smile change race hairstyle hair card something appearance useful different area starting generating new data sample data set photo editing face animation human pose estimation idea detect key point useful fitness apps example identify whether properly exercise use case studied human pose estimation guidein scenario meet type client expectation thing must consider writing code ai hope enjoyed session afterwards stay relevant world ai secondly layman example must complimented learning wish good luck learn grow high
91,https://www.analyticsvidhya.com/blog/2022/06/datahour-traversing-the-journey-of-an-analytics-problem/,analytics vidhya long forefront imparting data science knowledge community intent make learning data science engaging community began new initiative datahour datahour series webinars top industry expert teach democratize data science knowledge th may joined amitayu roy datahour session traversing journey analytics problem amitayu ray data science leader year experience analytics consulting ai solution design client management business development across industry practice area helping telecom medium giant around world realize value large scale ai ml implementation empowering organization box ai solution solve modern constantly evolving problem amitayu currently working senior manager field applied intelligence strategy consulting accenture work exchange strategy consulting lead analytics consulting data practice ai machine learning enablement north america geography session linkare excited dive deeper world data science machine learning got covered let get started major highlight session traversing journey analytics problem session learn objective session basically session data science journey storytelling journey taking problem business problem industrialized value based solution give business focusing focusing kind value realize analytics problem talk process associated typical analytics problem analytic solution talk evolution data science year heading help really translate business problem know translate certain analytical form also tell outline key responsibility area different rule evolving right session suggest deep dive aiml algorithm technical session data engineering big data cloud platform process oriented session help understand end end journey exactly work fit going talk model performance parameter feature engineering etc talk high level going get detail exactly going whether okay also going talk agile approach deliver aiml project important understand journey need basic foundational update example session group individual need plot data maturity curve already advanced analytics starting kept session generic possible presenter tried answering narrating journey manner becomes clear everyone let quick round data science evolved year analytics emerges bi emerged first time data started becoming important asset mostly excel vba etc know consumption business perspective whoever consuming looking manual report somebody opening excel pivot etc early data side looked data warehouse etl system report side looked dashboard tableau power bi dominant system point time sa spss statistical model working time regression model time series clustering analytics consumed report dashboard semi automated model run manually even feature engineering done manually people start investing building stronger data warehouse resultantly emerges analytics time people already realized data important asset big data started emerge hadoop already come picture dashboard power bi mostly automated ml model come picture first time late eventually become stronger application compared sa building high end ml model start model fully automated mean manually run model check result roc curve performance matrix model validation etc full manual check analytics actually happened advancement big data transformed way manage compute data big player like amazon google microsoft started investing integrated cloud platform realized tremendous profit cloud platform service end end data management reporting visualization modeling model implementation etc web based report automated report deep learning ai nlp tensorflow simulation model auto ml became common everybody started thinking productionizing analytics analytics model ml world came existence around analytics heading possibly going reach different dimension analytics quantum computing big data cloud become normal organization visualization mostly become ervr visualization fact visualization tableau power bi also exist production ready ai implementation new investing model become end end industrialized e end end automate must skill good skill top foundational skill next three five year since evolving industry one important take away upskill evolve consistently stay relevant market typical industry problem encounterlets start problem raw form problem look like analytics consultant data scientist need clarity mind able answer address question four major strategic priority earn bread generate money business imagine customer industry view customer journey customer analytics applicable everywhere right prospect assessment acquisition onboarding growth loyalty etc source amitayu presentation churn industry agnostic mean could service providing industry banking telecom retail consumer good e commerce loyalty management churn retention two major function associated churn five major question churn problem anywhere world need address impact value business trying achieve addressing particular problem data scientist business approach u ask befitting solution main reason behind churn cost acquisition main thing organization typically kind churn analysis cost acquisition high need retain customer ensure data scientist able sort hypothesis driven approach proven approach consulting firm analytics firm sort adopted many year hypothesis seek explain something happened might happen certain condition often written statement hypothesis driven approach five major way solve problem look solve question draw analytics solution analytical problem example perform issue tree source amitayu presentation every problem build issue tree build hypothesis framework source amitayu presentation kind role involved journey mostly business analyst domain expert small representation data analyst small representation data scientist sitting together brainstorming translate business problem analytical problem business analyst typical consultant play big role know look industry practice look benchmark accordingly try come hypothesis data engineering expected bulk work data engineer data scientist data analyst also play small role field space analytical approach model building insight visualization everything happens data scientist big role play data analyst data engineer also significant role play one building data ml engineer role implement model new role coming industry people implement model existing framework kind role requires understanding model well understanding technical architecture therefore say particular role play big part solution implementation ml engineer business analyst also play big part one bringing entity together making solution success data engineering powerhouse mitochondrion today data driven world data scientist data scientist many year data engineer one hold everything together data engineering team ensuring data flow gap solution business self service business anything everything enabled data engineer data scientist key trend assessment existing data architecture address churn problem first thing ass architecture business initial step building assessment enterprise data architecture little advanced know sandbox setting sandbox virtual workbench connecting data source application within sandbox data warehouse data lake also part data engineer development analytical data record churn problem need build analytical data record customer level data set help build model data engineer well data scientist going build data together task acquisition data quality assessment creation feature integration lake merge data warehouse data dictionary metadata management deployment analytical solution framework create data pipeline automate end end solution mlops creating scaling automation data ensure code configurable deployable using parameter driven batch code performing feature engineering based use case applying meta data management output data engineer produce purpose model analytical data record customer view customer possible attribute customer brought together one platform like financial usage service product channel etc build exhaustive customer record thousand thousand feature cater many many different kind use case churn churn one source amitayu presentationby using model engine able answer two churn related question source amitayu presentationhow whatever model development deployment need future engineering hypothesis testing lot ensemble algorithm selection specialized algorithm specialized use case etc importantly iteration keep iterating process till time model look good offer recommendation model every segment even every single customer create offerpivotal element client digital transformation across industry source amitayu presentation source amitayu presentationsource amitayu presentationstart simple start delivering smaller value reach goal source amitayu presentation hope enjoyed session understood well major takeaway session
92,https://www.analyticsvidhya.com/blog/2022/06/snowflake-architecture-and-key-concepts-for-data-warehouse/,article published part data science blogathon article help focus depth understanding snowflake architecture store manages data well conceptual fragmentation concept end blog also able understand snowflake architecture differs cloud based website massively parallel processing database business today full data amount data produced daily truly amazing data explosion becomes increasingly difficult capture process store large complex data set therefore becomes necessary organization central archive data stored securely analyzed make informed decision data warehouse come play data warehouse also known one source truth central database support activity data analytics business intelligence bi data warehouse store large amount data multiple source one place intended questioning analysis improve business analytical power allows organization obtain important business information data order improve decision making snowflake cloud based data warehouse solution provided saas software service full ansi sql support also unique structure allows user simply create table start query data little management dba task required find snowflake price let talk great feature snowflake data warehouse data protection protection snowflake data repository provides advanced authentication providing multi factor authentication mfa federal authentication single login sso oauth communication client server secured tl standard extended sql support snowflake data repository support multiple ddl sql dml command also support advanced dml transaction lateral view saved process etc connectivity snowflake database support comprehensive set client driver connector python connector spark connector node j driver net driver etc data sharing securely share data snowflake account learn feature snowflake datawarehouse let learn snowflake building hevo data easy way test snowflake datahevo code data pipeline support pre built data integration data source reasonable price automatically streamline entire data transfer process offer set feature support compatibility database data storage area let look invincible feature hevo simple hevo simple intuitive user interface fault tolerant hevo offer faulty tolerant structure automatically detect confusing alert immediately record affected set aside correction real time hevo real time live streaming system ensures data always ready analysis schema map hevo automatically detect schema incoming data map destination schema data transformation provides simple visual interface complete edit enrich data want transfer live support hevo team available around clock provide specialized support via chat email support phone way improve data warehouse single tier architecture type architecture aim extract data order reduce amount data stored two tiered architecture type architecture aim separate actual data source database enables data warehouse expand support multiple end user three tiered architecture type architecture phase section contains data warehouse server database intermediate section online analytical processing olap server used provide vague view website finally advanced section advanced client framework includes tool apis used extract data component data warehouse follows database warehouse databasethe website form integral part database database store provides access corporate data amazon redshift azure sql come cloud based database service extraction transform load etl toolsall activity associated extraction conversion uploading etl data warehouse fall category traditional etl tool used extract data multiple source convert readable format finally upload data warehouse metadatametadata provides framework definition data allowing creation storage management use data database access toolsaccessibility tool allow user access usable business friendly information data warehouse warehouse tool include data reporting tool data inquiry tool application development tool data mining tool olap tool snowflake architecture contains combination standard shared disk unallocated format provide best let go building see snowflake integrates new mixed type construction used standard website shared disk architecture single storage layer accessible cluster node many cluster node cpu memory without disk storage connect central storage layer data processing processing contrast shared disk architecture shared nothing architecture distributed cluster node disk storage cpu memory advantage data categorized stored across cluster node cluster node disk storage snowflake support high level formation shown diagram snowflake different layer storage layout computer layer cloud service background storage layoutsnowflake organizes data many smaller compartment internalized compressed us column format save data stored cloud storage act shared disk model thus providing ease data management ensures user worry data distribution across multiple node unassigned model calculation note connect storage layer download query processing data since storage layer independent pay monthly storage amount used snowflake offered cloud storage expandable charged per monthly tb use computer layersnowflake us virtual warehouse described answer question snowflake split query processing layer disk storage us query layer using data storage layer virtual warehouse mpp compiles include multiple node cpu memory provided cloud snowflake multiple virtual warehouse created snowflake variety need depending workload visible warehouse operate single layer storage typically visible warehouse independent computer collection interact warehouse benefit visual warehouse listed virtual warehouse started suspended time measured time without impact effective query also set auto stop restart automatically storage stopped certain period inactivity query sent restarted also set default smaller larger collection size hence e g set minimum maximum depending load snowflake provide bulk storage container cloud service layerall function authentication security uploaded metadata data management query integration linking snowflake across layer occur layer example service hosted layer application entry filed must go section query sent snowflake sent developer layer forwarded compute layer processing query metadata needed improve query filter data stored layer three layer measure independently charge snowflake storage warehouse look separately service layer managed within given computer node charged advantage snowflake structure measure one layer without example measure storage layer extension charged storage separately virtual repository provided rated additional resource needed quickly process query improve efficiency learn snowflake building familiar snowflake building time discus connect snowflake let take look best foreign company tool technology create extended ecosystem connect snowflake snowflake ecosystem list take snowflake partner customer third party tool emerging technology snowflake ecosystem snowflake partner connect list take snowflake partner offer free snowflake connect test standard configuration client set standard configuration command apply client provided snowflake cli connector driver snowsql cli client snowsql next generation command line utility connect snowflake allows create sql query perform ddl dml task connector driver snowflake provides driver connector python jdbc spark odbc client improve app go listed start learning applying always connect snowflake using tool technology mentioned
93,https://www.analyticsvidhya.com/blog/2022/06/all-about-data-pipeline-and-kafka-basics/,article published part data science blogathon old day people would go collect water different resource available nearby based need technology emerged people automated process getting water use without collect different resource using single resource need data pipeline general term similar example like set step involve storing enriching data source destination help gain insight also help automation storing transformation data hear term data pipeline following question immediately deserve mention case data producer data consumer producer can not send data data processing data governance data cleaning type data pipeline determined purpose utilized might used data science machine learning business analytics among thing real time data pipeline batch data pipeline lambda architecture real time batch producer send either batch real time data batch data csv database mainframe example traditional data example weekly monthly billing system real time data information come iot device satellite source instance traffic management system entering central data pipeline batch data go batch ingestion real time data get stream ingestion batch ingestion processing data acquired period time contrast stream ingestion involves real time data processing done piece piece data pipeline made several separate component consider od operational data store batch data staged processing stream data also staged message hub using kafka nosql database mongodb used messaging hub organization able collect data number source store single location thanks od kafka distributed data storage may used create real time data pipeline even data already analyzed still possible enrich mdm used master database management assist data reducing error redundancy data ready may delivered intended destination data lake data warehouse customer may utilize create business report machine learning model dashboard among thing data put database data warehouse end etl pipeline data pipeline hand clearly different since involves importing data serf link source destination etl pipeline may thought subset data pipeline let dive apache kafka linkedin developed kafka handing apache foundation apache kafka open source platform work event fully understand event driven approach must first understand difference data driven event driven approach data driven consider online retailer amazon customer purchase product x date database record transaction would happen consider million customer data kept several database communicate one another online site event driven approach us sort interaction company website however data kept queue database may get information need queue might single activity group business action example customer purchase product x date event recorded following information customer id name aorder id date kafka encourages use queue store event customer use queue get information need next question come mind kafka differ middleware store large amount data period time unique event even customer place second purchase may purchasing product different date another notable role kafka us log data structure offset field log indicates data taken never overwrite existing data instead append data end log data structure log basic data structure describing append sequence record immutable log record added end log file exact sequential manner message row smallest unit kafka architecture topic tablepartition viewthe message mail symbol topic cylindrical pipe partition plainly visible image fault tolerance scalability division kept multiple setting event sorted sequenced within one partition kafka broker another name kafka server get data storage retrieval instruction producer consumer data replicates also kept separate broker leader event failure replicated broker exercise authority leader always interact producer consumer kafka cluster collection many broker explore kafka product kafka core topic log partition broker cluster kafka connect connecting mainframe db ksql sql designed kafka kafka client connect client kafka stream manage stream ingestion kafka loose decoupling flexible environment fully distributed fault tolerance event based approach zero downtime scalable architecture vendor lock open software clearly see concept data pipeline useful real world problem automation making data available consumer single source access making easier gain insight progressive analysis provides flexible environment medium shown article owned analytics vidhya used author discretion
94,https://www.analyticsvidhya.com/blog/2022/06/scraping-data-using-octoparse-for-product-assessment/,article published part data science blogathon hello data enthusiast thrilled see discus another compelling use case support data analytics data science know invariably depend landing area time data external source received either pulled pushed process data provider side followed landing data lake layer later apply cleaning technique data transformation technique business rule top data nothing data preparation task would served bi ai layer individual business requirement yes case e grab data external source using web scraping technique data torturing top data find insight data technique time forget use find relationship correlation feature expand opportunity explore applying mathematics statistic visualisation technique top selecting using machine learning algorithm finding prediction classification clustering improve business opportunity prospect tremendous journey focusing excellent data collection right resource critical success data platform project hope know article let try understand process gaining data using scraping technique zero code getting try understand thing better let touch basis data science lifecycle know please visit http editor analyticsvidhya com preview index article_id mentioned earlier data source d da could data source focus web scraping process web scraping process extracting data diverse volume specific format website form slice dice data analytics data science standpoint file format depending business requirement would csv json xlsx xml etc sometimes store data directly database web scraping critical process allows quick economical extraction data different source followed diverse data processing technique gather insight directed understand business better keep track brand reputation company align legal limit first step request target website specific content particular url return data specific format mentioned programming language script know parsing usually applied programming language java net python etc structured process taking code form text producing structured output understandable way last part scrapping download save data csv json format database use file input data analytics data science perspective multiple web scraping tool software available market let look market many web scraping tool available let review prowebscraper prowebscraper one powerful web scraping tool web based used materialise classical customer service cost effectiveness help u extract data website designed make scraping process completely uncomplicated exercise yes requires coding pointing clicking required item output extract target dataset let focus octoparse web data extraction tool octoparse stand device market extract required data without coding scrape data modern visual design automatically scrape data website along saas web data platform feature octoparse provides ready use scraping template different purpose including amazon ebay twitter instagram facebook bestbuy many let u tailor scraper according requirement specific compared tool available market beneficial organisational level massive web scraping demand use multiple industry like e commerce travel investment social crypto currency marketing real estate etc time scrape ebay product information using octoparse getting product information ebay let open ebay select search product copy urlbefore starting journey download octoparse version demo purpose http www octoparse com download window step able complete entire process let login octoparse paste url hit start button octoparse start auto detect pull detail separate window wait search reach get data need detection octoparse select critical element convenience save time verification page click create workflow note remove cooky please turn browser tag confirm detection workflow template ready configuration data preview bottom configure column convenient copy delete customize column etc add custom field data preview window import export data remove duplicate configure list column require done preview selected individual line item clicking right side paneon workflow window based hit navigation could move around web browser go web page scroll page loop item extract data add new step configure time file format json action performed often action perform required configuration done could act extract data save configuration run workflowschedule taskyou run device cloud data extraction process startsdata ready exportchose data format usagesaving extracted dataextracted data ready specified formatdata ready usage either data analytics data sciencewhat next yes doubt load jupiter notebook start using eda process extensively far explored various aspect sure could able understand data source data processing data science ml lifecycle web scraping process involved tool market key feature along detailed step extract product data ebay using octoparse quick summary article take away enjoyed web scraping tool impressed feature try want extract free data data science analytics practice project perspective medium shown article owned analytics vidhya used author discretion
95,https://www.analyticsvidhya.com/blog/2022/06/datahour-artificial-intelligence-in-retail/,analytics vidhya long forefront imparting data science knowledge community intent make learning data science engaging community began new initiative datahour datahour series webinars top industry expert teach democratize data science knowledge th april joined dr shanta mohan datahour session artificial intelligence retail dr shanta mohan mentor project guide integrated innovation institute carnegie mellon university pittsburgh pennsylvania usa co founded retail solution inc rsi pioneering retail analytics company based california ran global product development team started career bachelor engineering hons electronics communication engineering college engineering guindy pursued phd operation management tepper school management carnegie mellon university experience close decade dr shantha worked semiconductor industry worked software engineering led kaveri inc professional service business ceo co founded led global development team retail solution inc rsi year excited dive deeper world ai got covered let get started major highlight session artificial intelligence retail need know technology get technologically advanced altogether put many challenge u session learn kind problem exist artificial intelligence retail opportunity apply ai machine learning problem datahour focus retail data important address many challenge present retail retail mean selling small quantity ultimate consumer industry selling got several stakeholder one provides product service sell service retailer consumer product applying ai machine learning case study find flow pattern people use make process efficient make customer happier retail industry landscape focus three important dimension many focus mainly geography deal whereabouts particular location location determines challenge know important criterion part location product kind product retail implication product challenge encounter possible solution one think channel product get customer supply chain creates term unique challenge retail industry evolved lot year depending part world west kind technological advance way back term beginning cash register credit card e cash register information system w w w world wide web e commerce recently social medium advance made lot difference western developed world early wal mart first opened big box retailing thing never infact exist longer though exist place even today developing world phenomenon new developing example country like india kirana corner shop lifeline retail e commerce taken big way country still popular traditional kirana shop e commerce creates challenge opportunity location determines kind focus retail servicing customer west e g u focus automating store keep control cost production labour expensive principle led establishment amazongo source dr shanta mohan presentation developing country focus make product service available customer efficiently better price get good remote location reliable speedy manner focus different though often technology look happening developed part world wan na bring part developing country also always focus adapting new technology add progress kind product service get determine kind problem challenge retailer supplier encounter image source dr shanta mohan presentationthe product characteristic determine supply chain structured kind issue arise get product customer quickly freshness authenticity maintained white good fridge c etc one buy time lifetime compared something like fresh fruit perishable short life span fresh fruit need delivered quickly compared white good apparel consumer attracted supplier buy new fashion every season often kind problem arise sector different concerned servicing customer latest time problem wastage eg somebody may buy something internet size issue return initiated return big problem industry luxury good market focus quality customer experience one everything meet customer expectation lastly virtual product like netflix financial instrument etc requirement meet customer demand keep engagement aim able provide good service efficiently possible maintain customer base keep going away retailer therefore kind solution develop ai important mainly three type brick mortar retailer walmart costco today amazon example channel although amazon e commerce site time understood need physical presence mart order keep front customer well address challenge arise due e commerce source dr shanta mohan presentation ecommerce alibaba flipkart etc example channel advantage challenge trick crafting aiml solution understand advantage address challenge source dr shanta mohan presentation dtcs nestle cosper etc example channel cater customer directly benefit knowing customer better sum retailer providing customer whole idea behind aiml data able discover customer appealing source dr shanta mohan presentation without data transformation performed need search data collect make sure data collected relevant apply analytics order understand insight found data source dr shanta mohan presentation retail collaboration key order get benefit collaboration entity main stakeholder retail space owns part data example retailer data inventory data predicting data customer buying pattern etc data called master data retailer supplier make sense insight get data enough transaction data sale order understand better collaboration important benefit including customer source dr shanta mohan presentation type data deal retail one principle applies talk data e garbage garbage data good enough derive insight data useless early day used v data presently v data converted v data source dr shanta mohan presentation source dr shanta mohan presentationit help automating making product available frictionless experience walk store either shop walk queue cash point detector used technology automatically perform function exit example amazon go etc order survive business store need innovate example aldi global retailer innovated survive market wan na buy alcohol monitoring face decide accordingly another example watasale announced india today either vanquished exist source dr shanta mohan presentation challenge ai enabled smart mirror example lenskart us technology done get better faster sale perfect customer satisfaction use ai different way chatbots earlier chatbots home based ai enhanced robotic earlier used test test method shifted voice text alexa siri etc recommendation system recommendation showed u social medium happens aiml collaborative filtering data product searching last buying pattern etc traced accordingly feed managed involves well similar user like another content based filtering involve user focus solely behavior today better system combine collaborative filtering content based filtering source dr shanta mohan presentation two supply chain first one traditional one involves supplier distributor retailer delivery last customer supplier supply material distributor distributor retailer retailer delivery directly customer second comprises supplier customer supplier provides delivery product service customer supply chain problem best monitored aiml help supplier invoke ai check whether product reaching customer promised date best way use delivery fast manage return aiml use make long process little simpler faster source dr shanta mohan presentation per record e commerce penetration retail doubled since add retailer carbon footprint increasing need fast delivery added increased carbon footprint order deliver earliest club delivery concept vanishing nowadays resulting shipping number packaging issue retailer ship item one go could reduce carbon footprint great extent example ups kind delivery van us aiml ai help driver discovering efficient way delivering product source dr shanta mohan presentation going many new technology come hope enjoyed session artificial intelligence retail understood well hope employing use case provided clear understanding artificial intelligence retail
96,https://www.analyticsvidhya.com/blog/2022/06/make-amazing-data-science-projects-using-pyscript-js/,article published part data science blogathon source http betterdatascience com pyscript intro pyscript j front end framework enables use python browser developed using emscripten pyodide wasm modern web technology using python browser mean replace javascript provides convenience flexibility python developer especially machine learning engineer pyscript offer program communicate python javascript object require server dedicated hardware hosting include required file package python environment support use popular python library like numpy panda provides flexibility developer quickly build python program existing ui component button container tutorial show create machine learning model web gui using pyscript use famous car evaluation dataset predict car condition based six categorical feature discus dataset later first start setting pysscript j library section set html template include pyscript j library use vscode choose ide create directory named pyscripttut creating html templatecreate html template inside named index htmlinside template place starter html codebootstrap cdn used styling web pagepyscript installationwe install library machine directly import library pyscript website add code head section html template important note use local server run html code otherwise may face issue importing several library python environment try open webpage double clicking index html file like file home javascript src index j may get error cors policy importing library e module subject origin policy using vscode use live server extension also create python server writing command terminalyou access html page url http index htmlsample codeyou try sample code check whether pyscript successfully imported simple program print number using loop everything go fine output look like thathurray pyscript library installed successfully template section create web gui use machine learning model training testing mentioned use bootstrap library creating custom styling also used inline cs place add google font cdnadd code head section html template add montserrat font web page cs configurationadd code template enable smooth scrolling web page apply font adding bootstrap navbar componentadd code body section apply navbar adding heading contentwe create small landing page text image source image used component found component train modelin component create radio button input text user select classifier want train many test split component alert messagesthis component used alert success message component checking training resultsin see accuracy weighted f score selected model training component selecting car parameterswe select six parameter check performance car submit remain disabled train model component output resultthis component display predicted value footer optional footer web pageour gui created onwards train machine learning model need add library python environmentadd code head tag html template import required non standard library create py script py script tag inside body tag html template write python code inside py script tag firstly import necessary librariesas discussed earlier use car evaluation dataset uci ml repository download dataset link dataset contains six categorical feature buying price maintenance price door person luggage capacity safety qualification buying price low mid high high maintenance price low mid high high door person luggage small med big safety low mid highthe output classified four class namely unacc acc good vgoodthese follows unacc unaccepted acc accepted good good vgood goodfunction upsample datasetfunction read input data return processed data let understand function detail firstly read csv file using panda library may confused line py script write headingtext pre processing dataset code update message component html dom created write message html tagpy script write component_id your_message removed null value duplicate luckily dataset contain null value converted categorical data numerical data finally performed upsampling dataset observe number sample one particular class far class model biased towards specific class little data train class increase number sample class also called upsampling created separate function named upsampling upsample data function check machine learning model selected user training function train model chosen classifier function trigger user click train button code self explanatory also added comment code section test model six parameter discussed function test model firstly take input user feed input model prediction finally output result machine learning model trained finally add code trigger python function clicking html button github code link deployed version linkbefore pyscript proper tool use python client side framework django flask mainly use python backend recent year python grown population immensely used machine learning artificial intelligence robotics etc article trained tested machine learning model completely html language increase model accuracy tuning hyperparameters searching best parameter using grid search cv randomized search cv main focus article use pyscript j library achieve high accuracy classification model key takeaway tutorial firstly imported pyscript j library html template run basic python code created required component html dom performed dataset pre processing trained model according user selection finally written code test model based user input today hope enjoyed article doubt suggestion feel free comment also connect linkedin delighted get associated check article also thanks reading github instagram facebookthe medium shown article owned analytics vidhya used author discretion
97,https://www.analyticsvidhya.com/blog/2022/06/mental-health-prediction-using-machine-learning/,article published part data science blogathon technology evolving round clock recent time resulted job opportunity people around world come hectic schedule detrimental people mental health covid pandemic mental health one prominent issue stress loneliness depression rise last year diagnosing mental health difficult people always willing talk problem machine learning branch artificial intelligence mostly used nowadays ml becoming capable disease diagnosis also provides platform doctor analyze large number patient data create personalized treatment according patient medical situation article going predict mental health employee using various machine learning model download dataset link agecount e mean e std e min e e e e max e rangeindex entry data column total column column non null count dtype timestamp non null object age non null int gender non null object country non null object state non null object self_employed non null object family_history non null object treatment non null object work_interfere non null object no_employees non null object remote_work non null object tech_company non null object benefit non null object care_options non null object wellness_program non null object seek_help non null object anonymity non null object leave non null object mental_health_consequence non null object phys_health_consequence non null object coworkers non null object supervisor non null object mental_health_interview non null object phys_health_interview non null object mental_vs_physical non null object obs_consequence non null object comment non null object dtypes int object memory usage kb nonetotal percent comment state work_interfere self_employed benefit age gender country family_history treatment no_employees remote_work tech_company care_options obs_consequence wellness_program seek_help anonymity leave mental_health_consequence phys_health_consequence coworkers supervisor mental_health_interview phys_health_interview mental_vs_physical timestamp total percent age_range obs_consequence gender self_employed family_history treatment work_interfere no_employees remote_work tech_company benefit care_options wellness_program seek_help anonymity leave mental_health_consequence phys_health_consequence coworkers supervisor mental_health_interview phys_health_interview mental_vs_physical age text age inference plot show age column respect density see density higher age year dataset inference treatment mean treatment necessary mean first barplot show age year treatment necessary needed year text total distribution treated inference see male treated compared female dataset inference barplot show mental health female male transgender according different age group analyze age group mental health high female compared another gender age mental health high transgender compared male inference dataset family history mental health problem probability mental health high see probability mental health condition transgender almost family history medical health condition inference barplot show health status respect care option dataset care option probability mental health situation high see mental health transgender high care option low care option inference barplot show probability health condition respect benefit dataset benefit probability mental health condition high see probability mental health condition transgender high getting benefit probability low benefit option inference barplot show probability health condition respect work interference work interference probability mental health condition le probability high work interference rarely tuning cross validation scoretuning gridsearchcvtuning randomizedsearchcvtuning searching multiple parameter simultaneouslylogistic regressionaccuracy null accuracy name treatment dtype int percentage one percentage zero true value predicted value classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability kneighbors classifier def knn calculating best parameter knn kneighborsclassifier n_neighbors k_range list range weight_options uniform distance param_dist dict n_neighbors k_range weight weight_options tuningrandomizedsearchcv knn param_dist knn kneighborsclassifier n_neighbors weight uniform knn fit x_train y_train y_pred_class knn predict x_test accuracy_score evalclassmodel knn y_test y_pred_class true data final graph methoddict k neighbor accuracy_score knn rand best score rand best params weight uniform n_neighbors accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability decision tree def treeclassifier calculating best parameter tree decisiontreeclassifier featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv tree param_dist tree decisiontreeclassifier max_depth min_samples_split max_features criterion entropy min_samples_leaf tree fit x_train y_train y_pred_class tree predict x_test accuracy_score evalclassmodel tree y_test y_pred_class true data final graph methoddict decision tree classifier accuracy_score treeclassifier rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology clf adaboostclassifier clf fit x dftestpredictions clf predict x_test write prediction csv file result pd dataframe index x_test index treatment dftestpredictions save file result to_csv result csv index false result head submission result pd dataframe index x_test index treatment dftestpredictions result final prediction consists mean person needed mental health treatment mean person needed mental health treatment conclusion using employee record able build various machine learning model model ada boost achieved accuracy auc along able draw insight data via data analysis visualization medium shown article owned analytics vidhya used author discretion classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability kneighbors classifier def knn calculating best parameter knn kneighborsclassifier n_neighbors k_range list range weight_options uniform distance param_dist dict n_neighbors k_range weight weight_options tuningrandomizedsearchcv knn param_dist knn kneighborsclassifier n_neighbors weight uniform knn fit x_train y_train y_pred_class knn predict x_test accuracy_score evalclassmodel knn y_test y_pred_class true data final graph methoddict k neighbor accuracy_score knn rand best score rand best params weight uniform n_neighbors accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability decision tree def treeclassifier calculating best parameter tree decisiontreeclassifier featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv tree param_dist tree decisiontreeclassifier max_depth min_samples_split max_features criterion entropy min_samples_leaf tree fit x_train y_train y_pred_class tree predict x_test accuracy_score evalclassmodel tree y_test y_pred_class true data final graph methoddict decision tree classifier accuracy_score treeclassifier rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology clf adaboostclassifier clf fit x dftestpredictions clf predict x_test write prediction csv file result pd dataframe index x_test index treatment dftestpredictions save file result to_csv result csv index false result head submission result pd dataframe index x_test index treatment dftestpredictions result final prediction consists mean person needed mental health treatment mean person needed mental health treatment conclusion using employee record able build various machine learning model model ada boost achieved accuracy auc along able draw insight data via data analysis visualization medium shown article owned analytics vidhya used author discretion kneighbors classifierrand best score rand best params weight uniform n_neighbors accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability decision tree def treeclassifier calculating best parameter tree decisiontreeclassifier featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv tree param_dist tree decisiontreeclassifier max_depth min_samples_split max_features criterion entropy min_samples_leaf tree fit x_train y_train y_pred_class tree predict x_test accuracy_score evalclassmodel tree y_test y_pred_class true data final graph methoddict decision tree classifier accuracy_score treeclassifier rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology clf adaboostclassifier clf fit x dftestpredictions clf predict x_test write prediction csv file result pd dataframe index x_test index treatment dftestpredictions save file result to_csv result csv index false result head submission result pd dataframe index x_test index treatment dftestpredictions result final prediction consists mean person needed mental health treatment mean person needed mental health treatment conclusion using employee record able build various machine learning model model ada boost achieved accuracy auc along able draw insight data via data analysis visualization medium shown article owned analytics vidhya used author discretion classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability decision tree def treeclassifier calculating best parameter tree decisiontreeclassifier featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv tree param_dist tree decisiontreeclassifier max_depth min_samples_split max_features criterion entropy min_samples_leaf tree fit x_train y_train y_pred_class tree predict x_test accuracy_score evalclassmodel tree y_test y_pred_class true data final graph methoddict decision tree classifier accuracy_score treeclassifier rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology clf adaboostclassifier clf fit x dftestpredictions clf predict x_test write prediction csv file result pd dataframe index x_test index treatment dftestpredictions save file result to_csv result csv index false result head submission result pd dataframe index x_test index treatment dftestpredictions result final prediction consists mean person needed mental health treatment mean person needed mental health treatment conclusion using employee record able build various machine learning model model ada boost achieved accuracy auc along able draw insight data via data analysis visualization medium shown article owned analytics vidhya used author discretion decision tree rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability random forest def randomforest calculating best parameter forest randomforestclassifier n_estimators featuressize feature_cols __len__ param_dist max_depth none max_features randint featuressize min_samples_split randint min_samples_leaf randint criterion gini entropy tuningrandomizedsearchcv forest param_dist forest randomforestclassifier max_depth none min_samples_leaf min_samples_split n_estimators random_state my_forest forest fit x_train y_train y_pred_class my_forest predict x_test accuracy_score evalclassmodel my_forest y_test y_pred_class true data final graph methoddict random forest accuracy_score randomforest rand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology random forestsrand best score rand best params criterion entropy max_depth max_features min_samples_leaf min_samples_split accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability boosting def boosting building fitting clf decisiontreeclassifier criterion entropy max_depth boost adaboostclassifier base_estimator clf n_estimators boost fit x_train y_train y_pred_class boost predict x_test accuracy_score evalclassmodel boost y_test y_pred_class true data final graph methoddict boosting accuracy_score boosting accuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology boostingaccuracy null accuracy name treatment dtype int percentage one percentage zero true val pred val classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology classification accuracy classification error false positive rate precision auc score cross validated auc first predicted probability class member first predicted probability predicting neural network create input function tensorflow_version x import tensorflow tf import argparse tensorflow x selected batch_size train_steps x_train x_test y_train y_test train_test_split x test_size random_state def train_input_fn feature label batch_size dataset tf data dataset from_tensor_slices dict feature label return dataset shuffle repeat batch batch_size def eval_input_fn feature label batch_size feature dict feature label none label use feature input feature else input feature label dataset tf data dataset from_tensor_slices input dataset dataset batch batch_size return dataset return dataset define feature column define tensorflow feature column age tf feature_column numeric_column age gender tf feature_column numeric_column gender family_history tf feature_column numeric_column family_history benefit tf feature_column numeric_column benefit care_options tf feature_column numeric_column care_options anonymity tf feature_column numeric_column anonymity leave tf feature_column numeric_column leave work_interfere tf feature_column numeric_column work_interfere feature_column age gender family_history benefit care_options anonymity leave work_interfere instantiate estimator model tf estimator dnnclassifier feature_columns feature_columns hidden_units optimizer tf train proximaladagradoptimizer learning_rate l_regularization_strength train model model train input_fn lambda train_input_fn x_train y_train batch_size step train_steps evaluate model evaluate model eval_result model evaluate input_fn lambda eval_input_fn x_test y_test batch_size print ntest set accuracy accuracy f n format eval_result data final graph accuracy eval_result accuracy methoddict neural network accuracy test set accuracy making prediction inferring trained model prediction list model predict input_fn lambda eval_input_fn x_train y_train batch_size batch_size generate prediction model template nindex prediction f expected dictionary prediction col col col idx input p zip x_train index y_train prediction v p class_ids class_id p class_ids probability p probability class_id probability adding dataframe col append idx index col append v prediction col append input expecter print template format idx v probability input result pd dataframe index col prediction col expected col result head creating prediction test set generate prediction best methodology create input functiontensorflow x selected define feature columnsinstantiate estimatortrain modelevaluate modelthe test set accuracy making prediction inferring trained model generate prediction best methodologythe final prediction consists mean person needed mental health treatment mean person needed mental health treatment using employee record able build various machine learning model model ada boost achieved accuracy auc along able draw insight data via data analysis visualization medium shown article owned analytics vidhya used author discretion
98,https://www.analyticsvidhya.com/blog/2022/06/spring-security-oauth2-with-keycloak/,article published part data science blogathon day day life dealing many web mobile application access application many time may seen please log r please sign pop ups ever think application press background process background process authorize user whether authority access request achieve developer using many technique latest technology many concept authorize user via application one popular concept oauth article going learn oauth concept implement oauth feature inside spring boot application using spring security also look implement oauth pattern different kind application like server side rendered application developed using spring mvc model view controller single page application developed using framework like angular also going see implement machine machine authorization like case micro service use keycloak identity server authorization server help u implement oauth pattern oauth stand open authorization industry standard protocol developed authorization service application protocol right version usually known oauth spring mvc model view controller application single page application like angular react spring boot backend mm machine machine authorization micro service identity server authorization server note guy know spring boot spring security much better understand well image gallery application simply upload create edit print photo cloud service want access photo via google account facebook account image gallery application need provide permission hand username password google facebook account application risky hacker get access account database need safe way authorize image gallery application access google facebook account oauth framework developed reason note auth oauth stand authorization authentication instead giving google credential app redirects google login log grant permission image gallery application access google account also restrict permission grant permission google server return unique access token application application use access token access photo google account google server receives access token verifies whether valid token confirms valid token mean token generated google server grant access photo looked high level overview got know oauth protocol work move bit deeper starting oauth terminology understand oauth protocol need familiarize oauth terminology resource anything need accessed external service need authorization eg image gallery application image resource resource owner owner protected resource eg image gallery application resource owner resource server server serf host store protected resource eg image gallery application google drive resource server client service application access resource resource owner eg image gallery application web mobile desktop application micro service smart service note client two type public client web mobile desktop smart tv apps access public confidential client remote shell script cron job microservices authorization server server generates validates access token client option market aws cognito microsoft azure ad google identity platform okta key cloak spring authorization serverif want manage authorization keycloak good option understand oauth also need understand oidc oidc also known openid connect protocol built top oauth act identity layer identity layer previously show image gallery application client want access resource like photo google drive need access token authorization server access token random alphanumerical set character provide context information user make hard client understand get user information reason identity layer send additional token called id token contains basic information user like email first name last name access token used verify user authorized access resource id token used verify user information identity user clear explanation basic structure oauth protocol let start practical session keycloak spring boot configuration already discussed need configure keycloak get basic idea read introduction keycloak article using keycloak using docker image run docker imagekeycloak dashboardonce run keycloak server see dashboard need create account user created log authorization server logged first see master master default realm key cloak realm like placeholder manage set client user role realm connected create user one realm available accessible another realm create realmso first thing going create realm clicking add realm button creating realm called oauth demon realm create many client realm also many user role create clientlet go ahead create client u client going spring mvc application developed using thyme leaf going server side spring mvc application creating client called oauth demo client created see much detail client page client protocol leaving openid connect next access type dealing server side rendered spring mvc application choosing confidential leaving standard flow enabled default also providing valid redirect uris redirect uri authorization server use send u authorization code saw earlier field going provide value http localhost login oauth code oauth demo client note using client id oauth demo client default redirect uri spring boot support uri recognized spring boot application need implement endpoint spring automatically handle request keycloak redirected particular uri last thing generate client secret clicking credential tab client page particular client already generated default create usernow need create user login application achieve click user section need click add user case creating user called mino demo user created need set password click credential tab going provide random password choose login ask create password created realm client user keycloak let jump spring boot demo application let create spring boot application using http start spring io create application created application downloaded zip file extract opened intellij pom xml added dependency spring boot starter web activating spring activating mvc module spring boot starter oauth client initializing activating client authorization server spring boot starter thyme leaf enable thyme leaf feature going dependency deeper creating api request home controller serve initial request application create home html file template folderlet configure oauth client property application property spring security oauth client provider oauth demo client issuer uri http localhost realm oauth demo realmall done run spring boot application open browser network tab enter localhost api v keycloak home redirect login pagethe keycloak server request redirect_uri successful authentication along authorization code providing username password clicking login call redirect uri application us authorization code request token code highlighted yellow color access token id token received spring boot application background process can not see browser network tab providing username password clicking login call redirect uri application us authorization code request token code highlighted yellow color access token id token received spring boot application background process can not see browser network tab see token placed background process need enable spring security dependency pom xml spring boot application search external library find oidcauthorizationcodeauthenticationprovider classthen put breakpoint authenticationresult authenticate method refresh browser observe result contains accesstokenresponse idtokenresponse inside idtokenresponse claim see user information inside accesstokenresponse see access token refresh token copied token decode http jwt io response follows finally end expected result hopefully learned spring security oauth keycloak handling token request also learned oauth terminology like resource server client authorization server etc addition got know oidc also got know response authorizing access token refresh token id token please go article practical get ready next article learn keycloak key takeaway learned medium shown article owned analytics vidhya used author discretion
99,https://www.analyticsvidhya.com/blog/2022/06/a-brief-introduction-to-ai-ethics/,article published part data science blogathon artificial intelligence becoming prevalent increasingly interconnected world self driving car automated customer service agent ai slowly surely changing live work ai becomes sophisticated ethical implication use become increasingly complex several key issue consider regarding ai ethic data privacy algorithmic bias socio economic inequality one controversial issue company manipulate decision making feeding data algorithm power ai application understanding individual preference bias company control see think dictating live life result form mind control represents grave threat autonomy freedom even alarming may even aware manipulated result manipulation increasingly becoming isolated information disagrees viewpoint happens ai algorithm technology platform tailor make newsfeeds search result match belief preference consume filtered information get trapped known filter bubble cultural ideological echo chamber encounter information reaffirms existing belief result worldview become skewed one sided nothing inherently wrong echo chamber still lead problem careful example may close new idea perspective may also start see dissenting opinion threat instead simply different point view world increasingly reliant internet information essential aware echo chamber effect take step break bubble time time otherwise risk surrendering powerful machine intelligence know u better know conclusion use ai technology platform good used improve overall user experience providing user content may interested helping user similar interest find technology platform use ai invade user privacy another critical area concern ai practitioner algorithmic bias occurs ai system relies data skewed way favor certain group others happens data set used training process often contain human bias ai system learns replicates example facial recognition system trained dataset primarily white face likely misidentify people color serious consequence ai system often used make decision credit score job application therefore essential aware potential bias ai system take step avoid essential strive fairness ai eliminating bias often impossible instead necessary aware potential bias take step minimize impact may involve using diverse datasets incorporating human judgment decision making process monitoring result ai system identify correct error also technique used train machine learning model reduce amount bias contain idea add noise data set used train model force model learn diverse range example technique successfully reduces bias several different setting including facial recognition natural language processing help ensure ai system fair unbiased possible taking measure ai potential worsen problem inequality society impact ai social economic inequality significant far reaching though many potential benefit gained ai must aware potential pitfall take step mitigate right policy place harness power ai create prosperous society need hold technology platform accountable use personal data please keep mind responsibility also privilege protect data need aware information share time start thinking data ownership privacy new way demand technology platform respect data privacy interest concerned personal data used technology company let know comment important ai practitioner make algorithm fair unbiased help prevent unforeseen consequence may arise use biased algorithm furthermore removing algorithmic bias move one step closer towards creating inclusive society tried using ai ml technique remove bias data experience let know comment implementing right kind policy government ensure everyone benefit growth ai thoughtful planning execution made sure social economic class get left behind ai transforms life navigate challenge successfully ai potential enhance life countless way rapid development ai brings several ethical concern however must remain vigilant protecting fundamental right liberty must ensure ai used discriminate vulnerable group infringe privacy also need careful ai become tool power control manipulate mass risk believe potential benefit ai great ignore must find way navigate danger use ai benefit people select need proactive ensuring advancement ai unforeseen consequence society history shown machine given power used good evil future ai uncertain u society ensure rise ai detrimental community culture significant instead beneficial way help u gain innovation together thought facing inevitable future algorithm rule u still time course correction let know think comment medium shown article owned analytics vidhya used author discretion
100,https://www.analyticsvidhya.com/blog/2022/06/introduction-to-data-analysis-expression-dax/,article published part data science blogathon following depth article explaining dax stand data analysis expression dax language developed microsoft interact data variety platform power bi powerpivot ssa tabular model designed simple easy learn exposing capability flexibility tabular model dax power bi reach true potential covering working advantage disadvantage dax topic cover article dax importance dax power bi dax work function dax calculate measure calculated column conclusiondax quite important part bi provides functionality like dax built around three fundamental concept creation command fetch desired result input crucial syntax formula syntax refers shape formula constructing language used make formula example command sign operator destination column row etc example name parenthesis summation name table figure syntax dax expression function function refers predefined already existing command system example include sum add true false etc context context refers row included formula data retrieval calculation two type context row context filter context distinctcount count number distinct number count total number item column returned counta get number item column empty countrows get number row given table date get date date time format hour display hour pm format today get current date min find minimum value given column max return maximum value given column sum formula add value column produce total average take column data return average mina get minimum value along total function logical value text representation number minx calculates minimum value evaluating row expression table combine expression logically function performs logical disjunction expression negates given expression logically check criterion true return one value another value ab return absolute value exp return exponent value fact return factorial number important math function log pi power sqrt concatenate join two string together fixed round number given decimal replace replace character part string important text function search upper lower measure computed column dax expression used difference evaluation context computed column assessed row level table belongs measure evaluated context cell analysed report dax query dax query user selection report determine cell context figure navigate table tool power bi desktop create new column measure figure measure createdfigure column createdit necessary define measure table one dax requirement measure actually belong table measure moved one table another without losing functionality calculated column calculated measure use dax expression manipulate data however underlying formula different calculated column type calculation occurs row level within table calculated measure calculated cell level entire report query data analysis expression one widely used expression syntax day many application reference figure http www tutorialspoint com dax_functions dax_functions_quick_guide htmthe medium shown article owned analytics vidhya used author discretion
101,https://www.analyticsvidhya.com/blog/2022/06/top-10-power-bi-interview-questions-in-2022/,article published part data science blogathon power bi one popular data visualization analytics software product developed microsoft power bi interview provide insight variety data modelling data telling story data visualization using report dashboard source http doc microsoft com en u learn module data analytics microsoft introductiontherefore becomes necessary every aspiring data analyst good knowledge power bi article discus important power bi interview question help get good understanding power bi technique power bi data visualization tool developed microsoft help extract meaningful information raw data visualizing data set multiple source like excel sap etc using power bi share data insight anyone sharing power bi report five building block power bi visualization visualization visual like table column chart scatter chart map etc used represent data visually datasets dataset collection data used power bi creating visuals dataset combination many data source like web dataverse csv etc report report collection visualization appear together page report developed either using power bi desktop power bi service report may contain page dashboard dashboard single page presentation one visualization added example example sale dashboard pie chart kpi scorecard bar chart map etc tile tile rectangular box hold visuals like card pie chart etc power query data preparation data transformation engine used perform etl processing using power query change data type column pivot unpivot column summarize group row join append data etc thus power query help perform data transformation data cleaning change shape data etc source http doc microsoft com en u power bi transform model desktop query overviewthe three dataset connectivity mode available power bi import mode import mode default dataset connectivity mode power bi delivers fast performance imported data always stored disk querying data refreshing process data must fully loaded direct query mode direct query mode import data directly retrieves data underlying data source using native query mode used data volume large data nearly real time composite mode composite mode could integrate various direct query data source combine datasets imported import direct query mode give best performance filter used filter data filter applied visual level page level report level apart drillthrough filter used drill one page report another different type filter available power bi dashboardreportdata analysis expression dax formula language used building formula analysis service power bi power pivot excel data model dax contains variety function like filter function logical function aggregation function etc dax used calculate result calculated column measure power bi commonly used dax function power bi get data option available home tab power bi desktop get data used connect different data source loading data visualization analysis load data first perform data transformation loading user import data excel text csv web dataverse oracle mysql snowflake sap database google bigquery mariadb azure sql etc power bi source http doc microsoft com en u power bi connect data desktop tutorial importing analyzing data web pagecalculated columnmeasurein article seen important question asked power bi interview however recommended apart power bi interview question also practice developing report develop understanding data transformation sql summarize following major takeaway power bi interview question medium shown article owned analytics vidhya used author discretion
